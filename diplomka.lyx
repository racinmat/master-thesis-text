#LyX 2.2 created this file. For more info see http://www.lyx.org/
\lyxformat 508
\begin_document
\begin_header
\save_transient_properties true
\origin unavailable
\textclass scrreprt
\begin_preamble
%<-------------------------------společná nastavení------------------------------>
\usepackage[numbers,sort&compress]{natbib} %balíček pro citace literatury  
\usepackage{algorithmic}
\usepackage{color}%kvůli barvám ČVUT
\newcommand{\BibTeX}{{\sc Bib}\TeX}%BibTeX logo
\usepackage{multicol}
\usepackage[overload]{textcase}



%<-----------------------------volání stylů----------------------------------------->
% (znak % je označení komentáře: co je za ním, není aktivní)

%<--------matematické písmo--------------------------------------->

%\usepackage[helvet]{packages/sfmath}%matematika ala helvetica



%<------------------------------záhlaví stránek------------------------------------>
%\usepackage{packages/bc-headings}
\usepackage{packages/bc-fancyhdr}

%<------------------------------hlavičky kapitol------------------------------------>
%\usepackage{packages/bc-neueskapitel}
\usepackage{packages/bc-fancychap}
\end_preamble
\options cleardoublepage=empty,BCOR15mm,DIV12
\use_default_options false
\begin_modules
logicalmkup
\end_modules
\maintain_unincluded_children false
\language british
\language_package default
\inputencoding utf8
\fontencoding global
\font_roman "tgtermes" "default"
\font_sans "tgheros" "default"
\font_typewriter "tgcursor" "default"
\font_math "newtxmath" "auto"
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100 100
\font_tt_scale 100 100
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command bibtex
\index_command default
\paperfontsize default
\spacing single
\use_hyperref true
\pdf_bookmarks true
\pdf_bookmarksnumbered false
\pdf_bookmarksopen false
\pdf_bookmarksopenlevel 1
\pdf_breaklinks false
\pdf_pdfborder false
\pdf_colorlinks false
\pdf_backref false
\pdf_pdfusetitle true
\papersize a4paper
\use_geometry false
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 1
\use_package esint 0
\use_package mathdots 0
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine basic
\cite_engine_type default
\biblio_style \use_bibtopic false
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 0
\index Index
\shortcut idx
\color #008000
\end_index
\secnumdepth 2
\tocdepth 2
\paragraph_separation indent
\paragraph_indentation default
\quotes_language english
\papercolumns 1
\papersides 2
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Standard
\begin_inset space ~
\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
thispagestyle{empty}
\end_layout

\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
begin{center}
\end_layout

\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
pagenumbering{arabic}
\end_layout

\end_inset


\begin_inset VSpace 10mm
\end_inset


\end_layout

\begin_layout Standard

\family sans
\shape smallcaps
\size largest
\noun on
Czech Technical University in Prague
\family default
\shape default
\size default
\noun default

\begin_inset Newline newline
\end_inset


\begin_inset VSpace 0.5em
\end_inset


\family sans
\shape smallcaps
\size largest
\noun on
Faculty of Electrical Engineering
\family default
\shape default
\size default
\noun default

\begin_inset Newline newline
\end_inset


\begin_inset VSpace 1em*
\end_inset


\family sans
\shape smallcaps
\size larger
\noun on
Department of Cybernetics
\family default
\shape default
\size default
\noun default

\begin_inset VSpace 15mm
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Graphics
	filename obrazky/lev.png
	lyxscale 50
	width 30text%

\end_inset


\begin_inset VSpace 15mm
\end_inset


\end_layout

\begin_layout Standard

\family sans
\size huge
MASTER'S THESIS
\end_layout

\begin_layout Standard
\begin_inset VSpace 15mm
\end_inset


\end_layout

\begin_layout Standard

\family sans
\size largest
3D map estimation from a single RGB image
\end_layout

\begin_layout Standard
\begin_inset VSpace 10mm
\end_inset


\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
end{center} 
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset VSpace vfill*
\end_inset


\end_layout

\begin_layout Standard
\begin_inset VSpace 10mm
\end_inset


\end_layout

\begin_layout Description
\noindent
\align block

\size large
Author: 
\family sans
Bc.
 Matěj Račinský
\end_layout

\begin_layout Description
\noindent
\align block

\size large
Thesis
\begin_inset space ~
\end_inset

supervisor: doc.
 Ing.
 Karel Zimmermann, PhD.
\begin_inset space \hfill{}
\end_inset


\family sans
In Prague, May 2018
\end_layout

\begin_layout Standard
\begin_inset Newpage cleardoublepage
\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
thispagestyle{empty}
\end_layout

\end_inset


\begin_inset space ~
\end_inset


\end_layout

\begin_layout Standard

\size small
\begin_inset space \space{}
\end_inset


\end_layout

\begin_layout Standard
\noindent

\size small
\begin_inset VSpace vfill
\end_inset

 
\begin_inset ERT
status open

\begin_layout Plain Layout

% nastavuje dynamické umístění následujícího textu do spodní části stránky
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\begin_inset space ~
\end_inset


\end_layout

\begin_layout Subparagraph*
Author statement for the graduate thesis: 
\begin_inset Newline newline
\end_inset


\end_layout

\begin_layout Standard
I declare that the presented work was developed independently and that I
 have listed all the sources of information used within it in accordance
 with the methodical instructions for observing the ethical principles in
 the presentation of university theses.
 
\end_layout

\begin_layout Standard

\size small
\begin_inset VSpace bigskip
\end_inset


\size default

\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
noindent 
\end_layout

\end_inset


\size small
 Prague, date 
\size default
________
\size small

\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
hspace{
\backslash
fill}
\end_layout

\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout

$
\backslash
overline{
\backslash
textrm{~~~~~~~~~signature~~~}}$
\end_layout

\end_inset


\begin_inset Newline newline
\end_inset

 
\begin_inset ERT
status open

\begin_layout Plain Layout

% doplňte patřičné datum, jméno a příjmení
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard

\size small
\begin_inset ERT
status open

\begin_layout Plain Layout

%%%   Výtisk pak na tomto míste nezapomeňte PODEPSAT!
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout

%%%                                         *********
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Newpage cleardoublepage
\end_inset


\end_layout

\begin_layout Standard

\size small
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
thispagestyle{plain}
\end_layout

\end_inset

 
\end_layout

\begin_layout Standard

\size small
\begin_inset ERT
status open

\begin_layout Plain Layout

%
\backslash
setcounter{page}{3}
\end_layout

\end_inset

 
\begin_inset ERT
status open

\begin_layout Plain Layout

% nastavení číslování stránek
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
\noindent

\size small
\begin_inset space ~
\end_inset


\begin_inset VSpace vfill
\end_inset


\end_layout

\begin_layout Description
\noindent

\size small
Název
\begin_inset space ~
\end_inset

práce: Odhad 3D mapy z jednoho RGB obrazu
\end_layout

\begin_layout Description
\noindent

\size small
Autor: Bc.
 Matěj Račinský
\end_layout

\begin_layout Description
\noindent

\size small
Katedra
\begin_inset space ~
\end_inset

(ústav):
\size default
 Kate
\size small
dra kybernetiky
\end_layout

\begin_layout Description
\noindent

\size small
Vedoucí
\begin_inset space ~
\end_inset

bakalářské
\begin_inset space ~
\end_inset

práce: 
\size large
doc.
 Ing.
 Karel Zimmermann, PhD.
\end_layout

\begin_layout Description
\noindent

\size small
e-mail
\begin_inset space ~
\end_inset

vedoucího: zimmerk@fel.cvut.cz
\begin_inset Newline newline
\end_inset


\end_layout

\begin_layout Description
\noindent

\size small
Abstrakt 
\size default
Tato práce se zabývá využitím virtuálních světů z počítačových her jakožto
 zdroje dat pro strojové učení, a odhadem voxelové mapy z jednoho RGB obrázku
 za pomoci hlubokého učení.
 Tato práce zahrnuje skripty pro napojení se na PC hru GTA V a sběr dat
 z ní pro tvorbu automaticky anotovaných datasetů, a implementaci hluboké
 neuronové sítě v TensorFlow.
 
\end_layout

\begin_layout Description
\noindent

\size small
Klíčová
\begin_inset space ~
\end_inset

slova: Deep learning, Machine learning, GTA V, virtual world, depth estimation,
 voxelmap estimation, RAGE
\begin_inset Newline newline
\end_inset


\end_layout

\begin_layout Description
\noindent

\size small
\begin_inset CommandInset line
LatexCommand rule
offset "0.5ex"
width "100line%"
height "1pt"

\end_inset


\end_layout

\begin_layout Description
\noindent

\size small
Title: 3D map estimation from a single RGB image
\end_layout

\begin_layout Description
\noindent

\size small
Author: Bc.
 Matěj Račinský
\end_layout

\begin_layout Description
\noindent

\size small
Department: Department of Cybernetics
\end_layout

\begin_layout Description
\noindent

\size small
Supervisor: 
\size large
doc.
 Ing.
 Karel Zimmermann, Ph.D.
\end_layout

\begin_layout Description
\noindent

\size small
Supervisor's
\begin_inset space ~
\end_inset

e-mail
\begin_inset space ~
\end_inset

address: zimmerk@fel.cvut.cz
\begin_inset Newline newline
\end_inset


\end_layout

\begin_layout Description
\noindent

\size small
Abstract 
\size default
In this thesis we explore virtual worlds used as data source for machine
 learning and voxel map estimation from single RGB image with deep learning.
 This thesis describes principles and implementation of hooking into GTA
 V and gathering data from it to create automatically annotated dataset,
 and implementation of deep neural network in TensorFlow.
\end_layout

\begin_layout Description
\noindent

\size small
Keywords: Deep learning, Machine learning, GTA V, virtual world, depth estimatio
n, voxel map estimation, RAGE
\end_layout

\begin_layout Standard

\size small
\begin_inset CommandInset toc
LatexCommand tableofcontents

\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout

% vkládá automaticky generovaný obsah dokumentu
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Chapter
\begin_inset Argument 1
status open

\begin_layout Plain Layout
Introduction
\end_layout

\end_inset

Introduction
\end_layout

\begin_layout Standard
The main aim of this thesis is reactive 3D map estimation from single RGB
 image.
 The task of building 3D map is essential procedure in many robotics tasks
 and is then utilized in high-level planning.
 For many static environments, like buildings and roads, we have detailed
 maps available which are being used for this task.
 But for other environments, with lots of cars like in parking lots or crossroad
s, or off-road trajectories, the pre-calculated 3D map is not available
 and it needs to be created online.
 There are multiple approaches for 3D map estimation, lots of them using
 depth measurements.
 Often used approach is utilization of depth measurements and then building
 3D map which is continuously updated during movement.
 This is a complicated task due to matching newly obtained data on existing
 3D map.
 This thesis describes an approach based on reactive mapping without the
 need to build long-term internal world representation, using only immediate
 sensor input.
 Only RGB camera input is used for 3D map reconstruction instead of depth
 measurements.
 The RGB image has many advantages, RGB cameras are cheap and in higher
 resolution than depth sensors.
 RGB camera is a passive sensor, so multiple RGB cameras won't interfere
 with each other unlike lidars, so the RGB image can be obtained easily.
 Due to the problem of obtaining dense and precise ground truth, this thesis
 exploits usage of synthetic datasets as a fast and efficient way to obtain
 ground truth for such task.
 GTA V is used here as a simulator for the creation of a synthetic dataset.
\end_layout

\begin_layout Section
Obtaining large annotated datasets for high-capacity models
\end_layout

\begin_layout Standard
In recent years, both machine learning and deep learning has experienced
 great progress in many fields 
\begin_inset CommandInset citation
LatexCommand cite
key "history-began-from-alexnet"

\end_inset

.
 Deep learning has outperformed many other machine learning approaches by
 using deep, high-capacity models trained on large datasets.
 Especially in the field of computer vision, neural networks achieve state
 of the art results in most of the tasks.
 Many tasks in computer vision are the first where deep neural networks
 achieve state of the art results before being used in other fields, and
 in this field, deeper and deeper architectures are being proposed earlier
 than in other fields.
 With larger amount of parameters, the need for large datasets is growing,
 with current datasets unable to cover the need for annotated data.
 
\end_layout

\begin_layout Standard
Data has proven to be limiting factor in many computer vision tasks.
 The main problem is that manual data annotation is exhausting, time-consuming
 and costly.
 That is even more significant for pixel-wise annotation which is crucial
 for tasks of semantic segmentation.
 Pixel-wise annotated datasets are orders of magnitude smaller than image
 classification datasets.
 This is sometimes called 
\begin_inset Quotes eld
\end_inset

curse of dataset annotation
\begin_inset Quotes erd
\end_inset

 
\begin_inset CommandInset citation
LatexCommand cite
key "semantic-instance-annotation"

\end_inset

, because more detailed semantic labelling leads to smaller size of dataset.
 
\end_layout

\begin_layout Standard
Many novel neural network architectures are being proposed every year because
 of ongoing research and increasing computing power.
 With growing capacity and number of parameters in these new models, there
 is need for bigger and bigger datasets for training.
 Several papers shown positive correlation between the size of data and
 performance 
\begin_inset CommandInset citation
LatexCommand cite
key "unreasonable-effectiveness-of-data,revisiting-unreasonable-efectiveness,real-time-human-pose-recognition-in-parts-from-a-single-depth-image"

\end_inset

.
\end_layout

\begin_layout Subsection
Manual annotation services
\end_layout

\begin_layout Standard
There are attempts to develop tooling necessary to speed up the manual annotatio
n process during datasets creation, most known being the Amazon Mechanical
 Turk (AMT) 
\begin_inset CommandInset citation
LatexCommand cite
key "mechanical-turk"

\end_inset

 and Supervise.ly 
\begin_inset CommandInset citation
LatexCommand cite
key "supervise.ly"

\end_inset

.
\end_layout

\begin_layout Standard
Amazon Mechanical Turk is a platform for crowdsourcing work and has been
 used in many academic fields.
 AMT is a 
\begin_inset Quotes eld
\end_inset

marketplace for work that requires human intelligence
\begin_inset Quotes erd
\end_inset

 
\begin_inset CommandInset citation
LatexCommand cite
key "amt-article"

\end_inset

, a web-based platform for distributing tasks to a pool of human workers,
 known colloquially as Turkers.
 These tasks are typically small (i.e.
 a few minutes to perform a task rather than days or weeks) and payments
 pay task are low, in the orders of cents per task.
 This platform provides possibility to distribute the annotation process
 among many people at once and for low price.
 Although still being manual, it accelerates the annotation process and
 helps researchers to annotate data more easily 
\begin_inset CommandInset citation
LatexCommand cite
key "amt-cv-annotation"

\end_inset

.
 Quality of work can not be guaranteed since task provider can't supervise
 workers and correct them manually, but this disadvantage can be compensated
 by various quality assurance approaches 
\begin_inset CommandInset citation
LatexCommand cite
key "amt-cv-annotation"

\end_inset

.
 Supervise.ly is another web-based platform, focused on computer vision datasets
 annotation.
 It provides helpful tooling for annotators in unified web interface, speeding
 up the annotation process.
 It offers integration with the AMT, so together, they seem to be candidate
 for industrial standard of data annotation.
 
\end_layout

\begin_layout Standard
Although these tools lead to faster annotation process at lower cost, it
 still can not be compared to the fully automated data gathering and annotation,
 which could potentially solve these problems of lack of datasets in many
 computer vision and related tasks.
 The power of automatic annotation can be also expressed in terms of money.
 Although not many manually annotated datasets describe time of annotation,
 some of them do.
 In Cityscapes, the fine annotation took 1.5 hour per image 
\begin_inset CommandInset citation
LatexCommand cite
key "cityscapes"

\end_inset

.
 If we had annotators paid minimum hourly wage in USA, 7.25$, what would
 mean each fine annotated image has price 10.875$.
 The whole fine annotated Cityscapes dataset with 5000 images thus have
 has price at least 54 375$ and took 312.5 days to annotate.
 In the dataset I created as part of my thesis, I gathered 33292 images
 in 3.5 hours.
 If we would annotate this dataset same as Cityscapes fine annotation, it
 would take over 2080 days, which is approximately 5.7 years, and it would
 cost 362 050$.
 Now we can see how this automatic annotation approach is significantly
 both time and money saving.
\end_layout

\begin_layout Subsection
Generating synthetic datasets from game engines
\end_layout

\begin_layout Standard
In last decades, gaming industry has grown hugely and expanded from small
 and specific community into public society and became mainstream industry.
 The gaming industry became big driving force in many fields, and indirectly
 influenced even machine learning.
 The mainstream model of gaming is on personal computers, where each player
 has his own gaming PC, along with console gaming.
 Thanks to ever-growing number of players, lots of money got into industry
 and the growing demand for better graphics in games led to big improvements
 in both software-computer graphics and hardware-graphics cards.
 With lots of money being invested by players in their PCs, GPU manufacturers
 were able to deliver more powerful GPUs every year and we can see exponential
 growth of GPU computational power 
\begin_inset CommandInset citation
LatexCommand cite
key "high-performance-gpu"

\end_inset

.
\end_layout

\begin_layout Standard
Big companies in gaming industry have enough resources to develop the state
 of the art real-time computer graphics, which can we see in their products,
 AAA games with graphics very near to reality.
 Recent papers 
\begin_inset CommandInset citation
LatexCommand cite
key "playing-for-data,driving-in-matrix"

\end_inset

 have shown that we can use screenshots from PC games to obtain large automatica
lly or semi-automatically annotated datasets, which improve learning.
 This lets us to outperform same models trained only on real data and achieve
 state of the art results on public datasets (KITTI dataset in 
\begin_inset CommandInset citation
LatexCommand cite
key "driving-in-matrix"

\end_inset

 and CamVid dataset in 
\begin_inset CommandInset citation
LatexCommand cite
key "playing-for-data"

\end_inset

).
\end_layout

\begin_layout Standard
Other approaches utilizing computer games for machine learning have appeared
 recently, one of the most knows being OpenAI Gym, software platform providing
 reinforcement learning framework 
\begin_inset CommandInset citation
LatexCommand cite
key "openai-gym"

\end_inset

.
 Later, Universe 
\begin_inset CommandInset citation
LatexCommand cite
key "universe"

\end_inset

 has been released, designed to turn various computer games into OpenAI
 Gym environment, easing to deploy same reinforcement learning algorithms
 into many different virtual worlds, but due to legal issues, they require
 permission from the game owner to integrate particular game into the Universe.
 
\end_layout

\begin_layout Section
Thesis Contribution
\end_layout

\begin_layout Standard
In this thesis, I propose deep convolutional neural network for 3D map estimatio
n.
 For this task, I leverage synthetic data from rich virtual worlds in form
 of synthetic dataset and propose software architecture for dataset creation.
 Then I create data for training in form of pairs of RGB image capturing
 space in front of car, and voxelmap of that space.
 In the last part, Then I empirically evaluate multiple setups of predictions
 of both depth estimation and 3D map estimation
\end_layout

\begin_layout Subsection
Thesis structure
\end_layout

\begin_layout Standard
This thesis is structured as follows.
 Chapter 2 covers related work and previous achievements of synthetic datasets.
 Chapter 3 describes game Grand Theft Auto V and its utilization in creating
 synthetic datasets, this part serves as a documentation for using libraries
 for manipulating the GTA world.
 Here I describe whole modding process, the game API, internal game coordinate
 systems and data which can be obtained from the game.
 Chapter 4 describes voxel map estimation from a single RGB image.
 It is followed by chapter 5, where I describe all experiments done as part
 of this thesis.
\end_layout

\begin_layout Chapter
Related work
\end_layout

\begin_layout Section
Using Computer Games for Machine Learning
\end_layout

\begin_layout Standard
Richter et al.
 
\begin_inset CommandInset citation
LatexCommand cite
key "playing-for-data"

\end_inset

 used GTA V to obtain screenshots and performed semi-automated pixel-wise
 semantic segmentation.
 Although the process was not fully automatic, the annotation speed per
 image was drastically increased, being 771 times faster than fine per-image
 annotation of Cityscapes 
\begin_inset CommandInset citation
LatexCommand cite
key "cityscapes"

\end_inset

 and 514 times faster than per-image annotation of CamVid 
\begin_inset CommandInset citation
LatexCommand cite
key "camvid"

\end_inset

.
 Richter et al.
 extracted 24 966 images from game GTA V, which is roughly two orders of
 magnitude larger than CamVid and three orders of magnitude larger than
 semantic annotations for KITTI dataset.
 They trained the prediction module of Yu and Kolthun 
\begin_inset CommandInset citation
LatexCommand cite
key "kolthun-dilation"

\end_inset

 and by using on 
\begin_inset Formula $\frac{1}{3}$
\end_inset

 of the CamVid training set (which is ) and all 24 966 GTA V screenshots,
 they outperformed same model trained on whole CamVid training dataset.
\end_layout

\begin_layout Standard
For images extraction, they use RenderDoc
\begin_inset CommandInset citation
LatexCommand cite
key "renderdoc"

\end_inset

, stand-alone graphics debugger.
 It intercepts the communication between the game and the GPU and allows
 to gather screenshots.
 It's advantage is that it can be used for different games, allowing to
 gather datasets in various environments.
\end_layout

\begin_layout Standard
Johnson-Roberson et al.
 
\begin_inset CommandInset citation
LatexCommand cite
key "driving-in-matrix"

\end_inset

 used GTA V screenshots, depth and stencil buffer to produce car images
 and automatically calculated their bounding boxes.
 
\end_layout

\begin_layout Standard
On these generated data, they trained Faster R-CNN 
\begin_inset CommandInset citation
LatexCommand cite
key "faster-r-cnn"

\end_inset

 only on screenshots from the GTA V game, using up to 200 000 screenshots,
 which is one order of magnitude bigger than Cityscapes dataset.
 Using only screenshots for training, they outperformed same architecture
 trained on Cityscapes, evaluating on KITTI dataset.
 They developed their own GTA V mod
\begin_inset CommandInset ref
LatexCommand ref
reference "sec:GTA-V-modding"

\end_inset

 to hook into GPU calls and gather screenshots from here.
 
\end_layout

\begin_layout Standard
GTA V has also been used as a part of the OpenAI Universe, promising powerful
 environment for machine learning, but due to Rockstar Games Terms of Services,
 it has been removed and is not currently supported by the OpenAI Universe.
 Nowadays, OpenAI Universe supports many games, but focuses mainly on Reinforcem
ent Learning, and thus allows only communication with games through a strict
 API and does not allow the complex setup and modifications of the game
 described in this thesis.
\end_layout

\begin_layout Section
3D map estimation
\end_layout

\begin_layout Standard
3D map estimation is a complex an important task in computer vision, and
 many approaches have been used in attempts to solve this task.
 It is tightly coupled to the depth estimation, because the most important
 part of the 3D estimation is to estimate the depth of objects on the image,
 since spatial information about horizontal and vertical position is contained
 in the image directly.
 Most studies focus on binocular cameras 
\begin_inset CommandInset citation
LatexCommand cite
key "taxonomy-stereo"

\end_inset

, also called stereo vision where depth and 3D map is reconstructed using
 information from two cameras, and thus depth and position in 3D can be
 estimated from seeing same points from different views.
 The other approach is to use only single image, without the binocular informati
on.
 That task is more challenging due to lack of information from the second
 image, and even the State of the Art approaches fall short compared to
 the binocular vision.
 But thanks to the recent success of deep convolutional neural networks,
 there have been advances in monocular depth estimation.
\end_layout

\begin_layout Subsection
Deep convolutional neural networks
\end_layout

\begin_layout Standard
In machine learning, deep convolutional neural networks have been used more
 and more mostly due to significant breakthroughs in image classification
 tasks.
 Advantage of deep neural networks is the capability to naturally utilize
 low-level, mid-level, and high-level features, without need for manual
 feature-engineering and lets us train whole architectures end-to-end.
 Recent results 
\begin_inset CommandInset citation
LatexCommand cite
key "going-deeper-with-convolutions,very-deep-image-recognition"

\end_inset

 in visual recognition tasks, mostly on ImageNet dataset 
\begin_inset CommandInset citation
LatexCommand cite
key "imagenet"

\end_inset

, reveal depth of network plays crucial role in model accuracy and started
 the revolution in many computer vision tasks 
\begin_inset CommandInset citation
LatexCommand cite
key "deep-learning-nature"

\end_inset

.
 
\end_layout

\begin_layout Standard
Neural networks are machine learning models vaguely inspired by the biological
 neural networks that constitute human brains.
 Neural networks are being used as universal approximators, theoretically
 capable of arbitrarily accurate approximation to an arbitrary function
 
\begin_inset CommandInset citation
LatexCommand cite
key "universal-approximation"

\end_inset

 and thus are used for many different tasks, for instance classification,
 regression, clustering, and many other.
 The neural network, as name suggests, is network of interconnected artificial
 neurons.
 An artificial neuron is based on biological neuron, but with many simplificatio
ns.
 It consists of inputs to the neuron (inspired by dendrites), the neuron
 itself, and the output of the neuron (inspired by axon).
 Intuitively the artificial neuron sums all inputs weighted by neuron weights
 and send them to into its activation function, inspired by axon.
 Formally, the output of neuron 
\begin_inset Formula $y$
\end_inset

 with inputs 
\begin_inset Formula $\boldsymbol{x}=\left(x_{0},...,x_{n}\right)$
\end_inset

 , weights 
\begin_inset Formula $\boldsymbol{w}=\left(w_{0},...,w_{n}\right)$
\end_inset

, bias 
\begin_inset Formula $\beta$
\end_inset

 and activation function 
\begin_inset Formula $\varphi\left(\cdot\right)$
\end_inset

 is defined as 
\begin_inset Formula 
\[
y=\varphi\left(\beta+\stackrel[i=0]{n}{\sum}w_{i}x_{i}\right)=\varphi\left(\boldsymbol{w}\cdot\boldsymbol{x}+\beta\right)
\]

\end_inset

.
 In its typical architecture, the neural network comprises of many layers
 stacked on top of each other, with output of neurons in one layer connected
 to all inputs of all neurons in the next layer and information flows from
 each neuron of previous layer to each neuron of next layer.
 This is called fully-connected neural network.
 Other approach is to connect inputs of a neuron only to a subset of neurons
 in previous layers, specifically its corresponding neuron in previous layer
 and its neighbours.
 This dramatically reduces number of parameters needed to be learned.
 The neural network is then usually learned end-to-end by the back-propagation
 algorithm 
\begin_inset CommandInset citation
LatexCommand cite
key "backprop"

\end_inset

 which minimizes the loss function using stochastic gradient descent or
 its alternatives.
\end_layout

\begin_layout Standard
When neural network has many layers stacked on top of each other, it is
 called a deep neural network.
 The success of deep neural networks lies in their abilities to approximate
 non-trivial function because of high number of parameters of the network.
 The disadvantage of deep neural networks is the time they need to learn
 because it takes days or weeks to train state of the art deep neural networks.
 
\end_layout

\begin_layout Subsection
Residual networks
\end_layout

\begin_layout Standard
As mentioned above, deep network became very popular and widely used in
 computer vision and machine learning.
 But there are some caveats when building a new deep architecture for neural
 network.
 When deeper networks were created simply by stacking more and more layers
 in the model, these models become hard to train.
 One of these problems is the notorious vanishing gradient, which is being
 addressed by many approaches, like normalized initialization 
\begin_inset CommandInset citation
LatexCommand cite
key "efficient-backprop,difficulty-deep-forward"

\end_inset

 and normalization layers 
\begin_inset CommandInset citation
LatexCommand cite
key "batch-normalization"

\end_inset

.
 Other problem is the degradation problem, where training accuracy starts
 to worsen, after stacking more layers 
\begin_inset CommandInset citation
LatexCommand cite
key "highway-networks"

\end_inset

.
 Residual networks aim to address this problem of degradation.
\end_layout

\begin_layout Standard
If shallower model is able to learn with higher accuracy than deeper model,
 we want the deeper model to be able to learn at least same as shallower
 one, or better.
 If we stack a new layer into the model, we want them to be able to learn
 identity mapping.
 Then the model with new layers will behave same as shallower model during
 prediction.
 Experiments show that current solvers using gradient descent are not able
 to find such solution which would let newer layer to learn identity mapping
 in feasible time.
 The residual learning aims to tackle this problem by explicitly learning
 the residual mapping instead of original mapping.
 If we want our newly stacked layers to represent mapping 
\begin_inset Formula $\mathcal{H}\left(\boldsymbol{x}\right)$
\end_inset

, instead we let them learn another mapping 
\begin_inset Formula $\mathcal{F}\left(\boldsymbol{x}\right)=\mathcal{H}\left(\boldsymbol{x}\right)-\boldsymbol{x}$
\end_inset

.
 The original mapping we aim to, 
\begin_inset Formula $\mathcal{H}\left(\boldsymbol{x}\right)$
\end_inset

 is then reconstructed as 
\begin_inset Formula $\mathcal{H}\left(\boldsymbol{x}\right)=\mathcal{F}\left(\boldsymbol{x}\right)+\boldsymbol{x}$
\end_inset

.
 He et al.
 
\begin_inset CommandInset citation
LatexCommand cite
key "resnet"

\end_inset

 shows it is easier to learn the residual mapping than to learn the original
 mapping.
 It also more easily preserves the identity mapping, if 
\begin_inset Formula $\mathcal{F}\left(\boldsymbol{x}\right)=\boldsymbol{0}$
\end_inset

, because it is easier to fit these layers to zero than to explicitly learn
 identity mapping.
 
\end_layout

\begin_layout Standard
The formulation of 
\begin_inset Formula $\mathcal{F}\left(\boldsymbol{x}\right)+\boldsymbol{x}$
\end_inset

 can be realized by feed-forward neural network with shortcut connections
 
\begin_inset CommandInset citation
LatexCommand cite
key "bishop-neural-networks"

\end_inset

, also known as skip connections.
 The advantage of this approach is it can be easily implemented in current
 neural network frameworks out of the box, and it whole network can still
 be learned by SGD end-to-end without any further modifications.
 Intuitively, the identity mapping, shortcut connection can be seen as an
 information highway, where information flows unchanged both forwards and
 backwards without any changes.
 He et al.
 
\begin_inset CommandInset citation
LatexCommand cite
key "resnet"

\end_inset

 show the residual learning lets us train very deep models and present several
 architectures based on residual learning: ResNet-18, ResNet-34, Resnet-50,
 ResNet-101, and ResNet-152.
 basically ResNet architecture consists of 4 residual blocks, repeated many
 times.
 The difference between individual architectures is only in number of repetition
s these residual blocks.
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename obrazky/resnet-layers.png
	lyxscale 80
	width 6cm

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Residual and shortcut layers
\begin_inset CommandInset citation
LatexCommand cite
key "resnet"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
Residual networks have been widely adopted as a part of many upcoming architectu
res for various tasks.
 Thy are utilized in many image mapping tasks, for instance depth prediction
 or semantic segmentation.
 He et al.
 
\begin_inset CommandInset citation
LatexCommand cite
key "mask-rcnn"

\end_inset

 used ResNet as a backbone for their Mask R-CNN architecture for pixel-wise
 semantic segmentation and trained on COCO dataset.
 Their setup works well on both real photos and synthetic images, as can
 be seen in figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Synthetic-image-labelled"

\end_inset

 where image from synthetic dataset is being labelled.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename obrazky/mask-rcnn.jpg
	lyxscale 35
	width 8cm

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:Synthetic-image-labelled"

\end_inset

Synthetic image labelled by Mask R-CNN
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Depth estimation
\end_layout

\begin_layout Standard
Depth estimation is one of the most fundamental tasks in computer vision.
 Most existing methods 
\begin_inset CommandInset citation
LatexCommand cite
key "depth-and-surface-estimation,predicting-depth"

\end_inset

 formulate depth estimation as a regression task due to the continuous nature
 of depth values.
 Those models for depth estimation are usually trained to minimize L2 norm
 between ground truth and predicted depths.
 However it shows that regressing to exact depth is difficult task.
 In many applications, depth can be known only approximately.
 With aforementioned approach, I approximate the regression formulation
 of depth estimation with classification problem.
 Thus instead of training to predict exact depth, we predict only depth
 range, still small enough to be useful for our application.
 Advantage of classification formulation of the problem is that after applying
 softmax on output of neural network, depths are naturally predicted as
 confidence in form of probability that particular depth level is occupied
 
\begin_inset CommandInset citation
LatexCommand cite
key "depth-estimation-as-classification"

\end_inset

.
 Most notable methods 
\begin_inset CommandInset citation
LatexCommand cite
key "depth-estimation-as-classification,depth-estimation-hierarchical-fusion-soft-weighting"

\end_inset

 use deep convolutional networks based on famous object classification architect
ures.
 
\end_layout

\begin_layout Standard
Li et al.
 
\begin_inset CommandInset citation
LatexCommand cite
key "depth-estimation-hierarchical-fusion-soft-weighting"

\end_inset

 use architecture based on deep residual network 
\begin_inset CommandInset citation
LatexCommand cite
key "resnet"

\end_inset

, specifically Resnet-152.
 The architecture is modified, it does not use the fully connected layer
 in the end of the network, which drastically decreases the number of trainable
 parameters, and instead it appends one convolutional and one deconvolutional
 layer.
 Also, Li et al.
 utilize the hierarchical fusion, where output of each block of ResNet is
 concatenated to other outputs, with dropout layer afterwards, then the
 network utilizes both low-level and high-level features of the input image
 during the final layers of depth estimation.
 The output of final layer is 120x160x200, meaning it outputs 120x160 pixels
 image with 200 depth levels.
 The depth space is equally discretized in log space.
 Specifically, 
\begin_inset Formula 
\[
l=round\left(\frac{\log\left(d\right)-\log\left(d_{min}\right)}{q}\right)
\]

\end_inset

where 
\begin_inset Formula $l$
\end_inset

 is the quantized label, q is the width of quantization bin, 
\begin_inset Formula $d$
\end_inset

 is the continuous depth value and 
\begin_inset Formula $d_{min}$
\end_inset

 is the minimum depth value in the dataset.
 
\end_layout

\begin_layout Standard
Other notable approach of Li et al.
 is soft weighted sum inference.
 Usually, the depth is reconstructed intuitively as a centre of depth bin
 with maximum value pixel-wise 
\begin_inset CommandInset citation
LatexCommand cite
key "depth-estimation-as-classification"

\end_inset

.
 The last layer is softmax, so values can be interpreted as probabilities
 of each depth bin being being correct one.
 Thus the output contains probability distribution of depth pixel-wise.
 Li et al.
 
\begin_inset CommandInset citation
LatexCommand cite
key "depth-estimation-hierarchical-fusion-soft-weighting"

\end_inset

 reconstruct depth as a soft weighted sum inference by 
\begin_inset Formula 
\[
\hat{d}=\exp\left\{ \boldsymbol{w}^{T}\boldsymbol{p}\right\} ,w_{i}=\log\left(d_{min}\right)+q\cdot i
\]

\end_inset

where 
\begin_inset Formula $\boldsymbol{w}$
\end_inset

 is the weight vector of depth bins, 
\begin_inset Formula $i$
\end_inset

 is bin index, 
\begin_inset Formula $\boldsymbol{p}$
\end_inset

 is the output score, and 
\begin_inset Formula $\hat{d}$
\end_inset

 is reconstructed depth.
\end_layout

\begin_layout Standard
Then the model is trained end-to-end to minimize the pixel-wise multinomial
 logistic loss 
\begin_inset Formula 
\begin{equation}
L=-\left[\stackrel[i=1]{N}{\sum}\stackrel[k=1]{K}{\sum}1\left\{ D_{k}=D_{i}^{*}\right\} \log\left(p_{i}^{D_{k}}\right)\right]\label{eq:loss-multinomial}
\end{equation}

\end_inset

where 
\begin_inset Formula $N$
\end_inset

 is the number of pixels, 
\begin_inset Formula $K$
\end_inset

 is the number of depth bins, 
\begin_inset Formula $D_{i}^{*}$
\end_inset

 is ground truth depth on pixel 
\begin_inset Formula $i$
\end_inset

, and 
\begin_inset Formula $p_{i}^{D_{k}}$
\end_inset

 is the probability of pixel 
\begin_inset Formula $i$
\end_inset

 to be labelled with 
\begin_inset Formula $D_{k}$
\end_inset

 depth bin.
\end_layout

\begin_layout Standard
The classical multi-class classification assumes categorical labels without
 any ordering defined.
 However depth bins are ordinal labels, not categorical, because they have
 defined ordering and semantically it makes sense to take into account distance
 between correct and predicted label during training.
 This approach is not usable for categorical data, but for nominal data,
 this knowledge can be used to provide more information for the learning.
 Cao et al.
 
\begin_inset CommandInset citation
LatexCommand cite
key "depth-estimation-as-classification"

\end_inset

 propose modified loss function which takes distance between depth ins into
 account.
 They use similar architecture, with pixel-wise multinomial logistic loss,
 but with slight modification.
 In usual logistic loss, the loss is non-zero only for correct label, due
 to the 
\begin_inset Formula $1\left\{ D_{k}=D_{i}^{*}\right\} $
\end_inset

part of the function.
 Cao et al.
 propose 
\begin_inset Quotes eld
\end_inset

information gain
\begin_inset Quotes erd
\end_inset

 
\begin_inset CommandInset citation
LatexCommand cite
key "depth-estimation-as-classification"

\end_inset

 instead, where 
\begin_inset Formula $1\left\{ D_{k}=D_{i}^{*}\right\} $
\end_inset

 is replaced by 
\begin_inset Formula $H\left(D_{i}^{*},D_{k}\right)$
\end_inset

, where 
\begin_inset Formula $H$
\end_inset

 is 
\begin_inset Formula $B\times B$
\end_inset

 symmetric matrix with elements 
\begin_inset Formula $H\left(p,q\right)=exp\left(-\alpha\left(p-q\right)^{2}\right)$
\end_inset

 where 
\begin_inset Formula $\alpha$
\end_inset

 is a constant.
 This information gain matrix lets the SGD propagate the error not only
 through correct label and preceding softmax layer, but also through its
 neighbouring labels, which helps updating the network parameters.
 So the modified loss is 
\begin_inset Formula 
\begin{equation}
L=-\left[\stackrel[i=1]{N}{\sum}\stackrel[k=1]{K}{\sum}H\left(D_{i}^{*},D_{k}\right)\log\left(p_{i}^{D_{k}}\right)\right]\label{eq:loss-infogain}
\end{equation}

\end_inset

.
 
\end_layout

\begin_layout Subsection
3D map estimation
\end_layout

\begin_layout Standard
Choy et al.
 
\begin_inset CommandInset citation
LatexCommand cite
key "choy-3d-r2n2"

\end_inset

 use an LSTM framework extended for 3D representation reconstruction, called
 3D Recurrent Neural Network for reconstructing 3D images from single or
 multiple images and perform State of the Art results on single-view reconstruct
ion.
 In their setup they used encoder-decoder architecture with 
\begin_inset Formula $127\times127$
\end_inset

 input images and 
\begin_inset Formula $32\times32\times32$
\end_inset

 voxelmap and trained it to minimize the softmax cross-entropy loss.
 This setup too learns on synthetic data, using mostly CAD models.
 This approach is heavily focused on one object dominating the input image
 which is suited for precise 3D map estimation, but not much for estimation
 of 3D map of whole scene, usually with multiple objects.
\end_layout

\begin_layout Standard
todo: dodat Mask-RCNN a ukázku běhu na gta screenshotu
\end_layout

\begin_layout Chapter
Transforming GTA V into the State of the Art simulator
\end_layout

\begin_layout Standard
In this thesis, Grand Theft Auto V (GTA V) game is used for creating synthetic,
 nearly photo-realistic dataset.
 
\end_layout

\begin_layout Section
GTA V introduction
\end_layout

\begin_layout Standard
GTA V is action-adventure open-world video game developed by Rockstar North
 and published by Rockstar Games.
 The game was released on 17.9.2013 for PlayStation 3 and Xbox 360
\begin_inset CommandInset citation
LatexCommand cite
key "gta-release"

\end_inset

 , in 18.11.2014 for PS4 and Xbox One and in 14.4.2015 it was released on PC,
 Windows
\begin_inset CommandInset citation
LatexCommand cite
key "gta-release-pc"

\end_inset

.
\end_layout

\begin_layout Standard
The game is based on proprietary game engine, called 
\begin_inset CommandInset label
LatexCommand label
name "rage"

\end_inset

RAGE (Rockstar Advanced Game Engine) 
\begin_inset CommandInset citation
LatexCommand cite
key "gta-5-rage"

\end_inset

, which is used as a base for most of Rockstar Games products.
 
\end_layout

\begin_layout Standard
Till the release on Microsoft, Windows, it has been in development for 5
 years with approximate 1000-person team 
\begin_inset CommandInset citation
LatexCommand cite
key "gta-interview-studio"

\end_inset

.
 The world of GTA V was modelled on Los Angeles
\begin_inset CommandInset citation
LatexCommand cite
key "gta-v-interview"

\end_inset

 and other areas of Southern California, with road networks respecting design
 of Los Angeles map.
 
\end_layout

\begin_layout Standard
As could be expected from AAA game like GTA V, motion capture was used to
 character's both body and facial movements.
 
\end_layout

\begin_layout Standard
There are several reasons why GTA V is better for dataset creation than
 other games.
 To use a game for dataset creation, we have multiple requirements.
 The graphics of the game must be near photorealistic, since we try to to
 use it instead of photos for computer vision tasks.
 This disqualifies most of games, and leaves us only with AAA games produced
 by big companies and few other games with State of the Art graphics.
 
\end_layout

\begin_layout Standard
The other requirement is possibility of good-enough way to interact with
 the game programmatically.
 Usually we want to setup at least part of the environment before gathering
 data.
 This part heavily depends on community around the particular game.
\end_layout

\begin_layout Standard
Also the advantage of GTA V compared to some other games is abundance of
 models and various sceneries in its virtual world.
 It has complex transportation system of roads, highways, intersections,
 railroad crossing, tunnels, and pedestrians.
 It also has urban, suburban, and rural environments 
\begin_inset CommandInset citation
LatexCommand cite
key "video-games-for-autonomous-driving"

\end_inset

.
\end_layout

\begin_layout Standard
In gaming subculture, there are communities where people specialize in reverse-e
ngineering of games and development of modifications to these games.
 These people are called modders or mod developers, and these unofficial
 modifications and extension of games are called mods.
 For few games, developers welcome this kind of activity and sometimes they
 even release tools to ease the game modding.
 In most cases, the game developers simply don't care and in few cases,
 they actively fight against the reverse-engineering and modding.
 
\end_layout

\begin_layout Standard
The GTA V is second case, where Rockstar Games does not actively try to
 prevent the reverse-engineering, but they don't release any tools to ease
 it, either.
 This results in cyclic process of Rockstar Games releasing new version
 of game, including backward compatibility (BC) breaks, and community reverse
 engineering the new version and adjusting their mods to work with the new
 version.
\end_layout

\begin_layout Standard
The modding community around the GTA V is based mostly on community around
 GTA IV, which was previous big game produced by Rockstar Games.
 So many tools are just GTA IV based and only modified to work with GTA
 V.
 Luckily, the community is large and productive, so we have many mods and
 many function in GTA V reverse-engineered and thus prepared for programmatic
 interactions.
\end_layout

\begin_layout Subsection
Cars
\end_layout

\begin_layout Standard
There is big variety of cars models.
 Specifically, the are 259 car models, all of them are listed here 
\begin_inset CommandInset citation
LatexCommand cite
key "gta-vehicle-models"

\end_inset

.
 These models cars of various shapes and sizes, from golf carts to trucks
 and trailers.
 This diversity is representative of real distribution of vehicles.
 It even allows us to simulate environments with various types of vehicles,
 which would be very difficult in real environment.
 GTA V provides us many information about cars, more on this will be covered
 in section 
\begin_inset CommandInset ref
LatexCommand ref
reference "sec:GTA-V-data-obtaining"

\end_inset

.
\end_layout

\begin_layout Subsection
Pedestrians
\end_layout

\begin_layout Standard
GTA V has pedestrians and provides some information about them, more on
 this in section 
\begin_inset CommandInset ref
LatexCommand ref
reference "sec:GTA-V-data-obtaining"

\end_inset

.
 The game has pedestrians of both genders and various ethnicities.
 Pedestrians appear in various poses, like standing, walking, sitting, many
 animations etc.
 The main drawback of GTA V is that all pedestrians are about the same height
\begin_inset CommandInset citation
LatexCommand cite
key "video-games-for-autonomous-driving"

\end_inset

.
\end_layout

\begin_layout Section
Automotive Simulators
\end_layout

\begin_layout Standard
Currently, there are some open-source simulation platforms for automotive
 industry which could be theoretically used for creating synthetic datasets.
 But compared to AAA games like GTA V, they have much less resources and
 much less customers to finance the development.
 In result, simulators have worse graphics than AAA games and NPC (non playable
 characters) don't have as sophisticated behaviour.
 In GTA V, drivers mostly follow traffic regulations, traffic lights and
 traffic lanes, which leafs to very realistic environment better than simulators
 can provide.
\end_layout

\begin_layout Section
\begin_inset CommandInset label
LatexCommand label
name "sec:GTA-V-modding"

\end_inset

GTA V modding ecosystem
\end_layout

\begin_layout Standard
Although the modding community is quite big, as it is in lots of open-source
 communities, essential part of community depends on one person.
 Here, it is Alexander Blade.
 In his free time, he reverse-engineered big part of GTA V and developed
 ScriptHookV
\begin_inset CommandInset citation
LatexCommand cite
key "scripthookv-gtaforums"

\end_inset

, library enabling to perform native calls into GTA V in C++ and develop
 GTA V mods in C++.
 Currently, more people in community participates in reverse-engineering
 and they share their knowledge in GTA forum thread
\begin_inset CommandInset citation
LatexCommand cite
key "nativedb-research"

\end_inset

.
\end_layout

\begin_layout Standard
List of all reverse-engineered native functions is kept in following list
 
\begin_inset CommandInset citation
LatexCommand cite
key "nativedb"

\end_inset

.
 Assumably, GTA V contains ~5200 natives.
 There is no original native name list of functions in GTA V, name hashes
 are used instead.
 During reverse-engineering and game decompilation, ~2600 native names were
 discovered using brute-force and manual checking afterwards.
 For these functions, number of parameters and returns of these calls are
 also known.
 In the native functions list, for big part of functions we know their name,
 signature and how do they affect the game.
 The rest remains to be discovered yet.
\end_layout

\begin_layout Standard
When new version of game is released, in few days to weeks, new version
 of ScriptHookV is released, fixing BC breaks.
\end_layout

\begin_layout Standard
Other heavily used mod in community is ScriptHookDotNet2, which is built
 atop of ScriptHookV and creates bridge between C# language and ScriptHookV,
 effectively allowing to write GTA V mods in C#.
 It is available as open-source 
\begin_inset CommandInset citation
LatexCommand cite
key "scripthookvdotnet"

\end_inset

.
 Along with creating bridge between C# and GTA V, it wraps most used native
 calls into classes, leveraging object-oriented paradigm for mod development
 using getters and setters as proxies for native calls.
 
\end_layout

\begin_layout Standard
Next notable mod is NativeUI
\begin_inset CommandInset citation
LatexCommand cite
key "nativeui"

\end_inset

.
 It renders windows atop of GTA V GUI and allows us to define custom control
 panels for manipulating custom functionality in other mods.
 
\end_layout

\begin_layout Standard
Unlike most of other mods, these three mods act more as a framework for
 mod development.
 
\end_layout

\begin_layout Standard
Since GTA V is a game, it requires human interaction.
 For simulator-like behaviour we would want the car to drive autonomously
 to crawl data without human interaction.
 This can be done using VAutodrive
\begin_inset CommandInset citation
LatexCommand cite
key "vautodrive"

\end_inset

.
 This allows us to use NPC automatic behaviour patterns for main player,
 letting the player randomly wander the world, even in car, without need
 of human assistance during crawling.
 Unfortunately, this package is not open-source.
\end_layout

\begin_layout Standard
Generally, the community is not united in their view on open-source.
 Some mods are available open-source on GitHub.
 Other mods are being distributed only as compiled binaries
\begin_inset CommandInset citation
LatexCommand cite
key "gta-5-mods"

\end_inset

.
 Lots of modders develop mostly by trial and error, and no comprehensive
 documentation for mod development is available, unfortunately.
 There are some tutorials 
\begin_inset CommandInset citation
LatexCommand cite
key "gta-5-mod-tutorial"

\end_inset

, but they are far from complete and provide only basic knowledge, leaving
 reader without deeper understanding of underlying principles.
\end_layout

\begin_layout Standard
Modders mostly meet online on few GTA forums, where they exchange knowledge
 
\begin_inset CommandInset citation
LatexCommand cite
key "gta-forums,gta-5-mods-forum"

\end_inset

.
 GitHub or Stack Overflow, which are biggest information sources for usual
 software development, are not used much in GTA modding community.
 Due to this fact, these forums, along with source code of open-source mods
 comprise knowledge-base of mod development.
\end_layout

\begin_layout Section
Simulation environment and development stack
\end_layout

\begin_layout Standard
In this thesis, I use mod based on 
\begin_inset CommandInset citation
LatexCommand cite
key "driving-in-matrix"

\end_inset

 but enhanced to gain more control of the game and to obtain more information
 from the game.
 
\end_layout

\begin_layout Standard
In later text, I'll refer to some GTA V native functions or data structures
 which are output of GTA V native functions.
 To be consistent and to help understanding, I will use function names from
 native function list 
\begin_inset CommandInset citation
LatexCommand cite
key "nativedb"

\end_inset

.
\end_layout

\begin_layout Standard
The basic architecture of C# mods come from the ScriptHookDotNet2, where
 each mod script extends the GTA.Script class.
 For each child of this class, we can set integer Interval, Tick and KeyUp
 callbacks.
 Interval property determines how big interval in milliseconds is between
 consecutive Tick calls.
 Tick callback is being called periodically, so here we can set tasks which
 we want to perform periodically, e.g.
 screenshot gathering.
 The KeyUp callback is for interacting with user and reading the user's
 keyboard input.
 For data gathering mods, this is mostly used for debugging purposes, script
 disabling or restarting.
 
\end_layout

\begin_layout Standard
The ScriptHookDotNet2 brings useful feature for debugging and developing
 scripts based on in.
 When changing mod compiled binaries, we don't have to restart the game
 for newer version of mod to be active.
 Pressing Insert causes all C# binaries to reload, causing new version of
 source code to load into game, which dramatically decreases time of feedback
 loop during development.
 This does not work for C++ mods and compiled .asi files.
\end_layout

\begin_layout Standard
During the data retrieval from the game, two types of data are being gathered.
 Image data from GPU buffers and data about camera, cars, pedestrians and
 other in-game entities.
 All image data are being persistent into the filesystem directly.
 Other data are persisted into PostgreSQL database running locally on the
 machine.
 Since the main mod life-cycle loop is running in single thread, other threads
 are used for sending data into the database for faster simulation.
 Other setups are possible, depending on use-case of gathered data and need
 for scalability.
 Johnson-Roberson et al.
 
\begin_inset CommandInset citation
LatexCommand cite
key "driving-in-matrix"

\end_inset

 send all images into the Amazon Web Services and save them in a S3 bucket,
 also the database may run remotely and since the communication with database
 runs in separate thread, longer delay is not an issue.
 Other approached are saving all data into the filesystem directly, without
 database, denormalized per image.
 This is easier to setup, but leads to complications during data querying,
 because SQL is much more suited querying tool due to the relational nature
 of these data.
\end_layout

\begin_layout Standard
Since the data retrieval takes many hours usually, there is sometimes need
 to change some parameters of the environment without stopping the run or
 to generally control the data retrieval remotely.
 For this purpose, the mod opens a socket server during startup and reads
 from it during the Tick method call.
 Along with the modded game running, there is a python REST based web server
 with simple controls for changing time of day, weather, stopping and restarting
 the data retrieval.
 During the web server startup, it created socket client which connects
 to the socket server in GTA mod.
 This setup allows to remotely control the data retrieval over the internet.
 
\end_layout

\begin_layout Standard
The web-server part with socket client is available on 
\begin_inset Flex URL
status open

\begin_layout Plain Layout

https://github.com/racinmat/GTAVisionExport-Server
\end_layout

\end_inset

.
 The installation procedure and running is described in the readme.
 It can be run simply by 
\begin_inset Flex Code
status open

\begin_layout Plain Layout
python main.py
\end_layout

\end_inset

 which starts the server on port 5000.
 It also contains HTML website with server as a client for the REST API,
 this can be started either by simply running local Nginx web-server or
 by using prepared docker container with properly configured server.
 This is run, as usual docker container, by 
\begin_inset Flex Code
status open

\begin_layout Plain Layout
docker compose up
\end_layout

\end_inset

.
 The repository also contains lightweight web-based gallery for browsing
 gathered images.
 The main problem with viewing gathered data is their size.
 Usually there is hundreds of thousands images (RGB image, depth image and
 stencil image per sample) in single directory, which is difficult to handle
 for both windows and unix operating systems.
 To solve this issue, I developed a lightweight REST API based python script
 with pagination support for viewing this dataset.
 It can be run by 
\begin_inset Flex Code
status open

\begin_layout Plain Layout
python gallery.py
\end_layout

\end_inset

 which starts the REST server on port 5001.
 The client side of this paginated gallery is in file gallery.html and also
 uses the Nginx server to run, same as website for controlling the data
 gathering.
 It uses HTTPS protocol with self-signed certificate, and the Nginx server
 is configured to use the HTTP2 protocol which supports large number of
 requests at the same time.
 This bypasses the restriction of accessing 6 resources at the same time
 from one domain which is in HTTP1.1.
 Thanks to this, hundreds of images can be displayed in very fast manner,
 both locally and remotely.
 The gallery can be seen in figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Dataset-viewing-web"

\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Graphics
	filename obrazky/dataset-viewing.PNG
	lyxscale 30
	width 14cm

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:Dataset-viewing-web"

\end_inset

Dataset viewing web interface
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Section
\begin_inset CommandInset label
LatexCommand label
name "sec:GTA-V-data-obtaining"

\end_inset

GTA V native API and data obtaining
\end_layout

\begin_layout Standard
The data obtained from GTA V can be divided into multiple categories.
\end_layout

\begin_layout Itemize
Image data
\end_layout

\begin_layout Itemize
Rendering pipeline matrices
\end_layout

\begin_layout Itemize
GTA V internal data
\end_layout

\begin_deeper
\begin_layout Itemize
entities
\end_layout

\begin_layout Itemize
camera
\end_layout

\begin_layout Itemize
player
\end_layout

\begin_layout Itemize
other data via API
\end_layout

\end_deeper
\begin_layout Subsection
Image data
\end_layout

\begin_layout Standard
There are 3 image data.
 RGB image, depth image and stencil image.
\end_layout

\begin_layout Standard
RGB image is usual camera image.
 Depth image is content of GPU's depth buffer, in NDC.
 More detailed description of depth values is in subsection 
\begin_inset CommandInset ref
LatexCommand ref
reference "subsec:Camera-to-NDC"

\end_inset

.
 The last is pixel-wise stencil buffer.
 The stencil semantics is explained in the next paragraph.
\end_layout

\begin_layout Subsubsection
Stencil data
\end_layout

\begin_layout Standard
Stencil buffer contains auxiliary data per pixel.
 It is 8bit unsigned integer, where 1.-4.
 bits (counting from LSB) contain object type ID, and 5.-8.
 bits contain certain flags.
 
\end_layout

\begin_layout Standard
That means there are 15 object types and 4 flags.
 
\end_layout

\begin_layout Standard
For some object type IDs, I reverse engineered its semantics based on correspond
ing RGB image.
 
\end_layout

\begin_layout Standard
\align center
\begin_inset Tabular
<lyxtabular version="3" rows="6" columns="3">
<features tabularvalignment="middle">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<row>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Object type ID binary
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Object type ID decimal
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Semantics
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0000
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
background (buildings, roads, hills...)
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0001
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
1
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
pedestrian
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0010
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
2
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
vehicle
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0011
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
3
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
tree, grass
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0111
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
7
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
sky
\end_layout

\end_inset
</cell>
</row>
</lyxtabular>

\end_inset


\end_layout

\begin_layout Standard
The background, pedestrian and vehicle IDs are most important for vehicles
 detection and semantic segmentation, and other object types are not so
 valuable for these tasks, which is why I didn't investigate further in
 semantics and they remain to be reverse-engineered.
\end_layout

\begin_layout Standard
For stencil flags, semantics was discovered for half of them.
 
\end_layout

\begin_layout Standard
\align center
\begin_inset Tabular
<lyxtabular version="3" rows="3" columns="3">
<features tabularvalignment="middle">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<row>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
position of bit
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
position in whole stencil value
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Semantics
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
5
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
xxx1xxxx
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
artificial light source
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
8
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
1xxxxxxx
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
player's character
\end_layout

\end_inset
</cell>
</row>
</lyxtabular>

\end_inset


\end_layout

\begin_layout Standard
Sample stencil image can be seen in 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Visualization-pipeline"

\end_inset

.
\end_layout

\begin_layout Standard
\align center
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename obrazky/2018-03-30--06-00-56--114-stencil-color-ids.png
	width 12cm

\end_inset


\end_layout

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename obrazky/2018-03-30--06-00-56--114.jpg
	lyxscale 30
	width 12cm

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:Sample-stencil-object"

\end_inset

Sample stencil object types image and corresponding RGB image
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Subsubsection
Extracting images from GPU's internal buffers
\end_layout

\begin_layout Standard
To understand how the image gathering works, we need to dive deeper into
 Microsoft Windows graphics.
\end_layout

\begin_layout Standard
In Microsoft Windows, the main graphics engine is DirectX (roughly Windows
 equivalent of OpenGL).
 One part of DirectX is Direct3D, which is used to render 3D graphics with
 hardware acceleration, and most importantly, it provides graphics API.
 The whole process of obtaining image data is done by mod provided as part
 of the paper 
\begin_inset CommandInset citation
LatexCommand cite
key "driving-in-matrix"

\end_inset

.
 The mod has two parts, called native and managed plugin.
\end_layout

\begin_layout Standard
Image data is being obtained by native plugin by hooking into Direct3D 11's
 present callback.
 That means the native call is replaced with custom code, which is being
 executed and then returns to the native call.
 In that custom code, content of GPU's buffers is copied.
 Specifically, the depth and stencil data are captured by hooking into Direct3D’
s ID3D11ImmediateContext:: ClearDepthStencilView and saving the buffers
 before each call.
 Because of optimizations applied by the graphics card drivers, the function
 needs to be re-hooked into the clear function each frame.
 When saving each sample, the managed plugin requests all current buffers
 from the native plugin and the buffers are downloaded from the GPU and
 copied into managed memory.
\begin_inset CommandInset citation
LatexCommand cite
key "driving-in-matrix"

\end_inset

.
 
\end_layout

\begin_layout Subsection
\begin_inset CommandInset label
LatexCommand label
name "subsec:Rendering-pipeline-data"

\end_inset

Rendering pipeline data
\end_layout

\begin_layout Standard
Direct3D allows us to get rendering pipelines through the D3D11_MAP_READ
 call.
 This way, the world, world-view, and world-view-projection matrices are
 obtained.
 Let's denote them as world matrix = 
\begin_inset Formula $W$
\end_inset

, world-view by = 
\begin_inset Formula $VW$
\end_inset

, and world-view-projection = 
\begin_inset Formula $PVW$
\end_inset

.
 We need to get individual matrices, which we obtain by multiplication by
 matrices inversion:
\begin_inset Formula 
\begin{align*}
V & =VW\cdot W^{-1}\\
P & =PVW\cdot\left(VW\right)^{-1}=PV\cdot W^{-1}\cdot V^{-1}
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
View and projection matrices are important on their own, without the world
 matrix.
 Their semantics and importance is more described in section 
\begin_inset CommandInset ref
LatexCommand ref
reference "sec:Reverse-engineering-the-RAGE"

\end_inset

.
 These matrices can be simply obtained by inversion as stated above.
 But this approach is not numerically stable, and sometimes causes resulting
 matrix to be incorrect and not usable for further usage.
 Another caveat of this approach is that during data gathering in higher
 speeds, the native call becomes laggy and resulting matrices are highly
 imprecise.
 Although we can obtain these matrices via the native call, they can be
 reconstructed with higher precision, as described in 
\begin_inset CommandInset ref
LatexCommand ref
reference "sec:Reverse-engineering-the-RAGE"

\end_inset

.
\end_layout

\begin_layout Subsection
GTA V internal data
\end_layout

\begin_layout Standard
These data are probably most valuable compared to data gathering by other
 methods.
 In this section, I describe which data about various game objects can be
 obtained and how to obtain them.
 
\end_layout

\begin_layout Standard
All data in this section are obtained though native calls into GTA V, which
 are listed here 
\begin_inset CommandInset citation
LatexCommand cite
key "nativedb"

\end_inset

.
 As mentioned above, they can be called from C++ and C#.
 For convenience, and because most of my mod development is done in C#,
 I will also describe the C# wrappers.
 The ScriptHookDotNet2
\begin_inset CommandInset citation
LatexCommand cite
key "scripthookvdotnet"

\end_inset

 wrapper heavily uses object properties.
 The C# API is divided into multiple classes, listed 
\begin_inset CommandInset href
LatexCommand href
name "here"
target "https://github.com/crosire/scripthookvdotnet/tree/dev_v3/source/scripting"

\end_inset

.
 Each class has properties, whose getters and setters are implemented as
 calling the native functions.
 This feature is nice tooling and leads to more readable and maintainable
 code.
 I will describe parts of API which are most useful for synthetic datasets
 creation.
\end_layout

\begin_layout Subsubsection
Coordinate systems and axes
\end_layout

\begin_layout Standard
The model, world and view coordinate systems are all in meters.
 In the model coordinate system, e.g.
 when we have coordinate system of a car, Z-axis is oriented upwards, Y-axis
 is oriented in front of the car, and X-axis to the right of the car.
 In the camera coordinate system, the Z-axis is oriented behind the camera,
 Y-axis upwards of camera and the X-axis to the left of camera.
 In the world coordinate system, if the camera has rotation = 
\begin_inset Formula $\left(0,0,0\right)$
\end_inset

, it is heading in direction of Y-axis, X-axis is heading right of the camera,
 and Z axis is heading upwards.
\end_layout

\begin_layout Subsubsection
Game state manipulation
\end_layout

\begin_layout Standard
In the ScriptHookVDotNet2, where is Game class, which is the main entry-point
 for most of game state manipulation.
 Here I'll describe methods and properties useful and needed for data retrieval.
\end_layout

\begin_layout LyX-Code
Game.Pause(true)
\end_layout

\begin_layout Standard
pauses the whole game.
 
\end_layout

\begin_layout LyX-Code
Game.Pause(false)
\end_layout

\begin_layout Standard
un-pauses the game.
 The 
\end_layout

\begin_layout Standard
\begin_inset Flex Code
status open

\begin_layout Plain Layout
Game.ScreenResolution
\end_layout

\end_inset


\end_layout

\begin_layout Standard
property returns Size object with height and width of the current screen,
 so 
\end_layout

\begin_layout LyX-Code
Game.ScreenResolution.Width; Game.ScreenResolution.Height;
\end_layout

\begin_layout Standard
returns screen width and height respectively.
 As seen above, the main player character can be accessed by 
\begin_inset Flex Code
status open

\begin_layout Plain Layout
Game.Player.Character
\end_layout

\end_inset

 property.
 
\end_layout

\begin_layout Standard
The Player Character property is used for handling the main player.
 It has following method important for data retrieval.
 
\begin_inset Flex Code
status open

\begin_layout Plain Layout
Game.Player.Character.Position
\end_layout

\end_inset

 provides position of player in world coordinates.
 This is useful if we want to obtain entities only in certain distance of
 player or if we want to spawn new vehicle on player's position.
 Similarly to position, we can access player's current velocity by 
\end_layout

\begin_layout LyX-Code
Game.Player.Character.Velocity
\end_layout

\begin_layout Standard
which returns the velocity 3D vector.
 If we have a vehicle, we set the player as driver as follows 
\end_layout

\begin_layout LyX-Code
Game.Player.Character.SetIntoVehicle(vehicle, 
\end_layout

\begin_layout LyX-Code
VehicleSeat.Driver)
\end_layout

\begin_layout Standard
.
 When player is in a vehicle, we can access it later by the 
\end_layout

\begin_layout LyX-Code
Game.Player.Character.CurrentVehicle
\end_layout

\begin_layout Standard
property.
 
\end_layout

\begin_layout Standard
Other important Class is World, which, as seen above, provides the camera
 handling interface, and many other features.
 As mentioned above, the World.DestroyAllCameras() clears all scripted cameras.
 The 
\begin_inset Flex Code
status open

\begin_layout Plain Layout
World.CreateCamera()
\end_layout

\end_inset

 creates new scripted camera.
 The 
\begin_inset Flex Code
status open

\begin_layout Plain Layout
World.RenderingCamera
\end_layout

\end_inset

 holds reference to the currently active scripted camera.
 Setting the camera to the 
\begin_inset Flex Code
status open

\begin_layout Plain Layout
World.RenderingCamera
\end_layout

\end_inset

 activates that camera and starts rendering with that camera, and setting
 null reference returns the view to the Gameplay camera, which is default
 camera used during playing.
 
\end_layout

\begin_layout Standard
Other methods allow us directly manipulate the world.
 The 
\begin_inset Flex Code
status open

\begin_layout Plain Layout
World.CurrentDayTime
\end_layout

\end_inset

 returns the TimeSpan object, which is time in day in the GTA world.
 By setting this property we can change the time of day as we need by assigning
 the TimeSpan instance 
\end_layout

\begin_layout LyX-Code
World.CurrentDayTime = 
\end_layout

\begin_layout LyX-Code
new TimeSpan(int hours, int minutes, int seconds)
\end_layout

\begin_layout Standard
.
 The 
\begin_inset Flex Code
status open

\begin_layout Plain Layout
World.Weather
\end_layout

\end_inset

 uses same getter, setter interface, so 
\begin_inset Flex Code
status open

\begin_layout Plain Layout
World.Weather
\end_layout

\end_inset

 returns current weather, and 
\begin_inset Flex Code
status open

\begin_layout Plain Layout
World.Weather = Weather.Foggy
\end_layout

\end_inset

 sets the weather to be foggy.
 List of all possible weathers is in the Weather enum, which contains Unknown,
 ExtraSunny, Clear, Clouds, Smog, Foggy, Overcast, Raining, ThunderStorm,
 Clearing, Neutral, Snowing, Blizzard, Snowlight, Christmas, Halloween.
 
\end_layout

\begin_layout Standard
The World class also contains the 
\end_layout

\begin_layout LyX-Code
World.CreateVehicle(Model model, Vector3 position)
\end_layout

\begin_layout Standard
which spawn new car in the game and returns reference to the newly spawned
 car.
 Position is in world coordinates and model can be any of vehicle models
 in game.
 For instance,
\end_layout

\begin_layout LyX-Code
World.CreateVehicle(new Model(VehicleHash.Seven70), 
\end_layout

\begin_layout LyX-Code
Game.Player.Character.Position)
\end_layout

\begin_layout Standard
creates new sports car in player's position .
 Whole list of known models is available in 
\begin_inset Flex URL
status open

\begin_layout Plain Layout

https://github.com/crosire/scripthookvdotnet/blob/dev_v3/source/scripting/World/E
ntities/Vehicles/VehicleHash.cs
\end_layout

\end_inset

 where 554 model hashes are enumerated.
 
\end_layout

\begin_layout Subsubsection
Camera
\end_layout

\begin_layout Standard
The camera is probably one of the most crucial parts of the API.
 There is Gameplay Camera, which is the default camera used during usual
 playing.
 This camera can be manipulated, but its usage is limited.
 Other approach is usage of scripted cameras, which can be fully controlled
 programmatically.
 We can create multiple scripted cameras and switch between them, but the
 community discovered there is hard limit of 26 cameras at time
\begin_inset CommandInset citation
LatexCommand cite
key "gta-mod-camera"

\end_inset

.
 Camera can be created by calling 
\end_layout

\begin_layout LyX-Code
Camera camera = World.CreateCamera(
\end_layout

\begin_layout LyX-Code
new Vector3(x, y, z), new Vector3(x, y, z), float fov);
\end_layout

\begin_layout Standard
which returns handle to the new camera.
 The position is in world coordinates in meters.
 The rotation is in degrees as rotation around particular axis, as in OpenGL
 engine.
 In the Aircraft principal axes terminology, the 
\begin_inset Formula $\left(x,y,z\right)$
\end_inset

 rotation vector means 
\begin_inset Formula $\left(pitch,roll,yaw\right)$
\end_inset

 respectively.
 The fov argument is vertical field of view in degrees.
 The default value is 50.
 One can of course call the underlying native functions directly, but this
 wrapper helps with managing the handles.
 
\end_layout

\begin_layout Standard
All scripted cameras can be destroyed by calling the 
\end_layout

\begin_layout LyX-Code
World.DestroyAllCameras(); 
\end_layout

\begin_layout Standard
Switching to the scripted camera can be done by calling 
\end_layout

\begin_layout LyX-Code
camera.IsActive = true;
\end_layout

\begin_layout Standard
, and switching from this camera to some other by 
\end_layout

\begin_layout LyX-Code
camera.IsActive = false; 
\end_layout

\begin_layout Standard
Right after creating the camera, when we don't to switch to this camera
 immediately, we need to deactivate it by these two lines of code
\end_layout

\begin_layout LyX-Code
camera.IsActive = false;
\end_layout

\begin_layout LyX-Code
World.RenderingCamera = null;
\end_layout

\begin_layout Standard
this code ensures that camera is created properly.
 We don't know precisely, why is this needed, but this is the price for
 using the closed and reverse-engineered codebase.
 By calling the
\end_layout

\begin_layout LyX-Code
camera.IsActive = true; 
\end_layout

\begin_layout LyX-Code
World.RenderingCamera = camera;
\end_layout

\begin_layout Standard
scripted camera becomes active and view is switched to this camera.
 All properties of camera can be set simply by calling setters
\end_layout

\begin_layout LyX-Code
camera.position = new Vector3(x, y, z);
\end_layout

\begin_layout LyX-Code
camera.rotation = new Vector3(y, x, z);
\end_layout

\begin_layout LyX-Code
camera.nearClip = distance;
\end_layout

\begin_layout LyX-Code
camera.farClip = distance;
\end_layout

\begin_layout LyX-Code
camera.FieldOfView = fov;
\end_layout

\begin_layout Standard
and read by calling getters 
\end_layout

\begin_layout LyX-Code
Vector3 position = camera.position;
\end_layout

\begin_layout LyX-Code
Vector3 rotation = camera.rotation;
\end_layout

\begin_layout LyX-Code
float distance = camera.nearClip;
\end_layout

\begin_layout LyX-Code
float distance = camera.farClip;
\end_layout

\begin_layout LyX-Code
float fov = camera.FieldOfView;
\end_layout

\begin_layout Standard
The near clip and far clip is again in meters.
 So we can set all needed parameters simply by calling getters and setters.
 So if we want to use camera, we simply set parameters and then activate
 it.
 This creates the static camera.
 
\end_layout

\begin_layout Standard
Sometimes we want the camera to be moving, e.g.
 when gathering data by driving car.
 Camera can be attached to any entity by calling 
\end_layout

\begin_layout LyX-Code
camera.AttachTo(entity, new Vector3(x, y, z));
\end_layout

\begin_layout Standard
the second parameter is relative offset to middle of the attached entity.
 The offset is in model coordinate system which means its position moves
 and rotates with attached entity.
 During dataset gathering, I used this code
\end_layout

\begin_layout LyX-Code
camera.AttachTo(Game.Player.Character.CurrentVehicle, 
\end_layout

\begin_layout LyX-Code
new Vector3(0f, 2f, 0.4f));
\end_layout

\begin_layout Standard
to attach camera to the front part of player's current vehicle.
 As seen from code, it attaches camera 2 meters in front of mar model's
 centre and 40cm above it, making it effectively sitting on top of the hood
 of the car.
 These parameters are taken from 
\begin_inset CommandInset citation
LatexCommand cite
key "video-games-for-autonomous-driving"

\end_inset

.
 
\end_layout

\begin_layout Standard
When we have camera attached to the car, we need to update periodically
 its rotation, otherwise it will be heading the same direction in world
 coordinates and won't rotate with car it is attached to.
 So when we have our Tick method, which is being called periodically, we
 need to update the active camera like this:
\end_layout

\begin_layout LyX-Code
camera.AttachTo(Game.Player.Character.CurrentVehicle, 
\end_layout

\begin_layout LyX-Code
cameraPosition);
\end_layout

\begin_layout LyX-Code
camera.Rotation = cameraRotation;
\end_layout

\begin_layout Standard
if we want to have our camera rotated in fixed offset compared to car rotation
 (e.g.
 camera looking behind the car), we simply set camera's rotation as offset
 like this
\end_layout

\begin_layout LyX-Code
camera.AttachTo(Game.Player.Character.CurrentVehicle, 
\end_layout

\begin_layout LyX-Code
new Vector3(0f, -2f, 0.6f));  // now our camera is sitting in back of the
 car
\end_layout

\begin_layout LyX-Code
camera.Rotation = Game.Player.Character.CurrentVehicle.Rotation 
\end_layout

\begin_layout LyX-Code
+ new Vector3(0f, 0f, 180f);
\end_layout

\begin_layout Standard
which sets camera rotation in world coordinates as a sum of car's rotation
 and relative rotation of camera, 180° yaw.
\end_layout

\begin_layout Subsubsection
Game entities
\end_layout

\begin_layout Standard
All game entities extend the abstract Entity class.
 Important children of Entity class are Ped, Vehicle and Prop classes, wrapping
 pedestrians, vehicles, and various objects, respectively.
 List of all entities contained in the entity pool can be obtained by 
\begin_inset Flex Code
status open

\begin_layout Plain Layout
World.GetAllEntities()
\end_layout

\end_inset

.
 Methods 
\end_layout

\begin_layout LyX-Code
World.GetAllPeds(); World.GetAllVehicles(); World.GetAllProps();
\end_layout

\begin_layout Standard
will return list of all pedestrians, vehicles, and props, respectively.
 These lists are huge and we don't need all of returned entities usually.
 Most of the time, we are interested only in objects on the screen, which
 are only in certain distance from us.
 The most straightforward option is to filter these lists afterwards, but
 luckily, the library offers us helper methods for filtering the nearby
 entities more effectively, without need to instantiate more distant entities.
 These helper methods are 
\end_layout

\begin_layout LyX-Code
World.GetNearbyEntities(Vector3 position, float radius)
\end_layout

\begin_layout Standard
, and analogously, 
\end_layout

\begin_layout LyX-Code
World.GetNearbyPeds(Vector3 position, float radius); 
\end_layout

\begin_layout LyX-Code
World.GetNearbyVehicles(Vector3 position, float radius); 
\end_layout

\begin_layout LyX-Code
World.GetNearbyProps(Vector3 position, float radius);
\end_layout

\begin_layout Standard
.
 As expected, radius is in meters, position is in world coordinates.
 So obtaining for instance all vehicles up to half kilometres distant from
 the current player, we call 
\end_layout

\begin_layout LyX-Code
World.GetNearbyVehicles(Game.Player.Character, 500.0f)
\end_layout

\begin_layout Standard
.
 Every entity has position, rotation, velocity, and handle.
 Position and rotation have similar semantics as cameras.
 Velocity is velocity as a 3D vector.
 The handle is unique in-game identifier of particular entity, allowing
 identifying it across whole game during data gathering, and lets us identify
 same entity on multiple images, which is useful for identifying motion
 of entity through different images.
 By calling 
\begin_inset Flex Code
status open

\begin_layout Plain Layout

\family roman
entity.Model.Hash
\end_layout

\end_inset

 on a particular entity, we obtain its hash, which can be used to spawn
 new entities with same model.
 For vehicles, there is 
\begin_inset Flex Code
status open

\begin_layout Plain Layout

\family roman
entity.ClassType
\end_layout

\end_inset

 property, describing type as one of 21 vehicle types.
 Whole list can be seen in the enum VehicleClass in the 
\begin_inset Flex URL
status open

\begin_layout Plain Layout

https://github.com/crosire/scripthookvdotnet/blob/dev_v3/source/scripting/World/E
ntities/Vehicles/Vehicle.cs
\end_layout

\end_inset

.
 
\end_layout

\begin_layout Subsubsection
Gathering data from active camera
\end_layout

\begin_layout Standard
In this part, I'll describe how to access data from internal GPU buffers
 and how to persist these data.
 
\end_layout

\begin_layout Standard
Before we gather buffers contents, we want to pause the game.
 Since we don't get the data from buffers perfectly synchronized, we pause
 the game to make sure RGB, depth and stencil buffer contain data coming
 from same frame.
 
\begin_inset Flex Code
status open

\begin_layout Plain Layout
Game.Pause(true)
\end_layout

\end_inset

 pauses the game.
 The whole data retrieval from buffers is done via GTAVisionExport native
 plugin 
\begin_inset CommandInset citation
LatexCommand cite
key "driving-in-matrix"

\end_inset

.
 When the game is stopped, we can obtain RGB, depth and stencil buffer contents
 by 
\end_layout

\begin_layout LyX-Code
var color = VisionNative.GetColorBuffer();
\end_layout

\begin_layout LyX-Code
var depth = VisionNative.GetDepthBuffer();
\end_layout

\begin_layout LyX-Code
var stencil = VisionNative.GetStencilBuffer(); 
\end_layout

\begin_layout Standard
and then, we save it to the filesystem as TIFF image.
 Although TIFF is not the most used format for data, it is able to persist
 image whose pixels contain float values, which is crucial for depth buffer.
\end_layout

\begin_layout Standard
The RGB buffer is usual 8bit unsigned integer RGBA image.
 The depth buffer contains float values, with 32bits precision, in range
 from 0 to 1, representing depth value in NDC space.
 The stencil buffer contains 8bit unsigned integer image.
 
\end_layout

\begin_layout Section
\begin_inset CommandInset label
LatexCommand label
name "sec:Reverse-engineering-the-RAGE"

\end_inset

Reverse-engineering the RAGE rendering pipeline
\end_layout

\begin_layout Standard
As mentioned above 
\begin_inset CommandInset ref
LatexCommand ref
reference "rage"

\end_inset

, GTA V uses proprietary game engine, Rockstar Advanced Game Engine (RAGE).
 The basic premise of rendering pipeline is same as in well known graphics
 engines like OpenGL.
 The pipeline is shown in figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Visualization-pipeline"

\end_inset

.
 Following section will be discussing mostly computer graphics related problems.
 Due to some terminology inconsistency between computer graphics and computer
 vision, all terms used here will be computer graphics related.
 Probably most confusion here could be caused by projection matrix.
 In computer vision, projection matrix is projection from 3D to 2D, the
 matrix reduces dimension.
 In computer graphics, all coordinates are kept in 4D, in homogeneous coordinate
s as long as possible.
 Here the projection matrix represents projection from frustum seen by eye
 into cuboid space of Normalized Device Coordinates.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename obrazky/transformationPipeline.png
	lyxscale 30
	scale 65

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:Visualization-pipeline"

\end_inset

Rendering pipeline
\end_layout

\end_inset


\end_layout

\end_inset

In is part, I will describe some transformations between individual RAGE
 coordinate systems.
 Some points here will have part of name in lower index.
 The name of coordinate system will be denoted in upper index.
 In RAGE there are 6 coordinate systems.
\end_layout

\begin_layout Standard
\align center
\begin_inset Tabular
<lyxtabular version="3" rows="7" columns="3">
<features tabularvalignment="middle">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<row>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Name
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Abbreviation
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Example point 
\begin_inset Formula $x$
\end_inset


\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Object Coordinates
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
O
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $x^{O}$
\end_inset


\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
World Coordinates
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
W
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $x^{W}$
\end_inset


\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Camera Coordinates
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
C
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $x^{C}$
\end_inset


\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Clip Coordinates
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
L
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $x^{L}$
\end_inset


\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Normalized Device Coordinates
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
NDC
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $x^{NDC}$
\end_inset


\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Windows Coordinates
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
P
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $x^{P}$
\end_inset


\end_layout

\end_inset
</cell>
</row>
</lyxtabular>

\end_inset


\end_layout

\begin_layout Standard
Most of points we handle in GTA already are in world coordinates.
 
\end_layout

\begin_layout Standard
But some points, like GAMEPLAY::GET_MODEL_DIMENSIONS
\end_layout

\begin_layout Standard
\begin_inset Formula $=\begin{pmatrix}x_{max}^{O} & y_{max}^{O} & z_{max}^{O}\end{pmatrix}\begin{pmatrix}x_{min}^{O} & y_{min}^{O} & z_{min}^{O}\end{pmatrix}$
\end_inset

 output, are in object coordinates.
 Transitions between adjacent coordinate systems will be demonstrated on
 model dimensions because it is on the few vectors which are obtained in
 Object Coordinates and there is need to project them into Window Coordinates.
\end_layout

\begin_layout Subsection
Object to World Coordinates
\end_layout

\begin_layout Standard
To get world coordinates of model dimensions, we use traditional rigid body
 transformation based on ENTITY::GET_ENTITY_ROTATION
\begin_inset Formula $=\begin{pmatrix}\alpha & \beta & \gamma\end{pmatrix}$
\end_inset

 Euler angles, 
\end_layout

\begin_layout Standard
and ENTITY::GET_ENTITY_COORDS
\begin_inset Formula $=\begin{pmatrix}x^{W} & y^{W} & z^{W}\end{pmatrix}$
\end_inset

.
\end_layout

\begin_layout Standard
Because all coordinates will be homogeneous coordinates, the above-mentioned
 model dimensions vectors will be transformed to following form 
\begin_inset Formula $\begin{pmatrix}x_{max}^{O} & y_{max}^{O} & z_{max}^{O} & 1\end{pmatrix}\begin{pmatrix}x_{min}^{O} & y_{min}^{O} & z_{min}^{O} & 1\end{pmatrix}$
\end_inset

.
 
\end_layout

\begin_layout Standard
The transition is represented by model matrix
\begin_inset Formula 
\begin{align*}
M & =\begin{bmatrix}1 & 0 & 0 & x^{W}\\
0 & 1 & 0 & y^{W}\\
0 & 0 & 1 & z^{W}\\
0 & 0 & 0 & 1
\end{bmatrix}\begin{bmatrix}1 & 0 & 0 & 0\\
0 & \cos\left(\alpha\right) & -\sin\left(\alpha\right) & 0\\
0 & \sin\left(\alpha\right) & \cos\left(\alpha\right) & 0\\
0 & 0 & 0 & 1
\end{bmatrix}\begin{bmatrix}\cos\left(\beta\right) & 0 & \sin\left(\beta\right) & 0\\
0 & 1 & 0 & 0\\
-\sin\left(\beta\right) & 0 & \cos\left(\beta\right) & 0\\
0 & 0 & 0 & 1
\end{bmatrix}\begin{bmatrix}\cos\left(\gamma\right) & -\sin\left(\gamma\right) & 0 & 0\\
\sin\left(\gamma\right) & \cos\left(\gamma\right) & 0 & 0\\
0 & 0 & 1 & 0\\
0 & 0 & 0 & 1
\end{bmatrix}\\
 & =\begin{bmatrix}\cos\left(\beta\right)\cos\left(\gamma\right) & -\cos\left(\beta\right)\sin\left(\gamma\right) & \sin\left(\beta\right) & x^{W}\\
\sin\left(\alpha\right)\sin\left(\beta\right)\cos\left(\gamma\right)+\cos\left(\alpha\right)\sin\left(\gamma\right) & \cos\left(\alpha\right)\cos\left(\gamma\right)-\sin\left(\alpha\right)\sin\left(\beta\right)\sin\left(\gamma\right) & -\sin\left(\alpha\right)\cos\left(\beta\right) & y^{W}\\
\sin\left(\alpha\right)\sin\left(\gamma\right)-\cos\left(\alpha\right)\sin\left(\beta\right)\cos\left(\gamma\right) & \cos\left(\alpha\right)\sin\left(\beta\right)\sin\left(\gamma\right)+\sin\left(\alpha\right)\cos\left(\gamma\right) & \cos\left(\alpha\right)\cos\left(\beta\right) & z^{W}\\
0 & 0 & 0 & 1
\end{bmatrix}
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
and whole transformation is, as expected 
\begin_inset Formula 
\[
M\begin{bmatrix}x_{max}^{O} & x_{min}^{O}\\
y_{max}^{O} & y_{min}^{O}\\
z_{max}^{O} & z_{min}^{O}\\
1 & 1
\end{bmatrix}=\begin{bmatrix}x_{max}^{W} & x_{min}^{W}\\
y_{max}^{W} & y_{min}^{W}\\
z_{max}^{W} & z_{min}^{W}\\
w_{max}^{W} & w_{min}^{W}
\end{bmatrix}
\]

\end_inset


\end_layout

\begin_layout Subsection
\begin_inset CommandInset label
LatexCommand label
name "subsec:World-to-Camera"

\end_inset

World to Camera Coordinates
\end_layout

\begin_layout Standard
The transformation from world coordinates is principally the same, but counter-i
ntuitive in definition of used rotation matrices.
 It also is rigid body transformation, but rotation is defined differently
 than we are usually used to in computer graphics.
 The rotation matrices were reverse engineered as part of this thesis from
 camera position, rotation and resulting view matrix, this coordinate system
 is nowhere else documented.
 The camera position is CAM::GET_CAM_COORD
\begin_inset Formula $=\begin{pmatrix}x^{W} & y^{W} & z^{W}\end{pmatrix}$
\end_inset

 and the camera rotation is CAM::GET_CAM_ROT
\begin_inset Formula $=\begin{pmatrix}\alpha & \beta & \gamma\end{pmatrix}$
\end_inset

.
\end_layout

\begin_layout Standard
The transformation is represented by view matrix
\begin_inset Formula 
\begin{align*}
V & =\begin{bmatrix}1 & 0 & 0 & 0\\
0 & \sin\left(\alpha\right) & \cos\left(\alpha\right) & 0\\
0 & \cos\left(\alpha\right) & -\sin\left(\alpha\right) & 0\\
0 & 0 & 0 & 1
\end{bmatrix}\begin{bmatrix}\cos\left(\beta\right) & 0 & -\sin\left(\beta\right) & 0\\
0 & 1 & 0 & 0\\
\sin\left(\beta\right) & 0 & \cos\left(\beta\right) & 0\\
0 & 0 & 0 & 1
\end{bmatrix}\begin{bmatrix}\cos\left(\gamma\right) & \sin\left(\gamma\right) & 0 & 0\\
\sin\left(\gamma\right) & -\cos\left(\gamma\right) & 0 & 0\\
0 & 0 & 1 & 0\\
0 & 0 & 0 & 1
\end{bmatrix}\begin{bmatrix}1 & 0 & 0 & x^{W}\\
0 & 1 & 0 & y^{W}\\
0 & 0 & 1 & z^{W}\\
0 & 0 & 0 & 1
\end{bmatrix}
\end{align*}

\end_inset

to fit the matrix into page, let us propose following substitutions 
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\cos\left(\alpha\right)=c_{\alpha},sin\left(\alpha\right)=s_{\alpha}
\]

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\cos\left(\beta\right)=c_{\beta},sin\left(\beta\right)=s_{\beta}
\]

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\cos\left(\gamma\right)=c_{\gamma},sin\left(\gamma\right)=s_{\gamma}
\]

\end_inset


\begin_inset Formula 
\begin{align}
V & =\begin{bmatrix}c_{\beta}c_{\gamma} & c_{\beta}s_{\gamma} & -s_{\beta} & 0\\
c_{\alpha}s_{\beta}c_{\gamma}+s_{\alpha}s_{\gamma} & c_{\alpha}s_{\beta}s_{\gamma}-s_{\alpha}c_{\gamma} & c_{\alpha}c_{\beta} & 0\\
c_{\alpha}s_{\gamma}-s_{\alpha}s_{\beta}c_{\gamma} & -s_{\alpha}s_{\beta}s_{\gamma}-c_{\alpha}c_{\gamma} & -s_{\alpha}c_{\beta} & 0\\
0 & 0 & 0 & 1
\end{bmatrix}\begin{bmatrix}1 & 0 & 0 & x^{W}\\
0 & 1 & 0 & y^{W}\\
0 & 0 & 1 & z^{W}\\
0 & 0 & 0 & 1
\end{bmatrix}\nonumber \\
 & =\begin{bmatrix}c_{\beta}c_{\gamma} & c_{\beta}s_{\gamma} & -s_{\beta} & x^{W}c_{\beta}c_{\gamma}+y^{W}c_{\beta}s_{\gamma}-z^{W}s_{\beta}\\
c_{\alpha}s_{\beta}c_{\gamma}+s_{\alpha}s_{\gamma} & c_{\alpha}s_{\beta}s_{\gamma}-s_{\alpha}c_{\gamma} & c_{\alpha}c_{\beta} & x^{W}\left(c_{\alpha}s_{\beta}c_{\gamma}+s_{\alpha}s_{\gamma}\right)+y^{W}\left(c_{\alpha}s_{\beta}s_{\gamma}-s_{\alpha}c_{\gamma}\right)+z^{W}c_{\alpha}c_{\beta}\\
c_{\alpha}s_{\gamma}-s_{\alpha}s_{\beta}c_{\gamma} & -s_{\alpha}s_{\beta}s_{\gamma}-c_{\alpha}c_{\gamma} & -s_{\alpha}c_{\beta} & x^{W}\left(c_{\alpha}s_{\gamma}-s_{\alpha}s_{\beta}c_{\gamma}\right)+y^{W}\left(-s_{\alpha}s_{\beta}s_{\gamma}-c_{\alpha}c_{\gamma}\right)-z^{W}s_{\alpha}c_{\beta}\\
0 & 0 & 0 & 1
\end{bmatrix}\label{eq:camera-m}
\end{align}

\end_inset


\end_layout

\begin_layout Standard
and whole transformation is, as expected 
\begin_inset Formula 
\[
V\begin{bmatrix}x_{max}^{W} & x_{min}^{W}\\
y_{max}^{W} & y_{min}^{W}\\
z_{max}^{W} & z_{min}^{W}\\
w_{max}^{W} & w_{min}^{W}
\end{bmatrix}=\begin{bmatrix}x_{max}^{C} & x_{min}^{C}\\
y_{max}^{C} & y_{min}^{C}\\
z_{max}^{C} & z_{min}^{C}\\
w_{max}^{C} & w_{min}^{C}
\end{bmatrix}
\]

\end_inset


\end_layout

\begin_layout Standard
From definition of rotation axes in the rotation matrices, following observation
 can be made.
 
\begin_inset Formula $z^{C}$
\end_inset

 represents distance from camera in direction of camera heading, and 
\begin_inset Formula $x^{C}$
\end_inset

and 
\begin_inset Formula $y^{C}$
\end_inset

 represent horizontal and vertical position of point relative to camera,
 respectively.
 But the view frustum of camera is in opposite direction than 
\begin_inset Formula $z^{C}$
\end_inset

axis, which means the camera 
\begin_inset Quotes eld
\end_inset

is looking
\begin_inset Quotes erd
\end_inset

 into negative 
\begin_inset Formula $z^{C}$
\end_inset

 coordinates.
\end_layout

\begin_layout Subsection
\begin_inset CommandInset label
LatexCommand label
name "subsec:Camera-to-NDC"

\end_inset

Camera to NDC
\end_layout

\begin_layout Standard
This is the first transformation which is not rigid-body transformation.
 Because camera sees only frustum, this transformation represents transition
 from frustum to cuboid in Normalized Device Coordinates.
 The frustum being projected is specified by near clip, far clip, field
 of view and screen resolution width and height.
 Usually, none of these parameters are changing during the game, so the
 projection matrix is usually the same for multiple scenes during data gathering
 session.
 Although all of these parameters can be changed programmatically if needed.
 
\end_layout

\begin_layout Standard
The near clip and far clip of camera can be obtained by 
\begin_inset CommandInset label
LatexCommand label
name "native-call-near-clip"

\end_inset

 CAM::GET_CAM_NEAR_CLIP
\begin_inset Formula $=n_{c}$
\end_inset

 and CAM::GET_CAM_FAR_CLIP
\begin_inset Formula $=f_{c}$
\end_inset

.
 Width and height of screen resolution are obtained by GRAPHICS::_GET_ACTIVE_SCR
EEN_RESOLUTION
\begin_inset Formula $=\begin{pmatrix}W & H\end{pmatrix}$
\end_inset

 and field of view of camera by CAM::GET_CAM_FOV
\begin_inset Formula $=\varphi_{VD}$
\end_inset

 in degrees.
 
\begin_inset Formula $\varphi_{VD}$
\end_inset

 in radians will be denoted as 
\begin_inset Formula $\varphi_{VR}$
\end_inset

.
\end_layout

\begin_layout Standard
The near clip and far clip define planes between which the content is being
 rendered.
 Nothing before the near clip and behind the far clip is rendered.
\end_layout

\begin_layout Standard
The field of view 
\begin_inset Formula $\varphi_{VD}$
\end_inset

 is only vertical.
 Horizontal field of view can be calculated from 
\begin_inset Formula $W$
\end_inset

 and 
\begin_inset Formula $H$
\end_inset

 ratio, but currently we don't need it.
\end_layout

\begin_layout Standard
There is important observation, the far clip 
\begin_inset Formula $f_{c}$
\end_inset

 does not figure in the projection matrix at all.
 In the projection matrix, only 
\begin_inset Formula $n_{c}$
\end_inset

 is used.
 Far clip used in projection matrix is non-changing value which can not
 be obtained through Camera native function.
 By reverse-engineering I calculated the value of this new far clip to be
 
\begin_inset Formula $10003.815$
\end_inset

, details of this calculation are covered in experiments
\begin_inset CommandInset ref
LatexCommand ref
reference "subsec:Reverse-engineering"

\end_inset

.
 
\end_layout

\begin_layout Standard
The transformation is represented by projection matrix
\begin_inset Formula 
\begin{align}
P & =\begin{bmatrix}\frac{H}{W\cdot\tan\left(\frac{\varphi_{VR}}{2}\right)} & 0 & 0 & 0\\
0 & \frac{1}{\tan\left(\frac{\varphi_{VR}}{2}\right)} & 0 & 0\\
0 & 0 & \frac{-10003.815}{n_{c}-10003.815} & \frac{-10003.815\cdot n_{c}}{n_{c}-10003.815}\\
0 & 0 & -1 & 0
\end{bmatrix}\label{eq:projection-m}
\end{align}

\end_inset


\end_layout

\begin_layout Standard
So the projection to Clip Coordinates is 
\begin_inset Formula 
\[
P\begin{bmatrix}x_{max}^{C} & x_{min}^{C}\\
y_{max}^{C} & y_{min}^{C}\\
z_{max}^{C} & z_{min}^{C}\\
w_{max}^{C} & w_{min}^{C}
\end{bmatrix}=\begin{bmatrix}x_{max}^{L} & x_{min}^{L}\\
y_{max}^{L} & y_{min}^{L}\\
z_{max}^{L} & z_{min}^{L}\\
w_{max}^{L} & w_{min}^{L}
\end{bmatrix}
\]

\end_inset


\end_layout

\begin_layout Standard
The transition between Clip Coordinates and NDC is only division by width,
 so it is
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\begin{bmatrix}x_{max}^{L} & x_{min}^{L}\\
y_{max}^{L} & y_{min}^{L}\\
z_{max}^{L} & z_{min}^{L}\\
w_{max}^{L} & w_{min}^{L}
\end{bmatrix}\circ\begin{bmatrix}\frac{1}{w_{max}^{L}} & \frac{1}{w_{min}^{L}}\\
\frac{1}{w_{max}^{L}} & \frac{1}{w_{min}^{L}}\\
\frac{1}{w_{max}^{L}} & \frac{1}{w_{min}^{L}}\\
\frac{1}{w_{max}^{L}} & \frac{1}{w_{min}^{L}}
\end{bmatrix}=\begin{bmatrix}x_{max}^{NDC} & x_{min}^{NDC}\\
y_{max}^{NDC} & y_{min}^{NDC}\\
z_{max}^{NDC} & z_{min}^{NDC}\\
1 & 1
\end{bmatrix}
\]

\end_inset

where 
\begin_inset Formula $\circ$
\end_inset

 is Hadamard product, also known as entry-wise product or element-wise matrix
 multiplication.
 
\end_layout

\begin_layout Standard
Let us have vector 
\begin_inset Formula $\boldsymbol{x}=\begin{bmatrix}x & y & z & w\end{bmatrix}^{T}$
\end_inset

in both coordinate systems, 
\begin_inset Formula $\boldsymbol{x}^{L}=\begin{bmatrix}x^{L} & y^{L} & z^{L} & w^{L}\end{bmatrix}^{T}$
\end_inset

, 
\begin_inset Formula $\boldsymbol{x}^{NDC}=\begin{bmatrix}x^{NDC} & y^{NDC} & z^{NDC} & w^{NDC}\end{bmatrix}^{T}$
\end_inset

.
 Then, the relation between Clip Coordinates and NDC can also be expressed
 by following relationship 
\begin_inset Formula 
\[
\boldsymbol{x}^{L}=\begin{bmatrix}x^{L}\\
y^{L}\\
z^{L}\\
w^{L}
\end{bmatrix}=\begin{bmatrix}x^{NDC}w^{L}\\
y^{NDC}w^{L}\\
z^{NDC}w^{L}\\
w^{L}
\end{bmatrix}=w^{L}\begin{bmatrix}x^{NDC}\\
y^{NDC}\\
z^{NDC}\\
1
\end{bmatrix}=w^{L}\boldsymbol{x}^{NDC}
\]

\end_inset


\end_layout

\begin_layout Standard
The view frustum was now transformed into NDC cuboid.
 The NDC cuboid has dimensions 
\begin_inset Formula $x\in\left[-1,1\right],y\in\left[-1,1\right],z\in\left[0,1\right]$
\end_inset

.
 The x and y coordinates are intuitive, but the z-axis is reverted, so near
 clip is being mapped to 1 and far clip is being mapped to 0.
 The NDC is important because it is coordinate space in which GPU operates
 and depth is gathered from GPU in NDC.
 The value of 
\begin_inset Formula $z^{NDC}=0$
\end_inset

 usually belongs to sky.
\end_layout

\begin_layout Standard
The camera divides the camera space to two half-spaces, in front of camera
 
\begin_inset Formula $z^{C}<0$
\end_inset

, and behind camera 
\begin_inset Formula $z^{C}\geq0$
\end_inset

 .
 The projection transformation from camera space to NDC space works correctly
 only for points that belong to half-space 
\begin_inset Formula $z^{C}<0$
\end_inset

.
 For every point in camera space, we can easily verify to which half-space
 it belongs and project only points belonging to the 
\begin_inset Formula $z^{C}<0$
\end_inset

 half-space.
 If we project points behind the camera,
\begin_inset Formula $z^{C}\geq0$
\end_inset

 to the NDC space, they will be mapped into the NDC space as if they were
 in front of camera.
\end_layout

\begin_layout Subsection
NDC to Window Coordinates
\end_layout

\begin_layout Standard
This is the last transformation of the rendering pipeline and only in this
 transformation the dimension reduction happen.
 So far points have been kept in homogeneous coordinates, but window coordinates
 are only 2D, expressing 
\begin_inset Formula $x$
\end_inset

 and 
\begin_inset Formula $y$
\end_inset

 coordinates of pixel where point will be rendered.
 Here, we need only GRAPHICS::_GET_ACTIVE_SCREEN_RESOLUTION
\begin_inset Formula $=\begin{pmatrix}W & H\end{pmatrix}$
\end_inset

 because this transformation depends only on screen with and height.
 
\end_layout

\begin_layout Standard
The transformation matrix is 
\begin_inset Formula 
\begin{align*}
T & =\begin{bmatrix}\frac{W}{2} & 0 & 0 & \frac{W}{2}\\
0 & \frac{-H}{2} & 0 & \frac{H}{2}
\end{bmatrix}
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
so the NDC to screen transformation is
\begin_inset Formula 
\[
T\begin{bmatrix}x_{max}^{NDC} & x_{min}^{NDC}\\
y_{max}^{NDC} & y_{min}^{NDC}\\
z_{max}^{NDC} & z_{min}^{NDC}\\
1 & 1
\end{bmatrix}=\begin{bmatrix}x_{max}^{P} & x_{min}^{P}\\
y_{max}^{P} & y_{min}^{P}
\end{bmatrix}
\]

\end_inset


\end_layout

\begin_layout Standard
Due to the division by width, the pipeline unfortunately can not be expressed
 as matrix multiplication by matrix constant for all points in one scene.
 
\end_layout

\begin_layout Section
\begin_inset CommandInset label
LatexCommand label
name "sec:Datasets-proposal"

\end_inset

VirtualScapes Dataset proposal
\end_layout

\begin_layout Standard
As part of my thesis, I propose two novel synthetic datasets.
 Both of these datasets are outdoor, taken from virtual car.
 Both of these datasets contain outdoor images in all parts of day, dawn,
 day, evening, and night.
 They contain most of data described above, to provide as much information
 as possible for usability in various tasks.
 Both of these datasets contain full HD RGB-D images, stencil images, position
 and rotation of camera, positions, rotations, identifiers and types of
 cars and pedestrians around the camera, projection and view matrix for
 aligning data between different images.
 
\end_layout

\begin_layout Standard
The first dataset, named Closed VirtualScapes, was used for voxel map reconstruc
tion.
 It contains images from 4 virtual cameras attached to driving car, placed
 in circle opposite to each other, mapping space in front of car to create
 its detailed 3D reconstruction.
 There are 8371 scenes, each taken from 4 cameras.
 During this dataset gathering, 
\begin_inset Formula $13059$
\end_inset

 meters were driven in virtual world.
\end_layout

\begin_layout Standard
The second dataset, named Open VirtualScapes, is demonstration of common
 automotive dataset from driving car.
 Compared to real-world datasets, this one has advantage of pixel-wise depth,
 precise on all surfaces and even in high distances, outperforming lidar
 technology in accuracy and depth point density.
 It is directly aligned with pixels of RGB images.
 The datasets consists of 22285 scenes, every of them captured by 4 cameras
 attached to car, heading different direction, as seen in figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Camera-positions-for"

\end_inset

 where camera positions are marked by white cubes.
 During this dataset gathering, 
\begin_inset Formula $34340$
\end_inset

 meters were driven in virtual world.
 Both datasets are publicly available and can be downloaded from 
\begin_inset Flex URL
status open

\begin_layout Plain Layout

http://ptak.felk.cvut.cz/public_datasets/GTA_V/dataset-website/
\end_layout

\end_inset

.
 
\end_layout

\begin_layout Standard
To demonstrate possibilities of synthetic, automatically annotated datasets,
 here are some sample images obtained using my data extraction described
 in 
\begin_inset CommandInset ref
LatexCommand ref
reference "sec:GTA-V-data-obtaining"

\end_inset

 .
 The dataset contains position, rotation, model sizes, identifier and the
 pixel-wise class segmentation of each car.
 With this data, I was able to do the 3D and 2D bounding boxes extraction,
 pixel-wise object segmentation and trajectory tracking.
 All of these images are part of the VirtualScapes dataset and can be automatica
lly reconstructed using annotations included in dataset.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename obrazky/bbox-night.png
	lyxscale 70
	width 12cm

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
3D bounding box - night
\end_layout

\end_inset


\end_layout

\end_inset


\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename obrazky/bbox-day-people.png
	lyxscale 70
	width 12cm

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
3D bounding box - day
\end_layout

\end_inset


\end_layout

\end_inset


\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename obrazky/pixelwise-night.png
	lyxscale 70
	width 12cm

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
pixel-wise annotation of cars
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename obrazky/trajectories-day.png
	lyxscale 70
	width 12cm

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
individual car trajectories
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename obrazky/dataset-2-cameras.png
	lyxscale 40
	width 12cm

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:Camera-positions-for"

\end_inset

Camera positions for second dataset
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Chapter
3D map estimation
\end_layout

\begin_layout Standard
When thinking of 3D map estimation, we can utilize the similarity between
 3D map estimation and the depth estimation, because in depth estimation,
 we predict the distance of nearest object pixel-wise, and in 3D map estimation,
 we have multiple depth levels per pixel and estimate occupancy per each
 depth level.
 The neural network output is cuboid with size of 
\begin_inset Formula $X\times Y\times Z$
\end_inset

 neurons, hence it is useful to represent 3D map of a cuboid as a voxel
 map with X width levels, Y height levels and Z depth levels.
 Then we can estimate occupancy per voxel, in other words, per each neuron
 output.
 Thus we can represent both depth and 3D map estimation as similar instances,
 depth estimation being multi-class occupancy classification per pixel where
 classes are depth bins, and 3D map estimation being occupancy classification
 per voxel, where we predict occupancy for each voxel.
\end_layout

\begin_layout Section
TensorFlow
\end_layout

\begin_layout Standard
In the neural networks research, one of the important factors is the ability
 to easily and efficiently prototype new architecture and train neural network.
 TensorFlow 
\begin_inset CommandInset citation
LatexCommand cite
key "tensorflow2015-whitepaper"

\end_inset

 is a framework for developing and training neural networks, developed by
 Google Inc.
 It is based on declarative programming and the API provides a way to describe
 the computational graph.
 This graph is can be then queried for value of any node in the graph given
 specific input.
 Other advantage of TensorFlow is almost seeming-less transition between
 running the calculation graph on CPU and on GPU which allows to users to
 describe abstract mathematical operations without the need of low-level
 optimizations of calculations on CUDA, everything is taken care of by TensorFlo
w.
 As a powerful tool for introspections of neural network training, TensorBoard
 was developed as a visualization utility for TensorFlow.
 During the training, certain parameters can be logged periodically and
 then visualized in TensorBoard, enabling visualization of cost function,
 metrics, input and output images, or weight histograms in time, which leads
 to more intuitive understanding of issues during solving problems with
 neural networks training.
 
\end_layout

\begin_layout Section
Depth estimation
\end_layout

\begin_layout Standard
As a task preceding the 3D map estimation, I firstly trained neural network
 for depth estimation.
 There are many datasets for depth estimation, but most of them are either
 indoor with relatively precise ground truth depth or outdoor with sparse
 depth labels, like in KITTI dataset 
\begin_inset CommandInset citation
LatexCommand cite
key "kitti"

\end_inset

, or they are in relatively low resolution and loss accuracy in higher distance
 
\begin_inset CommandInset citation
LatexCommand cite
key "make3d-dataset"

\end_inset

, which is natural for real-world depth measurement.
 For training, I use Closed VirtualScapes dataset, synthetic dataset gathered
 from GTA V, described in section 
\begin_inset CommandInset ref
LatexCommand ref
reference "sec:Datasets-proposal"

\end_inset

.
 
\end_layout

\begin_layout Standard
This dataset is already in the form of corresponding RGB and depth images,
 but the depth is in the NDC space so it must first be pixel-wise transformed
 into the camera view space where the depth is in meters.
 We might train network directly in the NDC space, but the depth reconstruction
 learned would be deformed.
 The NDC space is not linearly mapped into the camera view space, in fact,
 it is mapped hyperbolically, according to the visualization pipeline 
\begin_inset CommandInset citation
LatexCommand cite
key "real-time-rendering"

\end_inset

.
 After obtaining the depth in meters, I performed depth binning to obtain
 the depth representation for the classification task.
 Since these data are outdoor, the depth varies from 1 meter to 10km, which
 is the camera far clip and nothing after 10km is rendered.
 Which is still much bigger depth range than in real world datasets.
 Depth estimation of very distant objects is harder than for close ones,
 mainly because their relative size on the image is lot smaller, but also
 small difference in image can mean big depth difference in meters.
 And in most of applications, we are interested only in relatively near
 objects depth estimation.
 This is why the depth range to 50 meters has been used for binning into
 100 depth levels, and one additional depth level was used as the 
\begin_inset Quotes eld
\end_inset

another
\begin_inset Quotes erd
\end_inset

 class, depth out of the range, so network would still classify far away
 pixels as some bin and wouldn't try to assign them some nearer depth level.
 This is why there are 101 channels in the network output.
 Due to this setup, ground truth depth which is used as the correct label
 during training, has all pixels more distant than 50 meters belong to the
 same depth class.
 This is important for visual evaluation of predicted depths.
\end_layout

\begin_layout Standard
In convention in neural networks for image processing each layer has usually
 4 dimensions.
 Those dimensions have semantics (batch size, image height, image width,
 number of channels).
 The batch size is usually omitted in most of notations because it is constant
 in all network.
 Other dimensions change its size in individual layers.
\end_layout

\begin_layout Standard
The basic architecture for depth estimation is based on Li et al.
 
\begin_inset CommandInset citation
LatexCommand cite
key "depth-estimation-hierarchical-fusion-soft-weighting"

\end_inset

, but there are modifications.
 The whole network can be seen in figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Depth-estimation-neural"

\end_inset

.
 The input is 
\begin_inset Formula $320\times240$
\end_inset

 RGB image.
 Bigger images are resized to this size.
 The overall architecture and individual parameters should be visible in
 the figure.
 In each box, the first row is the type of neural network layer, other rows
 specify either its parameters, or next layer.
 Conv means convolutional layer, next row after it specifies its parameters,
 namely kernel, size of last dimension (also sometimes called number of
 channels), stride, and activation function.
 For instance, in the second box, second row, 
\begin_inset Quotes eld
\end_inset

7x7, 64, 2, relu
\begin_inset Quotes erd
\end_inset

 means 7x7 kernel, last 64 output channels, stride 2 and Rectified Linear
 Unit activation function.
 When no activation function is specified, there is no activation function
 and convolutional layer output is directly input to the consecutive layer.
 Most of the network consists of two types of block.
 Resize blocks and non-resize blocks.
 The naming comes from the property of convolutional layers, specifically
 the stride parameter.
 When the stride is 1, width and height dimensions remain unchanged, but
 when the stride is higher, the width and height are resized.
 These blocks are the main building blocks of the ResNet network 
\begin_inset CommandInset citation
LatexCommand cite
key "resnet"

\end_inset

 and we can see the layers for residual mapping in non-resize blocks.
 Non-resize blocks are stacked onto each other multiple times, which is
 denoted in the figure, specifically non-resize blocks are repeated 2, 7,
 35 and 2 times, respectively.
 As depicted in the figure, output of all blocks from the second resize
 block onwards are then concatenated together in the channel dimensions,
 which allows utilization of low-level, mid-level and high-level feature
 is the last layers of the network.
 The input layer is denoted by the green background, output layer by blue
 background, and orange and yellow backgrounds represent layers with dilated
 convolution.
 Orange background is used for blocks whose convolution layers uses dilution
 in rate 2, which means neighbouring neurons in the convolution mask have
 distance=2 from its neighbours, every second neuron is omitted in each
 direction.
 The similar property holds for blocks with yellow background, where the
 dilated convolution has rate 4, which means neighbouring neurons in the
 convolution mask are 4 neurons distant from each other in the previous
 layer.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename obrazky/my-master-network-depth.png
	lyxscale 50
	width 14cm

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:Depth-estimation-neural"

\end_inset

Depth estimation neural network
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
Layers of network which have same architecture as ResNet-152 have been initializ
ed by trained ResNet network.
 For remaining parameters the following holds.
 Weights of neuron inputs have been initialized by Xavier initialization
 from normal distribution, and biases have been initialized to the constant
 value 0.1.
 After each layer, the batch normalization has been used, with 
\begin_inset Formula $\epsilon=10^{-3}$
\end_inset

 and decay
\begin_inset Formula $=0.9997$
\end_inset

.
 During the training, the cost is optimized, but it is hard to compare results
 when different cost is tried only based on visual perception, which is
 why other metrics are being used for evaluation the accuracy of trained
 network.
 For depth estimation, following metrics are reported.
 Accuracy under threshold, mean relative error, root mean squared error,
 mean squared log error, and average 
\begin_inset Formula $\log_{10}$
\end_inset

 error.
 All of them are calculated pixel-wise.They are formally defined as follows.
 
\end_layout

\begin_layout Enumerate
Accuracy under threshold 
\begin_inset Formula $aut\left(\tau\right)=\frac{1}{K}\stackrel[i=1]{K}{\sum}1\left\{ \max\left(\frac{d_{i}^{*}}{d_{i}},\frac{d_{i}}{d_{i}^{*}}\right)<\tau\right\} $
\end_inset


\end_layout

\begin_layout Enumerate
Mean relative error 
\begin_inset Formula $rel=\frac{1}{K}\stackrel[i=1]{K}{\sum}\frac{|d_{i}^{*}-d_{i}|}{d_{i}^{*}}$
\end_inset


\end_layout

\begin_layout Enumerate
Root mean squared error 
\begin_inset Formula $rms=\sqrt{\frac{1}{K}\stackrel[i=1]{K}{\sum}\left(d_{i}^{*}-d_{i}\right)^{2}}$
\end_inset


\end_layout

\begin_layout Enumerate
Root mean squared log error 
\begin_inset Formula $rmslog=\sqrt{\frac{1}{K}\stackrel[i=1]{K}{\sum}\left(\log_{10}\left(d_{i}^{*}\right)-\log_{10}\left(d_{i}\right)\right)^{2}}$
\end_inset


\end_layout

\begin_layout Enumerate
Average 
\begin_inset Formula $\log_{10}$
\end_inset

 error 
\begin_inset Formula $logerr=\frac{1}{K}\stackrel[i=1]{K}{\sum}|\log_{10}\left(d_{i}^{*}\right)-\log_{10}\left(d_{i}\right)|$
\end_inset


\end_layout

\begin_layout Standard
where 
\begin_inset Formula $d_{i}$
\end_inset

 is predicted depth on 
\begin_inset Formula $i$
\end_inset

-th pixel, 
\begin_inset Formula $d_{i}^{*}$
\end_inset

 is the ground truth depth on 
\begin_inset Formula $i$
\end_inset

-th pixel, 
\begin_inset Formula $K$
\end_inset

 is the total number of pixels per sample, and 
\begin_inset Formula $\tau$
\end_inset

 is the threshold for accuracy under threshold.
 The most frequent value is 
\begin_inset Formula $\tau=1.25$
\end_inset

, and this is also used during evaluations of this thesis.
 Other frequently used values are 
\begin_inset Formula $\tau=1.25^{2}$
\end_inset

 and 
\begin_inset Formula $\tau=1.25^{3}$
\end_inset

.
 
\end_layout

\begin_layout Section
3D map estimation
\end_layout

\begin_layout Standard
The 3D map estimation from single RGB image is an abstract task and has
 many possible representations.
 In many tasks, voxel-map has proven to be good representation of the 3D
 space.
 Voxel (volumetric pixel) is the smallest distinguishable unit in 3D space,
 it is 3D analogy of 2D pixel.
 In this scenario, each voxel has one of three states: free, occupied and
 unknown.
 The size of voxel is not generally set and is being chosen to fit particular
 task.
 Too big voxel size causes high loss of information, and too small voxel
 size leads to high amount of voxels, more complicated manipulation with
 the resulting voxelmap and higher size when persisting the voxelmap.
 
\end_layout

\begin_layout Standard
Image which is used as an input for 3D map estimation is representing only
 a frustum in 3D map and does not contain information about space outside
 of this frustum.
 Because of this fact, setup in this thesis focuses only on reconstruction
 of 3D space inside the frustum viewed by the camera taking the image.
 Output of the neural network is 
\begin_inset Formula $X\times Y\times Z$
\end_inset

 cuboid, which will be mapped to the frustum so output of each neuron represents
 point in the frustum.
 Mapping between the the cuboid and frustum is contained in the projection
 matrix 
\begin_inset CommandInset ref
LatexCommand ref
reference "subsec:Camera-to-NDC"

\end_inset

 which is used for the training dataset creation and for reconstruction
 of pointcloud from neural network output during prediction.
 With this setup, the model is predicting occupancy of points sampled from
 the frustum.
 In my setup, the frustum will be up to 25m from camera.
\end_layout

\begin_layout Subsection
Training dataset construction from depth images
\end_layout

\begin_layout Standard
The synthetic dataset created from GTA V contains depth images and camera
 parameters, so there is need to reconstruct the 3D map and sample it to
 create the training dataset.
 For 3D map reconstruction, I gathered data from four cameras with positions
 relative to the driving car, as seen in figure X where each white cube
 represents camera position 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Camera-positions-for-3d"

\end_inset

.
 Totally there are 4 cameras, equally placed on the circle with 8 meters
 circumference.
 This setup will demonstrate the whole process of building 3D frustum map
 from 4 cameras.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename obrazky/dataset-1-cameras.png
	lyxscale 50
	width 12cm

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:Camera-positions-for-3d"

\end_inset

Camera positions for 3d reconstruction
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Graphics
	filename obrazky/2018-05-08--14-15-35--576.jpg
	lyxscale 40
	width 7cm

\end_inset


\begin_inset Graphics
	filename obrazky/2018-05-08--14-15-35--972.jpg
	lyxscale 40
	width 7cm

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Graphics
	filename obrazky/2018-05-08--14-15-36--349.jpg
	lyxscale 40
	width 7cm

\end_inset


\begin_inset Graphics
	filename obrazky/2018-05-08--14-15-36--728.jpg
	lyxscale 40
	width 7cm

\end_inset


\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:extracted-RGB-images"

\end_inset

extracted RGB images from 4 cameras
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Graphics
	filename obrazky/2018-05-08--14-15-35--576-depth.png
	lyxscale 40
	width 7cm

\end_inset


\begin_inset Graphics
	filename obrazky/2018-05-08--14-15-35--972-depth.png
	lyxscale 40
	width 7cm

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Graphics
	filename obrazky/2018-05-08--14-15-36--349-depth.png
	lyxscale 40
	width 7cm

\end_inset


\begin_inset Graphics
	filename obrazky/2018-05-08--14-15-36--728-depth.png
	lyxscale 40
	width 7cm

\end_inset


\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:extracted-depth-images"

\end_inset

extracted depth images from 4 cameras
\end_layout

\end_inset


\end_layout

\end_inset

From these 4 cameras, I gathered 4 RGB and depth images, shown in figures
 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:extracted-RGB-images"

\end_inset

 and 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:extracted-depth-images"

\end_inset

.
 I also gathered camera parameters, namely position, rotation, near clip,
 and field of view.
 With these parameters, depth images can be easily transformed into the
 pointcloud in world coordinate system.
 Let us have 
\begin_inset Formula $\boldsymbol{I}^{1}$
\end_inset

depth image from the 1st camera with with width 
\begin_inset Formula $W$
\end_inset

 and height 
\begin_inset Formula $H$
\end_inset

 pixels.
 Since depth image contains value in NDC space then, 
\begin_inset Formula $\boldsymbol{I}^{1}=\left(d_{i,j}\right)\in\left[0,1\right]^{W\times H}$
\end_inset

 holds, where 
\begin_inset Formula $d_{i,j}$
\end_inset

 is value of pixel with coordinates 
\begin_inset Formula $\left[i,j\right]$
\end_inset

.
 For every pixel, we know its depth value and coordinates in the pixel space,
 thus we can describe it as a point in NDC space
\begin_inset Formula 
\[
\boldsymbol{x}_{i,j}^{NDC}=\begin{bmatrix}x^{NDC}\\
y^{NDC}\\
z^{NDC}\\
1
\end{bmatrix}=\begin{bmatrix}\frac{2i}{W}-1\\
-\left(\frac{2j}{H}-1\right)\\
d_{i,j}\\
1
\end{bmatrix}
\]

\end_inset

, where the 
\begin_inset Formula $\boldsymbol{x}_{i,j}^{NDC}$
\end_inset

 is pixel
\begin_inset Formula $\left[i,j\right]$
\end_inset

 transformed into NDC space.
 The sign change in 
\begin_inset Formula $y^{NDC}$
\end_inset

 is cause by indexing conventions of images, where lowest pixel height is
 in the upper part of image but in NDC space, lowest 
\begin_inset Formula $y^{NDC}$
\end_inset

 value is in the lower part of image.
 This holds because 
\begin_inset Formula $x^{NDC},x^{NDC}\in\left[-1,1\right]$
\end_inset

.
 With this pixel-wise transformation, we can transform each depth image
 
\begin_inset Formula $\boldsymbol{I}^{k},k\in1..4$
\end_inset

 into pointcloud in NDC 
\begin_inset Formula $\boldsymbol{P}_{k}^{NDC}$
\end_inset

.
 By using transformation matrices described in 
\begin_inset CommandInset ref
LatexCommand ref
reference "subsec:World-to-Camera"

\end_inset

 and 
\begin_inset CommandInset ref
LatexCommand ref
reference "subsec:Camera-to-NDC"

\end_inset

 we can transform these pointclouds from different cameras into the same
 world coordinate space.
 Let us have 
\begin_inset Formula $P$
\end_inset

 matrix from 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:projection-m"

\end_inset

 and 
\begin_inset Formula $V_{k}$
\end_inset

 matrix from 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:camera-m"

\end_inset

 denoting the view matrix of 
\begin_inset Formula $k$
\end_inset

-th camera, since these matrices are regular, we can do the transformation
 
\begin_inset Formula 
\[
\boldsymbol{P}_{k}^{W}=V_{k}^{-1}P^{-1}\boldsymbol{P}_{k}^{NDC}\forall k\in1..4
\]

\end_inset

.
 Then, all pointclouds are merged into one pointcloud 
\begin_inset Formula $\boldsymbol{P}^{W}$
\end_inset

 of whole scene seen from 4 cameras
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\boldsymbol{P}^{W}=\underset{k\in1..4}{\bigcup}\boldsymbol{P}_{k}^{W}
\]

\end_inset

.
 The resulting pointcloud can be seen in figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Merged-pointcloud-from"

\end_inset

.
 In this setup, all images are nearly Full HD, specifically 
\begin_inset Formula $1920\times1057$
\end_inset

.
 This leads to pointcloud of size 
\begin_inset Formula $1920\cdot1057=2029440\thickapprox2M$
\end_inset

 points.
 Most of these points are near camera, in unnecessarily high density for
 this application.
 To decrease the size of pointclouds and the time to process, them I clustered
 them into 12cm big voxels and sampled only 1 point per voxel.
 This sub-sampling decreases the pointcloud size from 
\begin_inset Formula $\sim2M$
\end_inset

 points into 
\begin_inset Formula $\sim80k$
\end_inset

 to 
\begin_inset Formula $\sim120k$
\end_inset

 points, which is 4% to 6% of original size.
 This sub-sampling is performed per depth image so we can easily link each
 point in pointcloud with camera it is seen from which is crucial for building
 occupancy voxelmap.
 The far clip is over 10000 
\begin_inset CommandInset ref
LatexCommand ref
reference "subsec:Reverse-engineering"

\end_inset

 and thus the maximum distance from camera is over 10km.
 For reconstruction of view frustum to 25m from camera, this is unnecessarily
 large and makes further calculation complicated which is why all depth
 levels further than 30m were projected into 30m distance before further
 transformed into pointcloud.
 This transformation does not affect the space near camera but lowers space
 needed for representation of the pointcloud during the voxelmap occupancy
 calculation.
 Merged pointcloud after projecting distant points into the 30m distance
 from their camera can be seen in figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Pointcloud-before-and"

\end_inset

 where I compare pointclouds before and after aforementioned sub-sampling.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename obrazky/pcl-orig-1.png
	lyxscale 20
	width 15cm

\end_inset


\end_layout

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename obrazky/pcl-orig-2.png
	lyxscale 20
	width 15cm

\end_inset


\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:Merged-pointcloud-from"

\end_inset

Merged pointcloud from all cameras
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename obrazky/pcl-full-2.png
	lyxscale 30
	width 15cm

\end_inset


\end_layout

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename obrazky/pcl-sub-2.png
	lyxscale 30
	width 15cm

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:Pointcloud-before-and"

\end_inset

Pointcloud before and after sub-sampling
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
After this projection and sub-sampling, I have everything prepared for building
 a occupancy voxelmap .
 Voxels in this setup are 25cm big, and each voxel is marked either as occupied,
 free, or unknown.
 The resulting occupancy voxelmap is depicted in figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Occupied-voxels-in"

\end_inset

.
 The last part of the processing is the sampling of the view frustum from
 this occupancy voxelmap.
 This is directly mapped to the output of the neural network.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename obrazky/occupied-pcl-thicc-3.png
	lyxscale 40
	width 14cm

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:Occupied-voxels-in"

\end_inset

Occupied voxels in occupancy voxelmap
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename obrazky/occupied-frustum-with-camera-3.png
	lyxscale 40
	width 14cm

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:Occupied-samples-in"

\end_inset

Occupied samples in frustum.
 Camera is shown by white tetrahedron.
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Metrics and network setup
\end_layout

\begin_layout Standard
Although the 3D map estimation is similar to the depth estimation by the
 problem setup, it is more difficult since we try to infer occupancy of
 occluded voxels, which was not an issue in depth prediction.
 Similarly to the depth predictions, metrics for comparing different models
 and optimizations of different loss functions are used.
 Compared to the depth estimation where metrics were being calculated on
 images and based on difference between predicted and ground truth depth,
 for 3D map estimation the classification based metrics are used since this
 problem is by definition classification problem.
 
\end_layout

\begin_layout Standard
In this setup of voxel-wise classification, we have 3 classes.
 Occupied, free, and unknown voxels.
 Even for the setup of synthesizing 3D occupancy grids, occupancy of some
 voxels is unknown.
 For instance, this happens for car interior, occluded when viewed from
 all cameras and for space below the road.
 During both training and validation, only known voxels are used for loss
 calculation and metrics calculation.
 Let us recapitulate usual terms used for classification problems with respect
 to this particular problem.
 Voxel is false positive, if it is classified as obstacle, but is free in
 ground truth.
 True positive voxel contains obstacle in ground truth and is correctly
 classified as an obstacle.
 True negative voxel is voxel without obstacle, a.k.a free voxel in ground
 truth classified as free voxel in the prediction.
 And false negative voxel is free in ground truth and in prediction.
 So number of voxels in individual categories, which is then used in metrics,
 can be expressed as
\end_layout

\begin_layout Standard
\align center
\begin_inset Tabular
<lyxtabular version="3" rows="3" columns="3">
<features tabularvalignment="middle">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<row>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
obstacle in ground truth
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
free in ground truth
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
obstacle in prediction
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $tp=\stackrel[i=1]{K}{\sum}1\left\{ obst\left(M_{i}\right)\land obst\left(M_{i}^{*}\right)\right\} $
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $fp=\stackrel[i=1]{K}{\sum}1\left\{ obst\left(M_{i}\right)\land free\left(M_{i}^{*}\right)\right\} $
\end_inset


\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
free in prediction
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $fn=\stackrel[i=1]{K}{\sum}1\left\{ free\left(M_{i}\right)\land obst\left(M_{i}^{*}\right)\right\} $
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $tn=\stackrel[i=1]{K}{\sum}1\left\{ free\left(M_{i}\right)\land free\left(M_{i}^{*}\right)\right\} $
\end_inset


\end_layout

\end_inset
</cell>
</row>
</lyxtabular>

\end_inset


\end_layout

\begin_layout Standard
where 
\begin_inset Formula $obst\left(\cdot\right)$
\end_inset

 is true for occupied voxels and 
\begin_inset Formula $free\left(\cdot\right)$
\end_inset

 is true for free voxels.
\end_layout

\begin_layout Standard
Following metrics are used:
\end_layout

\begin_layout Enumerate
False positive rate 
\begin_inset Formula $fpr=\frac{fp}{fp+tn}$
\end_inset


\end_layout

\begin_layout Enumerate
True positive rate 
\begin_inset Formula $fpr=\frac{tp}{fn+tp}$
\end_inset


\end_layout

\begin_layout Enumerate
Intersection over union 
\begin_inset Formula $iou=\frac{\stackrel[i=1]{K}{\sum}1\left\{ obstacle\left(M_{i}\right)\land obstacle\left(M_{i}^{*}\right)\right\} }{\stackrel[j=1]{K}{\sum}1\left\{ obstacle\left(M_{j}\right)\lor obstacle\left(M_{j}^{*}\right)\right\} }$
\end_inset


\end_layout

\begin_layout Enumerate
L1 distance on known voxels 
\begin_inset Formula $dist=\stackrel[i=1]{K}{\sum}1\left\{ known\left(M_{i}^{*}\right)\right\} \cdot|M_{i}-M_{i}^{*}|$
\end_inset


\end_layout

\begin_layout Standard
where 
\begin_inset Formula $M_{i}$
\end_inset

 is 
\begin_inset Formula $i$
\end_inset

-th voxel in predicted voxelmap 
\begin_inset Formula $M$
\end_inset

, 
\begin_inset Formula $M_{i}^{*}$
\end_inset

 is 
\begin_inset Formula $i$
\end_inset

-th voxel in ground truth voxelmap 
\begin_inset Formula $M^{*}$
\end_inset

, 
\begin_inset Formula $K$
\end_inset

 is number of voxels in voxelmap, 
\begin_inset Formula $known\left(\cdot\right)$
\end_inset

 is predicate true for free or obstacle voxels.
 
\end_layout

\begin_layout Standard
Also the loss function is different from depth estimation.
 The weighted logistic loss is used.
 This is voxel-wise, because there can be multiple occupied voxels per pixel
 and thus does not make sense to use the loss function from depth estimation.
 The loss function is same as in Zimmermann et al.
 
\begin_inset CommandInset citation
LatexCommand cite
key "zimmerman-active"

\end_inset

 and is as follows
\begin_inset Formula 
\begin{equation}
L=-\left[\stackrel[i=1]{K}{\sum}w_{i}\log\left(1+\exp\left\{ -M_{i}M_{i}^{*}\right\} \right)\right]\label{eq:logistic-loss}
\end{equation}

\end_inset

where 
\begin_inset Formula $M_{i}$
\end_inset

 is value of 
\begin_inset Formula $i$
\end_inset

-th voxel in predicted voxelmap, 
\begin_inset Formula $M_{i}^{*}$
\end_inset

 is value of 
\begin_inset Formula $i$
\end_inset

-th voxel in ground truth voxelmap and 
\begin_inset Formula $w_{i}$
\end_inset

 is weight of 
\begin_inset Formula $i$
\end_inset

-th voxel.
 The value of voxel in ground truth voxelmap is as follows: 
\begin_inset Formula $M_{i}^{*}=1$
\end_inset

 for voxels with obstacles, 
\begin_inset Formula $M_{i}^{*}=-1$
\end_inset

 for free voxels.
 The weight 
\begin_inset Formula $w_{i}$
\end_inset

 serves two purposes.
 The first purpose is masking out unknown voxels.
 The second purpose is to solve the imbalanced classes issue.
 There are more free voxels than occupied voxels, thus minimizing the sum
 per all voxels would lead to favouring more frequent voxels.
 In this case, that would be free voxels and thus it could learn to ignore
 some obstacles.
 The weighting modifies the loss so all weights sum to 1 and weights per
 each class sum to 
\begin_inset Formula $\frac{1}{2}$
\end_inset

.
 The weight 
\begin_inset Formula $w_{i}$
\end_inset

 for 
\begin_inset Formula $i$
\end_inset

-th voxel is defined as follows.
 
\begin_inset Formula $w_{i}=0$
\end_inset

 if 
\begin_inset Formula $i$
\end_inset

-th voxel in ground truth is unknown.
 
\begin_inset Formula $w_{i}=\frac{1}{2\cdot\#\,free\,voxels}$
\end_inset

 if 
\begin_inset Formula $i$
\end_inset

-th voxel is free and 
\begin_inset Formula $w_{i}=\frac{1}{2\cdot\#\,occupied\,voxels}$
\end_inset

 if 
\begin_inset Formula $i$
\end_inset

-th voxel is occupied.
 
\end_layout

\begin_layout Chapter
Experiments
\end_layout

\begin_layout Section
\begin_inset CommandInset label
LatexCommand label
name "subsec:Reverse-engineering"

\end_inset

Reverse engineering the true Far Clip
\end_layout

\begin_layout Standard
For reverse engineering the far clip, I gathered 33293 screenshots with
 parameters for projection matrix reconstruction and projection matrices.
 Because during whole data gathering none of the parameters used to reconstruct
 projection matrix, was changed, the projection matrix should be same for
 all records.
 As mentioned in 
\begin_inset CommandInset ref
LatexCommand ref
reference "subsec:Camera-to-NDC"

\end_inset

 parameters for reconstructing the Projection matrix are near clip, far
 clip, screen width, screen height and field of view.
\end_layout

\begin_layout Standard
The screenshot contain both RGB images and depth buffer from GPU.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename obrazky/2018-03-30--06-00-56--114.jpg
	lyxscale 40
	scale 20

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:Example-of-RGB"

\end_inset

Example of RGB image
\end_layout

\end_inset


\end_layout

\end_inset


\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename obrazky/2018-03-30--06-00-56--114-depth-8bit-rescaled.png
	lyxscale 40
	scale 20

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:Example-of-depth"

\end_inset

Example of depth buffer
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
The projection matrix transforms frustum into cuboid.
 Open frameworks have publicly available projection matrices, but RAGE does
 not have publicly available any information about projection matrix, so
 in order to obtain true far clip, I needed to reverse-engineer the mathematical
 description of the projection matrix.
 For approximate estimation of projection matrix parameters, I used DirectX
 projection matrix
\begin_inset CommandInset citation
LatexCommand cite
key "real-time-rendering"

\end_inset

 as a starting point for analysis, because GTA V requires DirectX, so I
 assumed it is underlying framework of RAGE.
\end_layout

\begin_layout Standard
The DirectX projection matrix is 
\begin_inset Formula 
\[
P^{DirectX}=\begin{bmatrix}\frac{2n}{r-l} & 0 & -\frac{r+l}{r-l} & 0\\
0 & \frac{2n}{t-b} & -\frac{t+b}{t-b} & 0\\
0 & 0 & \frac{f}{f-n} & -\frac{fn}{f-n}\\
0 & 0 & 1 & 0
\end{bmatrix}
\]

\end_inset


\end_layout

\begin_layout Standard
where 
\begin_inset Formula $n$
\end_inset

 is near clip, 
\begin_inset Formula $f$
\end_inset

 is far clip, 
\begin_inset Formula $l$
\end_inset

 and 
\begin_inset Formula $r$
\end_inset

 determine distance between left and right planes of the frustum and 
\begin_inset Formula $t$
\end_inset

 and 
\begin_inset Formula $b$
\end_inset

 determine distance between top and bottom planes.
\end_layout

\begin_layout Standard
The view frustum is symmetric, so 
\begin_inset Formula $r=-l$
\end_inset

 and 
\begin_inset Formula $t=-b$
\end_inset

 
\begin_inset CommandInset citation
LatexCommand cite
key "real-time-rendering"

\end_inset

.
 In that case, the projection matrix is simplified to form
\begin_inset Formula 
\[
P^{formal}=\begin{bmatrix}\frac{2n}{r+r} & 0 & -\frac{r-r}{r+r} & 0\\
0 & \frac{2n}{t+t} & -\frac{t-t}{t+t} & 0\\
0 & 0 & \frac{f}{f-n} & -\frac{fn}{f-n}\\
0 & 0 & 1 & 0
\end{bmatrix}=\begin{bmatrix}\frac{n}{r} & 0 & 0 & 0\\
0 & \frac{n}{t} & 0 & 0\\
0 & 0 & \frac{f}{f-n} & -\frac{fn}{f-n}\\
0 & 0 & 1 & 0
\end{bmatrix}
\]

\end_inset

The DirectX maps near clip to 0 and far clip to 1, but from data, where
 obviously
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Example-of-depth"

\end_inset

 nearer pixels had higher value in depth buffer than pixel more far from
 camera, I concluded that near and far clip are being mapped to 1 and 0,
 respectively.
 The far clip being mapped to 0 can also be deduced by pixels for sky having
 0 value.
\end_layout

\begin_layout Standard
Due to this fact, we switch the near and clip in the matrix formal description
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
P^{formal}=\begin{bmatrix}\frac{f}{r} & 0 & 0 & 0\\
0 & \frac{f}{t} & 0 & 0\\
0 & 0 & \frac{n}{n-f} & -\frac{fn}{n-f}\\
0 & 0 & 1 & 0
\end{bmatrix}
\]

\end_inset


\end_layout

\begin_layout Standard
The example in 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Example-of-depth"

\end_inset

 does not have actual depth buffer values, but instead, it is rescaled visualiza
tion.
 Since the depth buffer pixels are in range 
\begin_inset Formula $\left[0,1\right]$
\end_inset

 and PNG images take unsigned 8bit integer, this image is mapped linearly
 from 
\begin_inset Formula $\left[0,1\right]$
\end_inset

 to 
\begin_inset Formula $\left[0,255\right]$
\end_inset

.
 Since even the nearest pixels were distant from near clip and real range
 of pixels in this image was 
\begin_inset Formula $\left[0,19\right]$
\end_inset

, I rescaled it 10 times to range 
\begin_inset Formula $\left[0,190\right]$
\end_inset

, so the depth is visible.
\end_layout

\begin_layout Standard
At first, I assumed the camera near clip and far clip obtained by native
 calls 
\begin_inset CommandInset ref
LatexCommand ref
reference "native-call-near-clip"

\end_inset

 and the projection matrix is same as in DirectX.
 
\end_layout

\begin_layout Standard
The near clip and far clip calculation can be demonstrated on image 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Example-of-RGB"

\end_inset

.
 
\end_layout

\begin_layout Standard
By calling CAM::GET_CAM_NEAR_CLIP
\begin_inset Formula $=n_{c}$
\end_inset

 and CAM::GET_CAM_FAR_CLIP
\begin_inset Formula $=f_{c}$
\end_inset

 I obtained values 
\begin_inset Formula $n_{c}=1.5$
\end_inset

 and 
\begin_inset Formula $f_{c}=800$
\end_inset

.
 I also obtained projection matrix calculated by method described in 
\begin_inset CommandInset ref
LatexCommand ref
reference "subsec:Rendering-pipeline-data"

\end_inset

, which is 
\begin_inset Formula 
\[
P^{real}=\begin{bmatrix}1.210067 & 0 & 0 & -0.000004\\
0 & 2.144507 & 0 & 0.000002\\
0 & 0 & 0.00015 & 1.500225\\
0 & 0 & -1 & 0
\end{bmatrix}
\]

\end_inset


\end_layout

\begin_layout Standard
In the formalization of the matrix, 
\begin_inset Formula $P_{formal}$
\end_inset

, there are 4 variables.
 
\begin_inset Formula $r$
\end_inset

 and 
\begin_inset Formula $t$
\end_inset

 appear only in one element of matrix, so they can be verified only after
 reverse engineering the far clip.
 From the 
\begin_inset Formula $P_{2,2}^{formal}$
\end_inset

 and 
\begin_inset Formula $P_{2,3}^{formal}$
\end_inset

, I can calculate the near and far clip by 
\begin_inset Formula 
\begin{align*}
P_{2,2}^{formal} & =\frac{n}{n-f}\\
nP_{2,2}^{formal}-fP_{2,2}^{formal} & =n\\
\frac{n\left(P_{2,2}^{formal}-1\right)}{P_{2,2}^{formal}} & =f
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align*}
P_{2,3}^{formal} & =-\frac{fn}{n-f}\\
P_{2,3}^{formal}\left(n-f\right) & =-fn\\
nP_{2,3}^{formal} & =f\left(P_{2,3}^{formal}-n\right)\\
\frac{nP_{2,3}^{formal}}{\left(P_{2,3}^{formal}-n\right)} & =f
\end{align*}

\end_inset


\begin_inset Formula 
\begin{align*}
\frac{nP_{2,3}^{formal}}{\left(P_{2,3}^{formal}-n\right)} & =\frac{n\left(P_{2,2}^{formal}-1\right)}{P_{2,2}^{formal}}\\
P_{2,3}^{formal}P_{2,2}^{formal} & =\left(P_{2,2}^{formal}-1\right)\left(P_{2,3}^{formal}-n\right)\\
\frac{P_{2,3}^{formal}P_{2,2}^{formal}}{P_{2,2}^{formal}-1} & =P_{2,3}^{formal}-n\\
P_{2,3}^{formal}-\frac{P_{2,3}^{formal}P_{2,2}^{formal}}{P_{2,2}^{formal}-1} & =n\\
-\frac{P_{2,3}^{formal}}{P_{2,2}^{formal}-1} & =n
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align*}
\frac{\left(-\frac{P_{2,3}^{formal}}{P_{2,2}^{formal}-1}\right)\left(P_{2,2}^{formal}-1\right)}{P_{2,2}^{formal}} & =f\\
-\frac{P_{2,3}^{formal}}{P_{2,2}^{formal}} & =f
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
From these calculations, we can calculate near and far clip as 
\begin_inset Formula 
\begin{align*}
n & =-\frac{P_{2,3}^{formal}}{P_{2,2}^{formal}-1}=-\frac{1.500225}{0.00015-1}=1.500225\\
f & =-\frac{P_{2,3}^{formal}}{P_{2,2}^{formal}}=-\frac{1.500225}{0.00015}=-10001.5
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
From these calculations we can see the third column of the projection matrix
 has incorrect sign, because the 
\begin_inset Formula $P_{3,2}^{formal}$
\end_inset

 should be 1 and instead it is -1, and the far clip is negative, which should
 not be.
 When changing signs of third column of projection matrix, we obtain following
 formal definition of projection matrix.
 That sign switching means the view frustum is in opposite direction of
 Z axis.
\begin_inset Formula 
\[
P^{formal}=\begin{bmatrix}\frac{f}{r} & 0 & 0 & 0\\
0 & \frac{f}{t} & 0 & 0\\
0 & 0 & -\frac{n}{n-f} & -\frac{fn}{n-f}\\
0 & 0 & -1 & 0
\end{bmatrix}
\]

\end_inset


\end_layout

\begin_layout Standard
After fixing the sign issue, the relationship between 
\begin_inset Formula $P^{formal}$
\end_inset

 and clips is
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align*}
\frac{P_{2,3}^{formal}}{P_{2,2}^{formal}+1} & =n=\frac{1.500225}{0.00015+1}=1.499999\\
\frac{P_{2,3}^{formal}}{P_{2,2}^{formal}} & =f=\frac{1.500225}{0.00015}=10001.5
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
As we can see, the 
\begin_inset Formula $n=1.499999\approx n_{c}=1.5$
\end_inset

 so for near clip, we can say we successfully reverse-engineered the relation
 between the projection matrix and the near clip.
 The far clip, on the other hand, differs 
\begin_inset Formula $f=10001.5\neq f_{c}=800$
\end_inset

.
 The difference is very high, which lead us to assumption that there is
 some new far clip, which is not same as obtained through API, 
\begin_inset Formula $f_{c}$
\end_inset

.
 
\end_layout

\begin_layout Standard
The other check we can perform is projecting points laying on near clip
 and far clip into NDC space.
 
\end_layout

\begin_layout Standard
We prepare two points.
 Because of many zero elements in 
\begin_inset Formula $P^{formal}$
\end_inset

, we can see 
\begin_inset Formula $x$
\end_inset

-axis and 
\begin_inset Formula $y$
\end_inset

-axis don't affect the 
\begin_inset Formula $z$
\end_inset

-axis of projected point.
 Thus I prepared two points:
\begin_inset Formula 
\[
\begin{bmatrix}1 & 1\\
1 & 1\\
-1.5 & -800\\
1 & 1
\end{bmatrix}
\]

\end_inset

, which are laying on the near clip and far clip, respectively.
 We would assume that they would be mapped to 1 and 0, respectively.
 The negative sign is here because in RAGE, the camera view frustum is in
 negative part of Z axis.
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\begin{bmatrix}1.210067 & 0 & 0 & -0.000004\\
0 & 2.144507 & 0 & 0.000002\\
0 & 0 & 0.00015 & 1.500225\\
0 & 0 & -1 & 0
\end{bmatrix}\begin{bmatrix}1 & 1\\
1 & 1\\
-0.15 & -800\\
1 & 1
\end{bmatrix}=\begin{bmatrix}1.210063 & 1.210063\\
2.144509 & 2.144509\\
1.5 & 1.380225\\
1.5 & 800
\end{bmatrix}
\]

\end_inset

by normalization we obtain 
\begin_inset Formula 
\[
\begin{bmatrix}\frac{1.210063}{1.5} & \frac{1.210063}{800}\\
\frac{2.144509}{1.5} & \frac{2.144509}{800}\\
\frac{1.50045}{1.5} & \frac{1.620225}{800}\\
\frac{1.5}{1.5} & \frac{800}{800}
\end{bmatrix}=\begin{bmatrix}0.80670867 & 0.00151258\\
1.42967267 & 0.00268064\\
1 & 0.00172528\\
1 & 1
\end{bmatrix}
\]

\end_inset

from which we can see the near clip 
\begin_inset Formula $n_{c}$
\end_inset

 is being projected correctly, but far clip 
\begin_inset Formula $f_{c}$
\end_inset

 is not being projected into 0 and that true far clip 
\begin_inset Formula $f$
\end_inset

 is behind this far flip 
\begin_inset Formula $f_{c}$
\end_inset

.
 These calculations give us some insight into projection matrix and its
 role in far clip estimation, but for more robust estimate, I analysed all
 33293 matrices.
 In GTA, matrices are not gathered correctly every time and in some cases,
 resulting matrices are unusable.
 Because I knew the near clip precisely, I discarded all matrices, with
 calculated near clip with difference from real near clip 
\begin_inset Formula $>10^{-4}$
\end_inset

.
 For the rest of matrices, I calculated near clip and far clip for each
 of them.
 In the figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Frequencies-of-"

\end_inset

 can be seen histograms of these calculated far clip and near clip for each
 matrix, respectively.
 Even near clip values, which we know precisely, differ, but with very little
 variance, in range 
\begin_inset Formula $\left[1.5003,1.50048\right]$
\end_inset

 which means difference from true near clip in range 
\begin_inset Formula $\left[3\cdot10^{-4},4.8\cdot10^{-4}\right]$
\end_inset

 which is simply explained by numerical instability.
 Both histograms have logarithmic y scale, so we easily see which value
 occurs more frequently.
 The value 
\begin_inset Formula $-10003.814$
\end_inset

 occurred most frequently in matrices, one and half order of magnitude more
 frequently than second frequent value, and this value also corresponds
 to the median of all calculated far clips.
 Thus this far clip 
\begin_inset Formula $F=-10003.814$
\end_inset

 has been used in all later experiments as a true far clip.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename obrazky/far-clip-f.png
	width 6cm

\end_inset


\begin_inset Graphics
	filename obrazky/far-clip-n.png
	width 6cm

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:Frequencies-of-"

\end_inset

Frequencies of 
\begin_inset Formula $P_{2,2}$
\end_inset

 and 
\begin_inset Formula $P_{2,3}$
\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Section
Depth estimation
\end_layout

\begin_layout Standard
For all experiments, the same network architecture has been used, only hyper-par
ameters were being tuned.
 The initial experiments started with 201 depth bins, as used in 
\begin_inset CommandInset citation
LatexCommand cite
key "depth-estimation-hierarchical-fusion-soft-weighting"

\end_inset

 but with additional depth bin for more distant depth values.
 But the network failed to learn in this setup.
 The cost increased exponentially instead of decreasing, as can be seen
 in figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Failing-to-learn"

\end_inset

.
 The cost function which was being minimized in this setup was the multinomial
 loss, as described in 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:loss-multinomial"

\end_inset

.
 With time, cost function was increasing.
 In the run denoted by light blue colour, only 40 training images were fed
 into the network, to try if it is able to remember patterns, without any
 requirement of generalization.
 As can be seen, even in this setup network failed to learn.
 All runs are runs with different learning rates, in expectation that lower
 learning rate would prevent this cost explosion, but it did not seem to
 have any effect at all.
 The training and validation data from dataset were split in 80/20 ratio.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Graphics
	filename obrazky/initial-learn-fail.PNG
	lyxscale 50
	width 14cm

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:Failing-to-learn"

\end_inset

Failing to learn
\end_layout

\end_inset


\end_layout

\end_inset

After reducing the output number of depth levels to 101, 100 depth levels
 and one remaining level for all depth values outside the binned range,
 network started to learn, as.
 The training was then run in multiple setups, to find the best predictor.
 9 setups will be discussed.
 Setups parameters can be seen in the table 
\begin_inset CommandInset ref
LatexCommand ref
reference "tab:Depth-estimation-setups"

\end_inset

.
 As a loss function, the information gain based loss in 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:loss-infogain"

\end_inset

 has been used.
 The initial learning rate is same for all setups.
 The Nadam optimizer is abbreviation for Adam optimizer with Nesterov momentum.
 For all runs using Adam and Nadam optimizers, only epsilon parameter was
 tuned, beta1 and beta2 parameters were constant, beta1=
\begin_inset Formula $0.9$
\end_inset

, beta2=
\begin_inset Formula $0.999$
\end_inset

.
 The SGDN is abbreviation for the stochastic gradient descent with Nesterov
 momentum.The initial learning rate was set to 
\begin_inset Formula $10^{-4}$
\end_inset

for all runs.
 For setups 1 to 6, learning rate decay has been used.
 The learning rate decay is staircase, after a period given in 
\begin_inset Quotes eld
\end_inset

decay after
\begin_inset Quotes erd
\end_inset

 column the learning rate is divided by 10.
 For runs 7 to 9, learning rate remained unchanged for all training.
 Networks were trained on different GPUs, namely, Nvidia Titan Xp, Titan
 X, and Nvidia GTX 1080Ti.
 Due to different computation capability of these cards and different number
 of CUDA cores, run times are not highly correlated with number of iterations.
\end_layout

\begin_layout Standard
\begin_inset Float table
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Tabular
<lyxtabular version="3" rows="10" columns="5">
<features tabularvalignment="middle">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top" width="0pt">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<row>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
name
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
optimizer
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
decay after
\end_layout

\end_inset
</cell>
<cell multirow="3" alignment="center" valignment="middle" topline="true" bottomline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
training time
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
iterations
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
setup 1
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Adam(epsilon=
\begin_inset Formula $10^{-8}$
\end_inset

)
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $2\cdot10^{4}$
\end_inset

 steps
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
58h
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
237k
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
setup 2
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Adam(epsilon=
\begin_inset Formula $10^{-8}$
\end_inset

)
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $3\cdot10^{4}$
\end_inset

 steps
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
26h
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
164k
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
setup 3
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Nadam(epsilon=
\begin_inset Formula $10^{-8}$
\end_inset

)
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $3\cdot10^{4}$
\end_inset

 steps
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
26h
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
105k
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
setup 4
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Adam(epsilon=
\begin_inset Formula $10^{-5}$
\end_inset

)
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $3\cdot10^{4}$
\end_inset

 steps
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
26h
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
175k
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
setup 5
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Nadam(epsilon=
\begin_inset Formula $10^{-5}$
\end_inset

)
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $3\cdot10^{4}$
\end_inset

 steps
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
30h
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
202k
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
setup 6
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Nadam(epsilon=
\begin_inset Formula $10^{-2}$
\end_inset

)
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $3\cdot10^{4}$
\end_inset

 steps
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
30h
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
189k
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
setup 7
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Nadam(epsilon=
\begin_inset Formula $10^{-8}$
\end_inset

)
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
no decay
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
30h
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
120k
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
setup 8
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
SGDN(momentum=
\begin_inset Formula $0.999$
\end_inset

)
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
no decay
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
26h
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
184k
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
setup 9
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
SGDN(momentum=
\begin_inset Formula $0.9$
\end_inset

)
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
no decay
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
26h
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
108k
\end_layout

\end_inset
</cell>
</row>
</lyxtabular>

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "tab:Depth-estimation-setups"

\end_inset

Depth estimation setups
\end_layout

\end_inset


\end_layout

\end_inset

Metrics for depth estimation and loss have been calculated for the validation
 dataset on these 9 setups, they can be seen in table 
\begin_inset CommandInset ref
LatexCommand ref
reference "tab:Depth-estimation-metrics"

\end_inset

.
 The accuracy under threshold 
\begin_inset Formula $1.25$
\end_inset

 (
\begin_inset Formula $aut\left(1.25\right)$
\end_inset

) is in range 
\begin_inset Formula $\left[0,1\right]$
\end_inset

 by definition.
 0 accuracy means depth in not pixel is in range from 
\begin_inset Formula $\frac{1}{1.25}=0.5$
\end_inset

 to 
\begin_inset Formula $1.25$
\end_inset

 of ground truth depth, 1 accuracy means all pixels are in 
\begin_inset Formula $0.8$
\end_inset

 to 
\begin_inset Formula $1.25$
\end_inset

 of ground truth depth.
 All these calculations are done on reconstructed images, meaning higher
 accuracy means better models.
 Error metrics measure error rate and thus lower error means better model.
 Some metrics highly correlate with each other, showing they describe similar
 information.
 From accuracy under threshold and root mean squared log error (
\begin_inset Formula $rmls$
\end_inset

) we can see setups 6,8,9 perform far worse than other models.
 So we can see generally SGDN performed worse than Adam and Nadam optimizers.
 From Adam and Nadam optimizers, the setup 7 performed the best, which is
 Nadam optimizer with epsilon=
\begin_inset Formula $10^{-8}$
\end_inset

 and without the learning decay.
 That can be seen in all metrics except of mean relative error.
 
\end_layout

\begin_layout Standard
\begin_inset Float table
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Tabular
<lyxtabular version="3" rows="10" columns="7">
<features tabularvalignment="middle">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<row>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
name
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $aut\left(1.25\right)$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
mean relative error
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $rms$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $rmls$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
average 
\begin_inset Formula $\log_{10}$
\end_inset

 error
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
loss
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
setup 1
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.4390
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.3607
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
10.7808
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.5415
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.2880
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
14.6054
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
setup 2
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.4393
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.3432
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
11.0467
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.5533
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.2926
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
14.5629
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
setup 3
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.4554
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.3287
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
10.3699
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.5219
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.2730
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
14.5234
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
setup 4
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.4258
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.3392
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
9.7831
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.4849
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.2620
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
14.5913
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
setup 5
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.4314
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.3394
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
10.2730
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.5066
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.2708
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
14.6061
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
setup 6
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.0390
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.6588
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
13.8523
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.8978
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.6960
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
14.8873
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
setup 7
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.4852
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.3494
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
8.2342
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.4140
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.2180
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
14.5132
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
setup 8
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.0
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.7106
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
13.9028
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.9403
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.7651
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
14.9104
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
setup 9
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.0508
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.6488
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
13.8551
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.8948
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.6873
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
14.8796
\end_layout

\end_inset
</cell>
</row>
</lyxtabular>

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "tab:Depth-estimation-metrics"

\end_inset

Depth estimation metrics
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
Few images from the validation dataset and their depth predictions can be
 seen in figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Depth-estimation-samples"

\end_inset

.
 In the first row, we can see the RGB image, in the second row is the ground
 truth depth in meters.
 The third row depicts ground truth images reconstructed by the soft-sum
 inference from depth level bins.
 Here we can see all information behind 50 meters in truncated, which also
 visually helps us to focus on near objects.
 The red colour is for nearer objects and blue colour is for more distant
 objects.
 In rows 4-7 we can see the predictions of setups 3, 4, 5 and 7 which are
 performing better than rest of setups.
 
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Tabular
<lyxtabular version="3" rows="7" columns="5">
<features tabularvalignment="middle">
<column alignment="center" valignment="top" width="2cm">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top" width="2cm">
<row>
<cell multicolumn="1" alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
RGB image
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
vspace{0pt}
\end_layout

\end_inset


\begin_inset Graphics
	filename obrazky/depths/orig-rgb-1.png
	lyxscale 50
	width 3.5cm

\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
vspace{0pt}
\end_layout

\end_inset


\begin_inset Graphics
	filename obrazky/depths/orig-rgb-4.png
	lyxscale 50
	width 3.5cm

\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
vspace{0pt}
\end_layout

\end_inset


\begin_inset Graphics
	filename obrazky/depths/orig-rgb-3.png
	lyxscale 50
	width 3.5cm

\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
</row>
<row>
<cell multicolumn="1" alignment="center" valignment="top" usebox="none" width="2cm">
\begin_inset Text

\begin_layout Plain Layout
original depth
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
vspace{0pt}
\end_layout

\end_inset


\begin_inset Graphics
	filename obrazky/depths/colored-orig-depth-1.png.png
	width 3.5cm

\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
vspace{0pt}
\end_layout

\end_inset


\begin_inset Graphics
	filename obrazky/depths/colored-orig-depth-4.png.png
	width 3.5cm

\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
vspace{0pt}
\end_layout

\end_inset


\begin_inset Graphics
	filename obrazky/depths/colored-orig-depth-3.png.png
	width 3.5cm

\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Graphics
	filename obrazky/depths/colorbar-orig.png
	lyxscale 30
	height 3cm

\end_inset


\end_layout

\end_inset
</cell>
</row>
<row>
<cell multicolumn="1" alignment="center" valignment="top" usebox="none" width="2cm">
\begin_inset Text

\begin_layout Plain Layout
truncated depth
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
vspace{0pt}
\end_layout

\end_inset


\begin_inset Graphics
	filename obrazky/depths/colored-gt-depth-1.png.png
	width 3.5cm

\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
vspace{0pt}
\end_layout

\end_inset


\begin_inset Graphics
	filename obrazky/depths/colored-gt-depth-4.png.png
	width 3.5cm

\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
vspace{0pt}
\end_layout

\end_inset


\begin_inset Graphics
	filename obrazky/depths/colored-gt-depth-3.png.png
	width 3.5cm

\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Graphics
	filename obrazky/depths/colorbar-cropped.png
	lyxscale 30
	height 3cm

\end_inset


\end_layout

\end_inset
</cell>
</row>
<row>
<cell multicolumn="1" alignment="center" valignment="top" usebox="none" width="2cm">
\begin_inset Text

\begin_layout Plain Layout
setup 3
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
vspace{0pt}
\end_layout

\end_inset


\begin_inset Graphics
	filename obrazky/depths/colored-predicted-1-2018-04-01--00-26-49.png.png
	width 3.5cm

\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
vspace{0pt}
\end_layout

\end_inset


\begin_inset Graphics
	filename obrazky/depths/colored-predicted-4-2018-04-01--00-26-49.png.png
	width 3.5cm

\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
vspace{0pt}
\end_layout

\end_inset


\begin_inset Graphics
	filename obrazky/depths/colored-predicted-3-2018-04-01--00-26-49.png.png
	width 3.5cm

\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Graphics
	filename obrazky/depths/colorbar-cropped.png
	lyxscale 30
	height 3cm

\end_inset


\end_layout

\end_inset
</cell>
</row>
<row>
<cell multicolumn="1" alignment="center" valignment="top" usebox="none" width="2cm">
\begin_inset Text

\begin_layout Plain Layout
setup 4
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
vspace{0pt}
\end_layout

\end_inset


\begin_inset Graphics
	filename obrazky/depths/colored-predicted-1-2018-04-01--00-32-39.png.png
	width 3.5cm

\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
vspace{0pt}
\end_layout

\end_inset


\begin_inset Graphics
	filename obrazky/depths/colored-predicted-4-2018-04-01--00-32-39.png.png
	width 3.5cm

\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
vspace{0pt}
\end_layout

\end_inset


\begin_inset Graphics
	filename obrazky/depths/colored-predicted-3-2018-04-01--00-32-39.png.png
	width 3.5cm

\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Graphics
	filename obrazky/depths/colorbar-cropped.png
	lyxscale 30
	height 3cm

\end_inset


\end_layout

\end_inset
</cell>
</row>
<row>
<cell multicolumn="1" alignment="center" valignment="top" usebox="none" width="2cm">
\begin_inset Text

\begin_layout Plain Layout
setup 5
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
vspace{0pt}
\end_layout

\end_inset


\begin_inset Graphics
	filename obrazky/depths/colored-predicted-1-2018-04-02--02-51-28.png.png
	width 3.5cm

\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
vspace{0pt}
\end_layout

\end_inset


\begin_inset Graphics
	filename obrazky/depths/colored-predicted-4-2018-04-02--02-51-28.png.png
	width 3.5cm

\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
vspace{0pt}
\end_layout

\end_inset


\begin_inset Graphics
	filename obrazky/depths/colored-predicted-3-2018-04-02--02-51-28.png.png
	width 3.5cm

\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Graphics
	filename obrazky/depths/colorbar-cropped.png
	lyxscale 30
	height 3cm

\end_inset


\end_layout

\end_inset
</cell>
</row>
<row>
<cell multicolumn="1" alignment="center" valignment="top" usebox="none" width="2cm">
\begin_inset Text

\begin_layout Plain Layout
setup 7
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
vspace{0pt}
\end_layout

\end_inset


\begin_inset Graphics
	filename obrazky/depths/colored-predicted-1-2018-04-02--02-59-31.png.png
	width 3.5cm

\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
vspace{0pt}
\end_layout

\end_inset


\begin_inset Graphics
	filename obrazky/depths/colored-predicted-4-2018-04-02--02-59-31.png.png
	width 3.5cm

\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
vspace{0pt}
\end_layout

\end_inset


\begin_inset Graphics
	filename obrazky/depths/colored-predicted-3-2018-04-02--02-59-31.png.png
	width 3.5cm

\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Graphics
	filename obrazky/depths/colorbar-cropped.png
	lyxscale 30
	height 3cm

\end_inset


\end_layout

\end_inset
</cell>
</row>
</lyxtabular>

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:Depth-estimation-samples"

\end_inset

Depth estimation samples
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Graphics
	filename obrazky/depth-cost.png
	lyxscale 50
	width 14cm

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:Training-for-depth-loss"

\end_inset

Training for depth estimation - loss
\end_layout

\end_inset


\end_layout

\end_inset


\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Graphics
	filename obrazky/depth-root_mean_log_square_error.png
	lyxscale 50
	width 8cm

\end_inset


\begin_inset Graphics
	filename obrazky/depth-mean_relative_error.png
	lyxscale 50
	width 8cm

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Graphics
	filename obrazky/depth-root_mean_square_error.png
	lyxscale 50
	width 8cm

\end_inset


\begin_inset Graphics
	filename obrazky/depth-under_treshold_1.25.png
	lyxscale 50
	width 8cm

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:Training-for-depth-metrics"

\end_inset

Training for depth estimation - metrics
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
In the figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Training-for-depth-loss"

\end_inset

 we can see the loss during training and in figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Training-for-depth-metrics"

\end_inset

 we can see all metrics gathered during training.
 Interesting property of loss function based on information gain is the
 fact that it does not seem to converge most of the time during training,
 but all metrics are being optimized even when it is not seen in the loss
 figure.
 Here we can see clearly how setups 6, 8 and 9 performed poorly and setup
 7 performed the best in all metrics.
 The setup 7, which is Adam optimizer with Nesterov momentum, a.k.a.
 Nadam optimizer with constant learning rate seems to have best results
 and thus is used as a default optimizer for 3D map estimation and learning
 rate is constant in all setups.
\end_layout

\begin_layout Section
3D map estimation
\end_layout

\begin_layout Standard
For 3D map estimation, I also trained multiple setups and evaluated them,
 7 of them with most promising results are discussed here.
 The architecture was being modified, the last softmax layer was removed
 because now we don't aim to predict occupation of only 1 depth voxel per
 pixel, but we can predict multiple occupied voxels.
 Also, for setups 5, 6 and 7, new deconvolutional layer was stacked, between
 the last convolutional layer and the deconvolutional layer before the output.
 The parameters for optimizer were same, epsilon=
\begin_inset Formula $10^{-8}$
\end_inset

, beta1=
\begin_inset Formula $0.9$
\end_inset

, beta2=
\begin_inset Formula $0.999$
\end_inset

 for Nadam optimizer, and momentum=
\begin_inset Formula $0.9$
\end_inset

 was used for SGD optimizer with Nesterov momentum.
 For all setups, the weighted logistic loss 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:logistic-loss"

\end_inset

 is used.
 We can see the overview in the figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "tab:3d-estimation-setups"

\end_inset

.
 Then I evaluated these setups on validation sets and reported results in
 the table 
\begin_inset CommandInset ref
LatexCommand ref
reference "tab:3d-estimation-metrics"

\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Float table
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Tabular
<lyxtabular version="3" rows="6" columns="6">
<features tabularvalignment="middle">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<row>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
name
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
loss
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
optimizer
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
new deconvolutional layer
\end_layout

\end_inset
</cell>
<cell multirow="3" alignment="center" valignment="middle" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
training time
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
iterations
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
setup 1
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
logistic
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Nadam
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
-
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
25h
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
141k
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
setup 2
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
logistic
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
SGDN
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
-
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
24h
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
150k
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
setup 3
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
logistic
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Nadam
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
kernel=5, stride=1,out_dim=50
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
28h
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
147k
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
setup 4
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
logistic
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Nadam
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
kernel=2, stride=2,out_dim=50
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
44h
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
187k
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
setup 5
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
logistic
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Nadam
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
kernel=2, stride=2,out_dim=200
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
52h
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
154k
\end_layout

\end_inset
</cell>
</row>
</lyxtabular>

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "tab:3d-estimation-setups"

\end_inset

3D estimation setups
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Float table
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Tabular
<lyxtabular version="3" rows="6" columns="6">
<features tabularvalignment="middle">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<row>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
name
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
false positive rate
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
true positive rate
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
intersection over union
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $l_{1}$
\end_inset

 distance
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
loss
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
setup 1
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.2732
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.8603
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.6523
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
12151.0
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.1244
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
setup 2
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.0505
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.4712
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.2214
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
64437.4
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.13013582
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
setup 3
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.2120
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.8369
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.6233
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
13386.8
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.1080089
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
setup 4
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.2250
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.8355
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.6192
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
13587.4
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.097417
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
setup 5
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.2356
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.8514
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.6730
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
12359.5
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.123826
\end_layout

\end_inset
</cell>
</row>
</lyxtabular>

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "tab:3d-estimation-metrics"

\end_inset

Depth estimation metrics
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
The process of training is shown in figures 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Training-for-3d-loss"

\end_inset

 and 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Training-for-3d-metrics"

\end_inset

.
 In figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Training-for-3d-loss"

\end_inset

 we can see the cost being minimized.
 Here the logistic loss itself shows how individual setups perform.
 The figure has logarithmic Y scale for better visualization of individual
 setups.
 In the figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Training-for-3d-metrics"

\end_inset

 we can see metrics.
 The false positive rate is very low for all runs, but for other metrics
 we can see stochastic gradient descent with Nesterov momentum (setup 2)
 performed much poorer than other run.
 On the other hand, the setup 1 and setup 5 perform better than other setups
 in all metrics.
 From this we can than reduction of 4th dimension to the size od 50 in the
 end of the network worsens the performance, on the other hand, adding layer
 with 4th dimension of size 400 performs better in intersection over union
 metric.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Graphics
	filename obrazky/3d-cost.png
	lyxscale 50
	width 14cm

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:Training-for-3d-loss"

\end_inset

Training for 3D map estimation - loss
\end_layout

\end_inset


\end_layout

\end_inset


\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Graphics
	filename obrazky/3d-iou.png
	lyxscale 50
	width 8cm

\end_inset


\begin_inset Graphics
	filename obrazky/3d-l1_dist_on_known.png
	lyxscale 50
	width 8cm

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Graphics
	filename obrazky/3d-false_positive_rate.png
	lyxscale 50
	width 8cm

\end_inset


\begin_inset Graphics
	filename obrazky/3d-true_positive_rate.png
	lyxscale 50
	width 8cm

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:Training-for-3d-metrics"

\end_inset

Training for 3D map estimation - metrics
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
In figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:3d-estimation-samples"

\end_inset

 we can see two sample images from validation dataset, their ground truth
 voxelmaps and predicted voxelmaps from 2 best setups, setup 1 and setup
 5.
 In the first row, we can see the input image to the network, in the second
 row is the ground truth voxelmap in camera view frustum (and camera position
 denoted by white tetrahedron), and the last two rows show predictions of
 setup 1 and setup 5, respectively.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Tabular
<lyxtabular version="3" rows="4" columns="3">
<features tabularvalignment="middle">
<column alignment="center" valignment="top" width="2cm">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<row>
<cell multicolumn="1" alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
RGB image
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
vspace{0pt}
\end_layout

\end_inset


\begin_inset Graphics
	filename obrazky/3ds/orig-rgb-2.png
	lyxscale 50
	width 5.5cm

\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
vspace{0pt}
\end_layout

\end_inset


\begin_inset Graphics
	filename obrazky/3ds/orig-rgb-1.png
	lyxscale 50
	width 5.5cm

\end_inset


\end_layout

\end_inset
</cell>
</row>
<row>
<cell multicolumn="1" alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
original voxelmap
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
vspace{0pt}
\end_layout

\end_inset


\begin_inset Graphics
	filename obrazky/3ds/gt-2.PNG
	lyxscale 30
	width 6cm

\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
vspace{0pt}
\end_layout

\end_inset


\begin_inset Graphics
	filename obrazky/3ds/gt-1.PNG
	lyxscale 30
	width 6cm

\end_inset


\end_layout

\end_inset
</cell>
</row>
<row>
<cell multicolumn="1" alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
setup 1
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
vspace{0pt}
\end_layout

\end_inset


\begin_inset Graphics
	filename obrazky/3ds/img-2-setup-0.PNG
	lyxscale 30
	width 6cm

\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
vspace{0pt}
\end_layout

\end_inset


\begin_inset Graphics
	filename obrazky/3ds/img-1-setup-0.PNG
	lyxscale 30
	width 6cm

\end_inset


\end_layout

\end_inset
</cell>
</row>
<row>
<cell multicolumn="1" alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
setup 5
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
vspace{0pt}
\end_layout

\end_inset


\begin_inset Graphics
	filename obrazky/3ds/img-2-setup-5.PNG
	lyxscale 30
	width 6cm

\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
vspace{0pt}
\end_layout

\end_inset


\begin_inset Graphics
	filename obrazky/3ds/img-1-setup-5.PNG
	lyxscale 30
	width 6cm

\end_inset


\end_layout

\end_inset
</cell>
</row>
</lyxtabular>

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:3d-estimation-samples"

\end_inset

3D estimation samples
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Chapter
Future work
\end_layout

\begin_layout Standard
During working with GTA V, many aspects have been discovered, but more of
 them remained to be discovered in the future.
 In GTA V reverse-engineering, many stencil values semantics remain to be
 discovered.
 Also, in GTA V, the modding capabilities are far from using their full
 potential.
 Lots of existing mods could help with scenarios setup for gathering data
 in specific situations, also necessary tooling needs to be developed to
 provide simple interface for scenarios setup.
 But probably one of the most promising mod categories are visual mods,
 which enhance the graphics of the game.
 Many visual mods are being used currently and some of them can be combined
 together, utilizing advantages of them both.
 The main difficulty in using visual mods is evaluation of their photo-realism.
 In the gaming community, players long for stunning graphics and aesthetically
 pleasing visuals.
 These visuals are sometimes unfortunately different than photorealistic
 visuals.
 Even when surveying individual players for most realistic visual mod, most
 of them considered as realistic the most aesthetically pleasing mod, without
 considering the photorealism to be relevant.
 Photorealism of individual visual mods and their combination should be
 exploited and evaluated in order to get even more photo-realistic visual
 data.
 There is multiple visual mods, namely 
\begin_inset CommandInset href
LatexCommand href
name "Redux"
target "https://gta5redux.com/"

\end_inset

, 
\begin_inset CommandInset href
LatexCommand href
name "Make Visual Great Again"
target "https://www.darkmyre.net/files/file/30-make-visuals-great-again/"

\end_inset

, 
\begin_inset CommandInset href
LatexCommand href
name "Natural Vision Remastered"
target "https://cs.gta5-mods.com/misc/naturalvision-photorealistic-gtav"

\end_inset

, 
\begin_inset CommandInset href
LatexCommand href
name "Rhancer Photorealism Mod"
target "https://cs.gta5-mods.com/misc/r-hancer-graphics-mod"

\end_inset

, 
\begin_inset CommandInset href
LatexCommand href
name "VisualV"
target "https://cs.gta5-mods.com/misc/visualv"

\end_inset

, 
\begin_inset CommandInset href
LatexCommand href
name "K-putt's SweetFX Config (ReShade)"
target "https://cs.gta5-mods.com/misc/reshade-sweetfx-graphics-mod"

\end_inset

.
 The problem with evaluation of these mods in missing support of tooling
 for automated manipulation with GTA, mods need to be installed and uninstalled
 manually, same goes for starting the game which leads to slow and time-consumin
g evaluation of photorealism of these mods, but it definitely is worth it
 in the long term.
 Also developing custom visual mod which would make the game look more photoreal
istic would solve this problem.
 Also streets layout and static assets can be changed by mods and if changed
 correctly, they would better simulate real-life environment.
 Current setting of GTA V is heavily based on Los Angeles, so street layout,
 house architectures, cars and other objects are set to correspond with
 Los Angeles.
 This layout differs in other parts of world and many use-cases would profit
 from possibility of using more European or other streets layout, cars and
 houses architecture.
 Other notable aspect of the game is possibility to render multiple cameras
 at time, allowing to capture stereo data in real-time without switching
 between them or speeding up data gathering from multiple cameras.
 So far GTA V is VR ready which means it already supports stereo-camera
 rendering, and so it awaits for being used in data gathering.
 
\end_layout

\begin_layout Standard
Other potentially perspective, but yet not unexplored usage of GTA V is
 multiplayer setup, which can be used for multi-agent simulations.
 The official multiplayer of course does not allow using any modifications,
 but there is 
\begin_inset CommandInset href
LatexCommand href
name "FiveM"
target "https://fivem.net/"

\end_inset

, program for running private GTA V server on dedicated machines, with support
 for server-side and client-side scripting, which could be used for many
 multi-agent tasks.
\end_layout

\begin_layout Standard
todo: sepsat pořádné conclusion a future work mít jenom jako jeho část
\end_layout

\begin_layout Standard
\begin_inset CommandInset bibtex
LatexCommand bibtex
bibfiles "bibliography"
options "bibtotoc,plainnat"

\end_inset


\end_layout

\begin_layout Chapter
\start_of_appendix
Contents of the enclosed CD
\end_layout

\begin_layout Standard
\begin_inset listings
inline false
status open

\begin_layout Plain Layout

appendix content
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Newpage cleardoublepage
\end_inset


\end_layout

\end_body
\end_document
