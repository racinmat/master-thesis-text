#LyX 2.2 created this file. For more info see http://www.lyx.org/
\lyxformat 508
\begin_document
\begin_header
\save_transient_properties true
\origin unavailable
\textclass scrreprt
\begin_preamble
%<-------------------------------společná nastavení------------------------------>
\usepackage[numbers,sort&compress]{natbib} %balíček pro citace literatury  
\usepackage{algorithmic}
\usepackage{color}%kvůli barvám ČVUT
\newcommand{\BibTeX}{{\sc Bib}\TeX}%BibTeX logo
\usepackage{multicol}
\usepackage[overload]{textcase}



%<-----------------------------volání stylů----------------------------------------->
% (znak % je označení komentáře: co je za ním, není aktivní)

%<--------matematické písmo--------------------------------------->

%\usepackage[helvet]{packages/sfmath}%matematika ala helvetica



%<------------------------------záhlaví stránek------------------------------------>
%\usepackage{packages/bc-headings}
\usepackage{packages/bc-fancyhdr}

%<------------------------------hlavičky kapitol------------------------------------>
%\usepackage{packages/bc-neueskapitel}
\usepackage{packages/bc-fancychap}
\end_preamble
\options cleardoublepage=empty,BCOR15mm,DIV12
\use_default_options false
\begin_modules
logicalmkup
\end_modules
\maintain_unincluded_children false
\language british
\language_package default
\inputencoding utf8
\fontencoding global
\font_roman "tgtermes" "default"
\font_sans "tgheros" "default"
\font_typewriter "tgcursor" "default"
\font_math "newtxmath" "auto"
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100 100
\font_tt_scale 100 100
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command bibtex
\index_command default
\paperfontsize default
\spacing single
\use_hyperref true
\pdf_bookmarks true
\pdf_bookmarksnumbered false
\pdf_bookmarksopen false
\pdf_bookmarksopenlevel 1
\pdf_breaklinks false
\pdf_pdfborder false
\pdf_colorlinks false
\pdf_backref false
\pdf_pdfusetitle true
\papersize a4paper
\use_geometry false
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 1
\use_package esint 0
\use_package mathdots 0
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine basic
\cite_engine_type default
\biblio_style \use_bibtopic false
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 0
\index Index
\shortcut idx
\color #008000
\end_index
\secnumdepth 2
\tocdepth 2
\paragraph_separation indent
\paragraph_indentation default
\quotes_language english
\papercolumns 1
\papersides 2
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Standard
\begin_inset space ~
\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
thispagestyle{empty}
\end_layout

\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
begin{center}
\end_layout

\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
pagenumbering{arabic}
\end_layout

\end_inset


\begin_inset VSpace 10mm
\end_inset


\end_layout

\begin_layout Standard

\family sans
\shape smallcaps
\size largest
\noun on
Czech Technical University in Prague
\family default
\shape default
\size default
\noun default

\begin_inset Newline newline
\end_inset


\begin_inset VSpace 0.5em
\end_inset


\family sans
\shape smallcaps
\size largest
\noun on
Faculty of Electrical Engineering
\family default
\shape default
\size default
\noun default

\begin_inset Newline newline
\end_inset


\begin_inset VSpace 1em*
\end_inset


\family sans
\shape smallcaps
\size larger
\noun on
Department of Cybernetics
\family default
\shape default
\size default
\noun default

\begin_inset VSpace 15mm
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Graphics
	filename obrazky/lev.png
	lyxscale 50
	width 30text%

\end_inset


\begin_inset VSpace 15mm
\end_inset


\end_layout

\begin_layout Standard

\family sans
\size huge
MASTER'S THESIS
\end_layout

\begin_layout Standard
\begin_inset VSpace 15mm
\end_inset


\end_layout

\begin_layout Standard

\family sans
\size largest
3D map estimation from a single RGB image
\end_layout

\begin_layout Standard
\begin_inset VSpace 10mm
\end_inset


\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
end{center} 
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset VSpace vfill*
\end_inset


\end_layout

\begin_layout Standard
\begin_inset VSpace 10mm
\end_inset


\end_layout

\begin_layout Description
\noindent
\align block

\size large
Author: 
\family sans
Bc.
 Matěj Račinský
\end_layout

\begin_layout Description
\noindent
\align block

\size large
Thesis
\begin_inset space ~
\end_inset

supervisor: doc.
 Ing.
 Karel Zimmermann, PhD.
\begin_inset space \hfill{}
\end_inset


\family sans
In Prague, May 2018
\end_layout

\begin_layout Standard
\begin_inset Newpage cleardoublepage
\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
thispagestyle{empty}
\end_layout

\end_inset


\begin_inset space ~
\end_inset


\end_layout

\begin_layout Standard

\size small
\begin_inset space \space{}
\end_inset


\end_layout

\begin_layout Standard
\noindent

\size small
\begin_inset VSpace vfill
\end_inset

 
\begin_inset ERT
status open

\begin_layout Plain Layout

% nastavuje dynamické umístění následujícího textu do spodní části stránky
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\begin_inset space ~
\end_inset


\end_layout

\begin_layout Subparagraph*
Author statement for the graduate thesis: 
\begin_inset Newline newline
\end_inset


\end_layout

\begin_layout Standard
I declare that the presented work was developed independently and that I
 have listed all the sources of information used within it in accordance
 with the methodical instructions for observing the ethical principles in
 the presentation of university theses.
 
\end_layout

\begin_layout Standard

\size small
\begin_inset VSpace bigskip
\end_inset


\size default

\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
noindent 
\end_layout

\end_inset


\size small
 Prague, date 
\size default
________
\size small

\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
hspace{
\backslash
fill}
\end_layout

\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout

$
\backslash
overline{
\backslash
textrm{~~~~~~~~~signature~~~}}$
\end_layout

\end_inset


\begin_inset Newline newline
\end_inset

 
\begin_inset ERT
status open

\begin_layout Plain Layout

% doplňte patřičné datum, jméno a příjmení
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard

\size small
\begin_inset ERT
status open

\begin_layout Plain Layout

%%%   Výtisk pak na tomto míste nezapomeňte PODEPSAT!
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout

%%%                                         *********
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Newpage cleardoublepage
\end_inset


\end_layout

\begin_layout Standard

\size small
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
thispagestyle{plain}
\end_layout

\end_inset

 
\end_layout

\begin_layout Standard

\size small
\begin_inset ERT
status open

\begin_layout Plain Layout

%
\backslash
setcounter{page}{3}
\end_layout

\end_inset

 
\begin_inset ERT
status open

\begin_layout Plain Layout

% nastavení číslování stránek
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
\noindent

\size small
\begin_inset space ~
\end_inset


\begin_inset VSpace vfill
\end_inset


\end_layout

\begin_layout Description
\noindent

\size small
Název
\begin_inset space ~
\end_inset

práce: Odhad 3D mapy z jednoho RGB obrazu
\end_layout

\begin_layout Description
\noindent

\size small
Autor: Bc.
 Matěj Račinský
\end_layout

\begin_layout Description
\noindent

\size small
Katedra
\begin_inset space ~
\end_inset

(ústav):
\size default
 Kate
\size small
dra kybernetiky
\end_layout

\begin_layout Description
\noindent

\size small
Vedoucí
\begin_inset space ~
\end_inset

bakalářské
\begin_inset space ~
\end_inset

práce: 
\size large
doc.
 Ing.
 Karel Zimmermann, PhD.
\end_layout

\begin_layout Description
\noindent

\size small
e-mail
\begin_inset space ~
\end_inset

vedoucího: zimmerk@fel.cvut.cz
\begin_inset Newline newline
\end_inset


\end_layout

\begin_layout Description
\noindent

\size small
Abstrakt 
\size default
Tato práce se zabývá využitím virtuálních světů z počítačových her jakožto
 zdroje dat pro strojové učení, a odhadem voxelové mapy z jednoho RGB obrázku
 za pomoci hlubokého učení.
 Tato práce zahrnuje skripty pro napojení se na PC hru GTA V a sběr dat
 z ní pro tvorbu automaticky anotovaných datasetů, a implementaci hluboké
 neuronové sítě v TensorFlow.
 
\end_layout

\begin_layout Description
\noindent

\size small
Klíčová
\begin_inset space ~
\end_inset

slova: Deep learning, Machine learning, GTA V, virtual world, depth estimation,
 voxelmap estimation, RAGE
\begin_inset Newline newline
\end_inset


\end_layout

\begin_layout Description
\noindent

\size small
\begin_inset CommandInset line
LatexCommand rule
offset "0.5ex"
width "100line%"
height "1pt"

\end_inset


\end_layout

\begin_layout Description
\noindent

\size small
Title: 3D map estimation from a single RGB image
\end_layout

\begin_layout Description
\noindent

\size small
Author: Bc.
 Matěj Račinský
\end_layout

\begin_layout Description
\noindent

\size small
Department: Department of Cybernetics
\end_layout

\begin_layout Description
\noindent

\size small
Supervisor: 
\size large
doc.
 Ing.
 Karel Zimmermann, Ph.D.
\end_layout

\begin_layout Description
\noindent

\size small
Supervisor's
\begin_inset space ~
\end_inset

e-mail
\begin_inset space ~
\end_inset

address: zimmerk@fel.cvut.cz
\begin_inset Newline newline
\end_inset


\end_layout

\begin_layout Description
\noindent

\size small
Abstract 
\size default
In this thesis we explore virtual worlds used as data source for machine
 learning and voxel map estimation from single RGB image with deep learning.
 This thesis describes principles and implementation of hooking into GTA
 V and gathering data from it to create automatically annotated dataset,
 and implementation of deep neural network in TensorFlow.
\end_layout

\begin_layout Description
\noindent

\size small
Keywords: Deep learning, Machine learning, GTA V, virtual world, depth estimatio
n, voxel map estimation, RAGE
\end_layout

\begin_layout Standard

\size small
\begin_inset CommandInset toc
LatexCommand tableofcontents

\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout

% vkládá automaticky generovaný obsah dokumentu
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Chapter
\begin_inset Argument 1
status open

\begin_layout Plain Layout
Introduction
\end_layout

\end_inset

Introduction
\end_layout

\begin_layout Standard
This thesis aims to solve two problems.
 The first problem is the slow process of manual dataset creation and annotation
, and the second problem is voxel map estimation from a single RGB image.
 Due to increasing interest in synthetic datasets, this thesis aims to be
 the documentation for using GTA V as a simulator for a creation of synthetic
 datasets.
\end_layout

\begin_layout Section
Deep learning is data hungry
\end_layout

\begin_layout Standard
In recent years, both machine learning and deep learning has experienced
 great progress in many fields 
\begin_inset CommandInset citation
LatexCommand cite
key "history-began-from-alexnet"

\end_inset

.
 Deep learning has outperformed many other machine learning approaches by
 using deep, high-capacity models trained on large datasets.
 Especially in the field of computer vision, neural networks achieve state
 of the art results in most of the tasks.
 Many tasks in computer vision are the first where deep neural networks
 achieve state of the art results before being used in other fields, and
 in this field, deeper and deeper architectures are being proposed earlier
 than in other fields.
 
\end_layout

\begin_layout Standard
With larger amount of parameters, the need for large datasets is growing,
 with current datasets unable to cover the need for annotated data.
\end_layout

\begin_layout Standard
Data has proven to be limiting factor in many computer vision tasks.
 The main problem is that manual data annotation is exhausting, time-consuming
 and costly.
 That is even more significant for pixel-wise annotation which is crucial
 for tasks of semantic segmentation.
 Pixel-wise annotated datasets are orders of magnitude smaller than image
 classification datasets.
 This is sometimes called 
\begin_inset Quotes eld
\end_inset

curse of dataset annotation
\begin_inset Quotes erd
\end_inset

 
\begin_inset CommandInset citation
LatexCommand cite
key "semantic-instance-annotation"

\end_inset

, because more detailed semantic labelling leads to smaller size of dataset.
 
\end_layout

\begin_layout Standard
Many novel neural network architectures are being proposed every year because
 of ongoing research and increasing computing power.
 With growing capacity and number of parameters in these new models, there
 is need for bigger and bigger datasets for training.
 Several papers shown positive correlation between the size of data and
 performance 
\begin_inset CommandInset citation
LatexCommand cite
key "unreasonable-effectiveness-of-data,revisiting-unreasonable-efectiveness,real-time-human-pose-recognition-in-parts-from-a-single-depth-image"

\end_inset

.
\end_layout

\begin_layout Standard
There are attempts to develop tooling necessary to speed up the manual annotatio
n process during datasets creation, most known being the Amazon Mechanical
 Turk (AMT) 
\begin_inset CommandInset citation
LatexCommand cite
key "mechanical-turk"

\end_inset

 and Supervise.ly 
\begin_inset CommandInset citation
LatexCommand cite
key "supervise.ly"

\end_inset

.
\end_layout

\begin_layout Standard
Amazon Mechanical Turk is a platform for crowdsourcing work and has been
 used in many academic fields.
 AMT is a 
\begin_inset Quotes eld
\end_inset

marketplace for work that requires human intelligence
\begin_inset Quotes erd
\end_inset

 
\begin_inset CommandInset citation
LatexCommand cite
key "amt-article"

\end_inset

, a web-based platform for distributing tasks to a pool of human workers,
 known colloquially as Turkers.
 These tasks are typically small (i.e.
 a few minutes to perform a task rather than days or weeks) and payments
 pay task are low, in the orders of cents per task.
 This platform provides possibility to distribute the annotation process
 among many people at once and for low price.
 Although still being manual, it accelerates the annotation process and
 helps researchers to annotate data more easily 
\begin_inset CommandInset citation
LatexCommand cite
key "amt-cv-annotation"

\end_inset

.
 Quality of work can not be guaranteed since task provider can't supervise
 workers and correct them manually, but this disadvantage can be compensated
 by various quality assurance approaches 
\begin_inset CommandInset citation
LatexCommand cite
key "amt-cv-annotation"

\end_inset

.
 
\end_layout

\begin_layout Standard
Supervise.ly is another web-based platform, focused on computer vision datasets
 annotation.
 It provides helpful tooling for annotators in unified web interface, speeding
 up the annotation process.
 It offers integration with the AMT, so together, they seem to be candidate
 for industrial standard of data annotation.
 
\end_layout

\begin_layout Standard
Although these tools lead to faster annotation process at lower cost, it
 still can not be compared to the fully automated data gathering and annotation,
 which could potentially solve these problems of lack of datasets in many
 computer vision and related tasks.
\end_layout

\begin_layout Standard
The power of automatic annotation can be also expressed in terms of money.
 Although not many manually annotated datasets describe time of annotation,
 some of them do.
 In Cityscapes, the fine annotation took 1.5 hour per image 
\begin_inset CommandInset citation
LatexCommand cite
key "cityscapes"

\end_inset

.
 If we had annotators paid minimum hourly wage in USA, 7.25$, what would
 mean each fine annotated image has price 10.875$.
 The whole fine annotated Cityscapes dataset with 5000 images thus have
 has price at least 54 375$ and took 312.5 days to annotate.
 In the dataset I created as part of my thesis, I gathered 33292 images
 in 3.5 hours.
 If we would annotate this dataset same as Cityscapes fine annotation, it
 would take over 2080 days, which is approximately 5.7 years, and it would
 cost 362 050$.
 Now we can see how this automatic annotation approach is significantly
 both time and money saving.
\end_layout

\begin_layout Section
Gaming industry to the rescue
\end_layout

\begin_layout Standard
In last decades, gaming industry has grown hugely and expanded from small
 and specific community into public society and became mainstream industry.
 
\end_layout

\begin_layout Standard
The gaming industry became big driving force in many fields, and indirectly
 influenced even machine learning.
 
\end_layout

\begin_layout Standard
The mainstream model of gaming is on personal computers, where each player
 has his own gaming PC, along with console gaming.
 Thanks to ever-growing number of players, lots of money got into industry
 and the growing demand for better graphics in games led to big improvements
 in both software-computer graphics and hardware-graphics cards.
 With lots of money being invested by players in their PCs, GPU manufacturers
 were able to deliver more powerful GPUs every year and we can see exponential
 growth of GPU computational power 
\begin_inset CommandInset citation
LatexCommand cite
key "high-performance-gpu"

\end_inset

.
\end_layout

\begin_layout Standard
Big companies in gaming industry have enough resources to develop the state
 of the art real-time computer graphics, which can we see in their products,
 AAA games with graphics very near to reality.
 
\end_layout

\begin_layout Standard
Recent papers 
\begin_inset CommandInset citation
LatexCommand cite
key "playing-for-data,driving-in-matrix"

\end_inset

 have shown that we can use screenshots from PC games to obtain large automatica
lly or semi-automatically annotated datasets, which improve learning.
 This lets us to outperform same models trained only on real data and achieve
 state of the art results on public datasets (KITTI dataset in 
\begin_inset CommandInset citation
LatexCommand cite
key "driving-in-matrix"

\end_inset

 and CamVid dataset in 
\begin_inset CommandInset citation
LatexCommand cite
key "playing-for-data"

\end_inset

).
\end_layout

\begin_layout Section
Contribution
\end_layout

\begin_layout Standard
In this thesis, I demonstrate the power of rich virtual worlds in creating
 synthetic datasets, and propose software architecture of dataset creation.
 Then I create voxel map of space in front of camera to create RGB image,
 voxel map pairs for learning.
 In the last part, I train deep convolutional neural network for voxel map
 estimation.
\end_layout

\begin_layout Standard
This thesis is structured as follows.
 Chapter 2 covers related work and previous achievements of synthetic datasets.
 Chapter 3 describes game Grand Theft Auto V and its utilization in creating
 synthetic datasets.
 I describe here whole modding process, the game API, internal game coordinate
 systems and data which can be obtained from the game.
 Chapter 4 describes voxel map estimation from a single RGB image.
 It is followed by chapter 5, where I describe all experiments done as part
 of this thesis.
\end_layout

\begin_layout Standard
To demonstrate possibilities of synthetic, automatically annotated datasets,
 here are some sample images obtained using my data extraction described
 in 
\begin_inset CommandInset ref
LatexCommand ref
reference "sec:GTA-V-data-obtaining"

\end_inset

 .
 The dataset contains position, rotation, model sizes, identifier and the
 pixel-wise class segmentation of each car.
 With this data, I was able to do the 3D and 2D bounding boxes extraction,
 pixel-wise object segmentation and trajectory tracking.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Graphics
	filename obrazky/bbox-night.png
	lyxscale 70
	width 12cm

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
3D bounding box - night
\end_layout

\end_inset


\end_layout

\end_inset


\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Graphics
	filename obrazky/bbox-day-people.png
	lyxscale 70
	width 12cm

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
3D bounding box - day
\end_layout

\end_inset


\end_layout

\end_inset


\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Graphics
	filename obrazky/pixelwise-night.png
	lyxscale 70
	width 12cm

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
pixel-wise annotation of cars
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Graphics
	filename obrazky/trajectories-day.png
	lyxscale 70
	width 12cm

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
individual car trajectories
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Chapter
Related work
\end_layout

\begin_layout Standard
Richter et al.
 
\begin_inset CommandInset citation
LatexCommand cite
key "playing-for-data"

\end_inset

 used GTA V to obtain screenshots and performed semi-automated pixel-wise
 semantic segmentation.
 Although the process was not fully automatic, the annotation speed per
 image was drastically increased, being 771 times faster than fine per-image
 annotation of Cityscapes 
\begin_inset CommandInset citation
LatexCommand cite
key "cityscapes"

\end_inset

 and 514 times faster than per-image annotation of CamVid 
\begin_inset CommandInset citation
LatexCommand cite
key "camvid"

\end_inset

.
 Richter et al.
 extracted 24 966 images from game GTA V, which is roughly two orders of
 magnitude larger than CamVid and three orders of magnitude larger than
 semantic annotations for KITTI dataset.
 They trained the prediction module of Yu and Kolthun 
\begin_inset CommandInset citation
LatexCommand cite
key "kolthun-dilation"

\end_inset

 and by using on 
\begin_inset Formula $\frac{1}{3}$
\end_inset

 of the CamVid training set (which is ) and all 24 966 GTA V screenshots,
 they outperformed same model trained on whole CamVid training dataset.
\end_layout

\begin_layout Standard
For images extraction, they use RenderDoc
\begin_inset CommandInset citation
LatexCommand cite
key "renderdoc"

\end_inset

, stand-alone graphics debugger.
 It intercepts the communication between the game and the GPU and allows
 to gather screenshots.
 It's advantage is that it can be used for different games, allowing to
 gather datasets in various environments.
\end_layout

\begin_layout Standard
Johnson-Roberson et al.
 
\begin_inset CommandInset citation
LatexCommand cite
key "driving-in-matrix"

\end_inset

 used GTA V screenshots, depth and stencil buffer to produce car images
 and automatically calculated their bounding boxes.
 
\end_layout

\begin_layout Standard
On these generated data, they trained Faster R-CNN 
\begin_inset CommandInset citation
LatexCommand cite
key "faster-r-cnn"

\end_inset

 only on screenshots from the GTA V game, using up to 200 000 screenshots,
 which is one order of magnitude bigger than Cityscapes dataset.
 Using only screenshots for training, they outperformed same architecture
 trained on Cityscapes, evaluating on KITTI dataset.
 They developed their own GTA V mod
\begin_inset CommandInset ref
LatexCommand ref
reference "sec:GTA-V-modding"

\end_inset

 to hook into GPU calls and gather screenshots from here.
 
\end_layout

\begin_layout Chapter
Transforming GTA V into the State of the Art simulator
\end_layout

\begin_layout Standard
In this thesis, Grand Theft Auto V (GTA V) game is used for creating synthetic,
 nearly photo-realistic dataset.
 
\end_layout

\begin_layout Section
GTA V introduction
\end_layout

\begin_layout Standard
GTA V is action-adventure open-world video game developed by Rockstar North
 and published by Rockstar Games.
 The game was released on 17.9.2013 for PlayStation 3 and Xbox 360
\begin_inset CommandInset citation
LatexCommand cite
key "gta-release"

\end_inset

 , in 18.11.2014 for PS4 and Xbox One and in 14.4.2015 it was released on PC,
 Windows
\begin_inset CommandInset citation
LatexCommand cite
key "gta-release-pc"

\end_inset

.
\end_layout

\begin_layout Standard
The game is based on proprietary game engine, called 
\begin_inset CommandInset label
LatexCommand label
name "rage"

\end_inset

RAGE (Rockstar Advanced Game Engine)
\begin_inset CommandInset citation
LatexCommand cite
key "gta-5-rage"

\end_inset

, which is used as a base for most of Rockstar Games products.
 
\end_layout

\begin_layout Standard
Till the release on Microsoft, Windows, it has been in development for 5
 years with approximate 1000-person team
\begin_inset CommandInset citation
LatexCommand cite
key "gta-interview-studio"

\end_inset

.
 The world of GTA V was modelled on Los Angeles
\begin_inset CommandInset citation
LatexCommand cite
key "gta-v-interview"

\end_inset

 and other areas of Southern California, with road networks respecting design
 of Los Angeles map.
 
\end_layout

\begin_layout Standard
As could be expected from AAA game like GTA V, motion capture was used to
 character's both body and facial movements.
 
\end_layout

\begin_layout Standard
There are several reasons why GTA V is better for dataset creation than
 other games.
 To use a game for dataset creation, we have multiple requirements.
 The graphics of the game must be near photorealistic, since we try to to
 use it instead of photos for computer vision tasks.
 This disqualifies most of games, and leaves us only with AAA games produced
 by big companies and few other games with State of the Art graphics.
 
\end_layout

\begin_layout Standard
The other requirement is possibility of good-enough way to interact with
 the game programmatically.
 Usually we want to setup at least part of the environment before gathering
 data.
 This part heavily depends on community around the particular game.
\end_layout

\begin_layout Standard
Also the advantage of GTA V compared to some other games is abundance of
 models and various sceneries in its virtual world.
 It has complex transportation system of roads, highways, intersections,
 railroad crossing, tunnels, and pedestrians.
 It also has urban, suburban, and rural environments 
\begin_inset CommandInset citation
LatexCommand cite
key "video-games-for-autonomous-driving"

\end_inset

.
\end_layout

\begin_layout Standard
In gaming subculture, there are communities where people specialize in reverse-e
ngineering of games and development of modifications to these games.
 These people are called modders or mod developers, and these unofficial
 modifications and extension of games are called mods.
 For few games, developers welcome this kind of activity and sometimes they
 even release tools to ease the game modding.
 In most cases, the game developers simply don't care and in few cases,
 they actively fight against the reverse-engineering and modding.
 
\end_layout

\begin_layout Standard
The GTA V is second case, where Rockstar Games does not actively try to
 prevent the reverse-engineering, but they don't release any tools to ease
 it, either.
 This results in cyclic process of Rockstar Games releasing new version
 of game, including backward compatibility (BC) breaks, and community reverse
 engineering the new version and adjusting their mods to work with the new
 version.
\end_layout

\begin_layout Standard
The modding community around the GTA V is based mostly on community around
 GTA IV, which was previous big game produced by Rockstar Games.
 So many tools are just GTA IV based and only modified to work with GTA
 V.
 Luckily, the community is large and productive, so we have many mods and
 many function in GTA V reverse-engineered and thus prepared for programmatic
 interactions.
\end_layout

\begin_layout Subsection
Cars
\end_layout

\begin_layout Standard
There is big variety of cars models.
 Specifically, the are 259 car models, all of them are listed here 
\begin_inset CommandInset citation
LatexCommand cite
key "gta-vehicle-models"

\end_inset

.
 These models cars of various shapes and sizes, from golf carts to trucks
 and trailers.
 This diversity is representative of real distribution of vehicles.
 It even allows us to simulate environments with various types of vehicles,
 which would be very difficult in real environment.
 GTA V provides us many information about cars, more on this will be covered
 in section 
\begin_inset CommandInset ref
LatexCommand ref
reference "sec:GTA-V-data-obtaining"

\end_inset

.
\end_layout

\begin_layout Subsection
Pedestrians
\end_layout

\begin_layout Standard
GTA V has pedestrians and provides some information about them, more on
 this in section 
\begin_inset CommandInset ref
LatexCommand ref
reference "sec:GTA-V-data-obtaining"

\end_inset

.
 The game has pedestrians of both genders and various ethnicities.
 Pedestrians appear in various poses, like standing, walking, sitting, many
 animations etc.
 The main drawback of GTA V is that all pedestrians are about the same height
\begin_inset CommandInset citation
LatexCommand cite
key "video-games-for-autonomous-driving"

\end_inset

.
\end_layout

\begin_layout Section
Automotive Simulators
\end_layout

\begin_layout Standard
Currently, there are some open-source simulation platforms for automotive
 industry which could be theoretically used for creating synthetic datasets.
 But compared to AAA games like GTA V, they have much less resources and
 much less customers to finance the development.
 In result, simulators have worse graphics than AAA games and NPC (non playable
 characters) don't have as sophisticated behaviour.
 In GTA V, drivers mostly follow traffic regulations, traffic lights and
 traffic lanes, which leafs to very realistic environment better than simulators
 can provide.
\end_layout

\begin_layout Section
\begin_inset CommandInset label
LatexCommand label
name "sec:GTA-V-modding"

\end_inset

GTA V modding ecosystem
\end_layout

\begin_layout Standard
Although the modding community is quite big, as it is in lots of open-source
 communities, essential part of community depends on one person.
 Here, it is Alexander Blade.
 In his free time, he reverse-engineered big part of GTA V and developed
 ScriptHookV
\begin_inset CommandInset citation
LatexCommand cite
key "scripthookv-gtaforums"

\end_inset

, library enabling to perform native calls into GTA V in C++ and develop
 GTA V mods in C++.
 Currently, more people in community participates in reverse-engineering
 and they share their knowledge in GTA forum thread
\begin_inset CommandInset citation
LatexCommand cite
key "nativedb-research"

\end_inset

.
\end_layout

\begin_layout Standard
List of all reverse-engineered native functions is kept in following list
 
\begin_inset CommandInset citation
LatexCommand cite
key "nativedb"

\end_inset

.
 Assumably, GTA V contains ~5200 natives.
 There is no original native name list of functions in GTA V, name hashes
 are used instead.
 During reverse-engineering and game decompilation, ~2600 native names were
 discovered using brute-force and manual checking afterwards.
 For these functions, number of parameters and returns of these calls are
 also known.
 In the native functions list, for big part of functions we know their name,
 signature and how do they affect the game.
 The rest remains to be discovered yet.
\end_layout

\begin_layout Standard
When new version of game is released, in few days to weeks, new version
 of ScriptHookV is released, fixing BC breaks.
\end_layout

\begin_layout Standard
Other heavily used mod in community is ScriptHookDotNet2, which is built
 atop of ScriptHookV and creates bridge between C# language and ScriptHookV,
 effectively allowing to write GTA V mods in C#.
 It is available as open-source 
\begin_inset CommandInset citation
LatexCommand cite
key "scripthookvdotnet"

\end_inset

.
 Along with creating bridge between C# and GTA V, it wraps most used native
 calls into classes, leveraging object-oriented paradigm for mod development
 using getters and setters as proxies for native calls.
 
\end_layout

\begin_layout Standard
Next notable mod is NativeUI
\begin_inset CommandInset citation
LatexCommand cite
key "nativeui"

\end_inset

.
 It renders windows atop of GTA V GUI and allows us to define custom control
 panels for manipulating custom functionality in other mods.
 
\end_layout

\begin_layout Standard
Unlike most of other mods, these three mods act more as a framework for
 mod development.
 
\end_layout

\begin_layout Standard
Since GTA V is a game, it requires human interaction.
 For simulator-like behaviour we would want the car to drive autonomously
 to crawl data without human interaction.
 This can be done using VAutodrive
\begin_inset CommandInset citation
LatexCommand cite
key "vautodrive"

\end_inset

.
 This allows us to use NPC automatic behaviour patterns for main player,
 letting the player randomly wander the world, even in car, without need
 of human assistance during crawling.
 Unfortunately, this package is not open-source.
\end_layout

\begin_layout Standard
Generally, the community is not united in their view on open-source.
 Some mods are available open-source on GitHub.
 Other mods are being distributed only as compiled binaries
\begin_inset CommandInset citation
LatexCommand cite
key "gta-5-mods"

\end_inset

.
 Lots of modders develop mostly by trial and error, and no comprehensive
 documentation for mod development is available, unfortunately.
 There are some tutorials 
\begin_inset CommandInset citation
LatexCommand cite
key "gta-5-mod-tutorial"

\end_inset

, but they are far from complete and provide only basic knowledge, leaving
 reader without deeper understanding of underlying principles.
\end_layout

\begin_layout Standard
Modders mostly meet online on few GTA forums, where they exchange knowledge
 
\begin_inset CommandInset citation
LatexCommand cite
key "gta-forums,gta-5-mods-forum"

\end_inset

.
 GitHub or Stack Overflow, which are biggest information sources for usual
 software development, are not used much in GTA modding community.
 Due to this fact, these forums, along with source code of open-source mods
 comprise knowledge-base of mod development.
\end_layout

\begin_layout Section
Simulation environment and development stack
\end_layout

\begin_layout Standard
In this thesis, I use mod based on 
\begin_inset CommandInset citation
LatexCommand cite
key "driving-in-matrix"

\end_inset

 but enhanced to gain more control of the game and to obtain more information
 from the game.
 
\end_layout

\begin_layout Standard
In later text, I'll refer to some GTA V native functions or data structures
 which are output of GTA V native functions.
 To be consistent and to help understanding, I will use function names from
 native function list 
\begin_inset CommandInset citation
LatexCommand cite
key "nativedb"

\end_inset

.
\end_layout

\begin_layout Standard
The basic architecture of C# mods come from the ScriptHookDotNet2, where
 each mod script extends the GTA.Script class.
 For each child of this class, we can set integer Interval, Tick and KeyUp
 callbacks.
 Interval property determines how big interval in milliseconds is between
 consecutive Tick calls.
 Tick callback is being called periodically, so here we can set tasks which
 we want to perform periodically, e.g.
 screenshot gathering.
 The KeyUp callback is for interacting with user and reading the user's
 keyboard input.
 For data gathering mods, this is mostly used for debugging purposes, script
 disabling or restarting.
 
\end_layout

\begin_layout Standard
Little, but useful note for debugging and developing scripts based on ScriptHook
DotNet2: when changing mod compiled binaries, we don't have to restart the
 game for newer version of mod to be active.
 Pressing Insert causes all C# binaries to reload, causing new version of
 source code to load into game, which dramatically decreases time of feedback
 loop during development.
 This does not work for C++ mods and compiled .asi files.
\end_layout

\begin_layout Standard
todo: popsat sem architekturu
\end_layout

\begin_layout Section
\begin_inset CommandInset label
LatexCommand label
name "sec:GTA-V-data-obtaining"

\end_inset

GTA V native API and data obtaining
\end_layout

\begin_layout Standard
The data obtained from GTA V can be divided into multiple categories.
\end_layout

\begin_layout Itemize
Image data
\end_layout

\begin_layout Itemize
Rendering pipeline matrices
\end_layout

\begin_layout Itemize
GTA V internal data
\end_layout

\begin_deeper
\begin_layout Itemize
entities
\end_layout

\begin_layout Itemize
camera
\end_layout

\begin_layout Itemize
player
\end_layout

\begin_layout Itemize
other data via API
\end_layout

\end_deeper
\begin_layout Subsection
Image data
\end_layout

\begin_layout Standard
There are 3 image data.
 RGB image, depth image and stencil image.
\end_layout

\begin_layout Standard
RGB image is usual camera image.
 Depth image is content of GPU's depth buffer, in NDC.
 More detailed description of depth values is in subsection 
\begin_inset CommandInset ref
LatexCommand ref
reference "subsec:Camera-to-NDC"

\end_inset

.
 The last is pixel-wise stencil buffer.
 The stencil semantics is explained in the next paragraph.
\end_layout

\begin_layout Subsubsection
Stencil data
\end_layout

\begin_layout Standard
Stencil buffer contains auxiliary data per pixel.
 It is 8bit unsigned integer, where 1.-4.
 bits (counting from LSB) contain object type ID, and 5.-8.
 bits contain certain flags.
 
\end_layout

\begin_layout Standard
That means there are 15 object types and 4 flags.
 
\end_layout

\begin_layout Standard
For some object type IDs, I reverse engineered its semantics based on correspond
ing RGB image.
 
\end_layout

\begin_layout Standard
\align center
\begin_inset Tabular
<lyxtabular version="3" rows="6" columns="3">
<features tabularvalignment="middle">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<row>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Object type ID binary
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Object type ID decimal
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Semantics
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0000
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
background (buildings, roads, hills...)
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0001
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
1
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
pedestrian
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0010
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
2
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
vehicle
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0011
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
3
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
tree, grass
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0111
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
7
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
sky
\end_layout

\end_inset
</cell>
</row>
</lyxtabular>

\end_inset


\end_layout

\begin_layout Standard
The background, pedestrian and vehicle IDs are most important for vehicles
 detection and semantic segmentation, and other object types are not so
 valuable for these tasks, which is why I didn't investigate further in
 semantics and they remain to be reverse-engineered.
\end_layout

\begin_layout Standard
For stencil flags, semantics was discovered for half of them.
 
\end_layout

\begin_layout Standard
\align center
\begin_inset Tabular
<lyxtabular version="3" rows="3" columns="3">
<features tabularvalignment="middle">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<row>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
position of bit
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
position in whole stencil value
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Semantics
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
5
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
xxx1xxxx
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
artificial light source
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
8
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
1xxxxxxx
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
player's character
\end_layout

\end_inset
</cell>
</row>
</lyxtabular>

\end_inset


\end_layout

\begin_layout Standard
Sample stencil image can be seen in 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Visualization-pipeline"

\end_inset

.
\end_layout

\begin_layout Standard
\align center
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename obrazky/2018-03-30--06-00-56--114-stencil-color-ids.png
	width 12cm

\end_inset


\end_layout

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename obrazky/2018-03-30--06-00-56--114.jpg
	lyxscale 30
	width 12cm

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:Sample-stencil-object"

\end_inset

Sample stencil object types image and corresponding RGB image
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Subsubsection
Extracting images from GPU's internal buffers
\end_layout

\begin_layout Standard
To understand how the image gathering works, we need to dive deeper into
 Microsoft Windows graphics.
\end_layout

\begin_layout Standard
In Microsoft Windows, the main graphics engine is DirectX (roughly Windows
 equivalent of OpenGL).
 One part of DirectX is Direct3D, which is used to render 3D graphics with
 hardware acceleration, and most importantly, it provides graphics API.
 The whole process of obtaining image data is done by mod provided as part
 of the paper 
\begin_inset CommandInset citation
LatexCommand cite
key "driving-in-matrix"

\end_inset

.
 The mod has two parts, called native and managed plugin.
\end_layout

\begin_layout Standard
Image data is being obtained by native plugin by hooking into Direct3D 11's
 present callback.
 That means the native call is replaced with custom code, which is being
 executed and then returns to the native call.
 In that custom code, content of GPU's buffers is copied.
 Specifically, the depth and stencil data are captured by hooking into Direct3D’
s ID3D11ImmediateContext:: ClearDepthStencilView and saving the buffers
 before each call.
 Because of optimizations applied by the graphics card drivers, the function
 needs to be re-hooked into the clear function each frame.
 When saving each sample, the managed plugin requests all current buffers
 from the native plugin and the buffers are downloaded from the GPU and
 copied into managed memory.
\begin_inset CommandInset citation
LatexCommand cite
key "driving-in-matrix"

\end_inset

.
 
\end_layout

\begin_layout Subsection
\begin_inset CommandInset label
LatexCommand label
name "subsec:Rendering-pipeline-data"

\end_inset

Rendering pipeline data
\end_layout

\begin_layout Standard
Direct3D allows us to get rendering pipelines through the D3D11_MAP_READ
 call.
 This way, the world, world-view, and world-view-projection matrices are
 obtained.
 Let's denote them as world matrix = 
\begin_inset Formula $W$
\end_inset

, world-view by = 
\begin_inset Formula $VW$
\end_inset

, and world-view-projection = 
\begin_inset Formula $PVW$
\end_inset

.
 We need to get individual matrices, which we obtain by multiplication by
 matrices inversion:
\begin_inset Formula 
\begin{align*}
V & =VW\cdot W^{-1}\\
P & =PVW\cdot\left(VW\right)^{-1}=PV\cdot W^{-1}\cdot V^{-1}
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
View and projection matrices are important on their own, without the world
 matrix.
 Their semantics and importance is more described in section 
\begin_inset CommandInset ref
LatexCommand ref
reference "sec:Reverse-engineering-the-RAGE"

\end_inset

.
 These matrices can be simply obtained by inversion as stated above.
 But this approach is not numerically stable, and sometimes causes resulting
 matrix to be incorrect and not usable for further usage.
 Another caveat of this approach is that during data gathering in higher
 speeds, the native call becomes laggy and resulting matrices are highly
 imprecise.
 Although we can obtain these matrices via the native call, they can be
 reconstructed with higher precision, as described in 
\begin_inset CommandInset ref
LatexCommand ref
reference "sec:Reverse-engineering-the-RAGE"

\end_inset

.
\end_layout

\begin_layout Subsection
GTA V internal data
\end_layout

\begin_layout Standard
These data are probably most valuable compared to data gathering by other
 methods.
 In this section, I describe which data about various game objects can be
 obtained and how to obtain them.
 
\end_layout

\begin_layout Standard
All data in this section are obtained though native calls into GTA V, which
 are listed here 
\begin_inset CommandInset citation
LatexCommand cite
key "nativedb"

\end_inset

.
 As mentioned above, they can be called from C++ and C#.
 For convenience, and because most of my mod development is done in C#,
 I will also describe the C# wrappers.
 The ScriptHookDotNet2
\begin_inset CommandInset citation
LatexCommand cite
key "scripthookvdotnet"

\end_inset

 wrapper heavily uses object properties.
 The C# API is divided into multiple classes, listed 
\begin_inset CommandInset href
LatexCommand href
name "here"
target "https://github.com/crosire/scripthookvdotnet/tree/dev_v3/source/scripting"

\end_inset

.
 Each class has properties, whose getters and setters are implemented as
 calling the native functions.
 This feature is nice tooling and leads to more readable and maintainable
 code.
 I will describe parts of API which are most useful for synthetic datasets
 creation.
\end_layout

\begin_layout Subsubsection
Coordinate systems and axes
\end_layout

\begin_layout Standard
The model, world and view coordinate systems are all in meters.
 In the model coordinate system, e.g.
 when we have coordinate system of a car, Z-axis is oriented upwards, Y-axis
 is oriented in front of the car, and X-axis to the right of the car.
 In the camera coordinate system, the Z-axis is oriented behind the camera,
 Y-axis upwards of camera and the X-axis to the left of camera.
 In the world coordinate system, if the camera has rotation = 
\begin_inset Formula $\left(0,0,0\right)$
\end_inset

, it is heading in direction of Y-axis, X-axis is heading right of the camera,
 and Z axis is heading upwards.
\end_layout

\begin_layout Subsubsection
Game state manipulation
\end_layout

\begin_layout Standard
In the ScriptHookVDotNet2, where is Game class, which is the main entry-point
 for most of game state manipulation.
 Here I'll describe methods and properties useful and needed for data retrieval.
\end_layout

\begin_layout LyX-Code
Game.Pause(true)
\end_layout

\begin_layout Standard
pauses the whole game.
 
\end_layout

\begin_layout LyX-Code
Game.Pause(false)
\end_layout

\begin_layout Standard
un-pauses the game.
 The 
\end_layout

\begin_layout Standard
\begin_inset Flex Code
status open

\begin_layout Plain Layout
Game.ScreenResolution
\end_layout

\end_inset


\end_layout

\begin_layout Standard
property returns Size object with height and width of the current screen,
 so 
\end_layout

\begin_layout LyX-Code
Game.ScreenResolution.Width; Game.ScreenResolution.Height;
\end_layout

\begin_layout Standard
returns screen width and height respectively.
 As seen above, the main player character can be accessed by 
\begin_inset Flex Code
status open

\begin_layout Plain Layout
Game.Player.Character
\end_layout

\end_inset

 property.
 
\end_layout

\begin_layout Standard
The Player Character property is used for handling the main player.
 It has following method important for data retrieval.
 
\begin_inset Flex Code
status open

\begin_layout Plain Layout
Game.Player.Character.Position
\end_layout

\end_inset

 provides position of player in world coordinates.
 This is useful if we want to obtain entities only in certain distance of
 player or if we want to spawn new vehicle on player's position.
 Similarly to position, we can access player's current velocity by 
\end_layout

\begin_layout LyX-Code
Game.Player.Character.Velocity
\end_layout

\begin_layout Standard
which returns the velocity 3D vector.
 If we have a vehicle, we set the player as driver as follows 
\end_layout

\begin_layout LyX-Code
Game.Player.Character.SetIntoVehicle(vehicle, 
\end_layout

\begin_layout LyX-Code
VehicleSeat.Driver)
\end_layout

\begin_layout Standard
.
 When player is in a vehicle, we can access it later by the 
\end_layout

\begin_layout LyX-Code
Game.Player.Character.CurrentVehicle
\end_layout

\begin_layout Standard
property.
 
\end_layout

\begin_layout Standard
Other important Class is World, which, as seen above, provides the camera
 handling interface, and many other features.
 As mentioned above, the World.DestroyAllCameras() clears all scripted cameras.
 The 
\begin_inset Flex Code
status open

\begin_layout Plain Layout
World.CreateCamera()
\end_layout

\end_inset

 creates new scripted camera.
 The 
\begin_inset Flex Code
status open

\begin_layout Plain Layout
World.RenderingCamera
\end_layout

\end_inset

 holds reference to the currently active scripted camera.
 Setting the camera to the 
\begin_inset Flex Code
status open

\begin_layout Plain Layout
World.RenderingCamera
\end_layout

\end_inset

 activates that camera and starts rendering with that camera, and setting
 null reference returns the view to the Gameplay camera, which is default
 camera used during playing.
 
\end_layout

\begin_layout Standard
Other methods allow us directly manipulate the world.
 The 
\begin_inset Flex Code
status open

\begin_layout Plain Layout
World.CurrentDayTime
\end_layout

\end_inset

 returns the TimeSpan object, which is time in day in the GTA world.
 By setting this property we can change the time of day as we need by assigning
 the TimeSpan instance 
\end_layout

\begin_layout LyX-Code
World.CurrentDayTime = 
\end_layout

\begin_layout LyX-Code
new TimeSpan(int hours, int minutes, int seconds)
\end_layout

\begin_layout Standard
.
 The 
\begin_inset Flex Code
status open

\begin_layout Plain Layout
World.Weather
\end_layout

\end_inset

 uses same getter, setter interface, so 
\begin_inset Flex Code
status open

\begin_layout Plain Layout
World.Weather
\end_layout

\end_inset

 returns current weather, and 
\begin_inset Flex Code
status open

\begin_layout Plain Layout
World.Weather = Weather.Foggy
\end_layout

\end_inset

 sets the weather to be foggy.
 List of all possible weathers is in the Weather enum, which contains Unknown,
 ExtraSunny, Clear, Clouds, Smog, Foggy, Overcast, Raining, ThunderStorm,
 Clearing, Neutral, Snowing, Blizzard, Snowlight, Christmas, Halloween.
 
\end_layout

\begin_layout Standard
The World class also contains the 
\end_layout

\begin_layout LyX-Code
World.CreateVehicle(Model model, Vector3 position)
\end_layout

\begin_layout Standard
which spawn new car in the game and returns reference to the newly spawned
 car.
 Position is in world coordinates and model can be any of vehicle models
 in game.
 For instance,
\end_layout

\begin_layout LyX-Code
World.CreateVehicle(new Model(VehicleHash.Seven70), 
\end_layout

\begin_layout LyX-Code
Game.Player.Character.Position)
\end_layout

\begin_layout Standard
creates new sports car in player's position .
 Whole list of known models is available in 
\begin_inset Flex URL
status open

\begin_layout Plain Layout

https://github.com/crosire/scripthookvdotnet/blob/dev_v3/source/scripting/World/E
ntities/Vehicles/VehicleHash.cs
\end_layout

\end_inset

 where 554 model hashes are enumerated.
 
\end_layout

\begin_layout Subsubsection
Camera
\end_layout

\begin_layout Standard
The camera is probably one of the most crucial parts of the API.
 There is Gameplay Camera, which is the default camera used during usual
 playing.
 This camera can be manipulated, but its usage is limited.
 Other approach is usage of scripted cameras, which can be fully controlled
 programmatically.
 We can create multiple scripted cameras and switch between them, but the
 community discovered there is hard limit of 26 cameras at time
\begin_inset CommandInset citation
LatexCommand cite
key "gta-mod-camera"

\end_inset

.
 Camera can be created by calling 
\end_layout

\begin_layout LyX-Code
Camera camera = World.CreateCamera(
\end_layout

\begin_layout LyX-Code
new Vector3(x, y, z), new Vector3(x, y, z), float fov);
\end_layout

\begin_layout Standard
which returns handle to the new camera.
 The position is in world coordinates in meters.
 The rotation is in degrees as rotation around particular axis, as in OpenGL
 engine.
 In the Aircraft principal axes terminology, the 
\begin_inset Formula $\left(x,y,z\right)$
\end_inset

 rotation vector means 
\begin_inset Formula $\left(pitch,roll,yaw\right)$
\end_inset

 respectively.
 The fov argument is vertical field of view in degrees.
 The default value is 50.
 One can of course call the underlying native functions directly, but this
 wrapper helps with managing the handles.
 
\end_layout

\begin_layout Standard
All scripted cameras can be destroyed by calling the 
\end_layout

\begin_layout LyX-Code
World.DestroyAllCameras(); 
\end_layout

\begin_layout Standard
Switching to the scripted camera can be done by calling 
\end_layout

\begin_layout LyX-Code
camera.IsActive = true;
\end_layout

\begin_layout Standard
, and switching from this camera to some other by 
\end_layout

\begin_layout LyX-Code
camera.IsActive = false; 
\end_layout

\begin_layout Standard
Right after creating the camera, when we don't to switch to this camera
 immediately, we need to deactivate it by these two lines of code
\end_layout

\begin_layout LyX-Code
camera.IsActive = false;
\end_layout

\begin_layout LyX-Code
World.RenderingCamera = null;
\end_layout

\begin_layout Standard
this code ensures that camera is created properly.
 We don't know precisely, why is this needed, but this is the price for
 using the closed and reverse-engineered codebase.
 By calling the
\end_layout

\begin_layout LyX-Code
camera.IsActive = true; 
\end_layout

\begin_layout LyX-Code
World.RenderingCamera = camera;
\end_layout

\begin_layout Standard
scripted camera becomes active and view is switched to this camera.
 All properties of camera can be set simply by calling setters
\end_layout

\begin_layout LyX-Code
camera.position = new Vector3(x, y, z);
\end_layout

\begin_layout LyX-Code
camera.rotation = new Vector3(y, x, z);
\end_layout

\begin_layout LyX-Code
camera.nearClip = distance;
\end_layout

\begin_layout LyX-Code
camera.farClip = distance;
\end_layout

\begin_layout LyX-Code
camera.FieldOfView = fov;
\end_layout

\begin_layout Standard
and read by calling getters 
\end_layout

\begin_layout LyX-Code
Vector3 position = camera.position;
\end_layout

\begin_layout LyX-Code
Vector3 rotation = camera.rotation;
\end_layout

\begin_layout LyX-Code
float distance = camera.nearClip;
\end_layout

\begin_layout LyX-Code
float distance = camera.farClip;
\end_layout

\begin_layout LyX-Code
float fov = camera.FieldOfView;
\end_layout

\begin_layout Standard
The near clip and far clip is again in meters.
 So we can set all needed parameters simply by calling getters and setters.
 So if we want to use camera, we simply set parameters and then activate
 it.
 This creates the static camera.
 
\end_layout

\begin_layout Standard
Sometimes we want the camera to be moving, e.g.
 when gathering data by driving car.
 Camera can be attached to any entity by calling 
\end_layout

\begin_layout LyX-Code
camera.AttachTo(entity, new Vector3(x, y, z));
\end_layout

\begin_layout Standard
the second parameter is relative offset to middle of the attached entity.
 The offset is in model coordinate system which means its position moves
 and rotates with attached entity.
 During dataset gathering, I used this code
\end_layout

\begin_layout LyX-Code
camera.AttachTo(Game.Player.Character.CurrentVehicle, 
\end_layout

\begin_layout LyX-Code
new Vector3(0f, 2f, 0.4f));
\end_layout

\begin_layout Standard
to attach camera to the front part of player's current vehicle.
 As seen from code, it attaches camera 2 meters in front of mar model's
 centre and 40cm above it, making it effectively sitting on top of the hood
 of the car.
 These parameters are taken from 
\begin_inset CommandInset citation
LatexCommand cite
key "video-games-for-autonomous-driving"

\end_inset

.
 
\end_layout

\begin_layout Standard
When we have camera attached to the car, we need to update periodically
 its rotation, otherwise it will be heading the same direction in world
 coordinates and won't rotate with car it is attached to.
 So when we have our Tick method, which is being called periodically, we
 need to update the active camera like this:
\end_layout

\begin_layout LyX-Code
camera.AttachTo(Game.Player.Character.CurrentVehicle, 
\end_layout

\begin_layout LyX-Code
cameraPosition);
\end_layout

\begin_layout LyX-Code
camera.Rotation = cameraRotation;
\end_layout

\begin_layout Standard
if we want to have our camera rotated in fixed offset compared to car rotation
 (e.g.
 camera looking behind the car), we simply set camera's rotation as offset
 like this
\end_layout

\begin_layout LyX-Code
camera.AttachTo(Game.Player.Character.CurrentVehicle, 
\end_layout

\begin_layout LyX-Code
new Vector3(0f, -2f, 0.6f));  // now our camera is sitting in back of the
 car
\end_layout

\begin_layout LyX-Code
camera.Rotation = Game.Player.Character.CurrentVehicle.Rotation 
\end_layout

\begin_layout LyX-Code
+ new Vector3(0f, 0f, 180f);
\end_layout

\begin_layout Standard
which sets camera rotation in world coordinates as a sum of car's rotation
 and relative rotation of camera, 180° yaw.
\end_layout

\begin_layout Subsubsection
Game entities
\end_layout

\begin_layout Standard
All game entities extend the abstract Entity class.
 Important children of Entity class are Ped, Vehicle and Prop classes, wrapping
 pedestrians, vehicles, and various objects, respectively.
 List of all entities contained in the entity pool can be obtained by 
\begin_inset Flex Code
status open

\begin_layout Plain Layout
World.GetAllEntities()
\end_layout

\end_inset

.
 Methods 
\end_layout

\begin_layout LyX-Code
World.GetAllPeds(); World.GetAllVehicles(); World.GetAllProps();
\end_layout

\begin_layout Standard
will return list of all pedestrians, vehicles, and props, respectively.
 These lists are huge and we don't need all of returned entities usually.
 Most of the time, we are interested only in objects on the screen, which
 are only in certain distance from us.
 The most straightforward option is to filter these lists afterwards, but
 luckily, the library offers us helper methods for filtering the nearby
 entities more effectively, without need to instantiate more distant entities.
 These helper methods are 
\end_layout

\begin_layout LyX-Code
World.GetNearbyEntities(Vector3 position, float radius)
\end_layout

\begin_layout Standard
, and analogously, 
\end_layout

\begin_layout LyX-Code
World.GetNearbyPeds(Vector3 position, float radius); 
\end_layout

\begin_layout LyX-Code
World.GetNearbyVehicles(Vector3 position, float radius); 
\end_layout

\begin_layout LyX-Code
World.GetNearbyProps(Vector3 position, float radius);
\end_layout

\begin_layout Standard
.
 As expected, radius is in meters, position is in world coordinates.
 So obtaining for instance all vehicles up to half kilometres distant from
 the current player, we call 
\end_layout

\begin_layout LyX-Code
World.GetNearbyVehicles(Game.Player.Character, 500.0f)
\end_layout

\begin_layout Standard
.
 Every entity has position, rotation, velocity, and handle.
 Position and rotation have similar semantics as cameras.
 Velocity is velocity as a 3D vector.
 The handle is unique in-game identifier of particular entity, allowing
 identifying it across whole game during data gathering, and lets us identify
 same entity on multiple images, which is useful for identifying motion
 of entity through different images.
 By calling 
\begin_inset Flex Code
status open

\begin_layout Plain Layout

\family roman
entity.Model.Hash
\end_layout

\end_inset

 on a particular entity, we obtain its hash, which can be used to spawn
 new entities with same model.
 For vehicles, there is 
\begin_inset Flex Code
status open

\begin_layout Plain Layout

\family roman
entity.ClassType
\end_layout

\end_inset

 property, describing type as one of 21 vehicle types.
 Whole list can be seen in the enum VehicleClass in the 
\begin_inset Flex URL
status open

\begin_layout Plain Layout

https://github.com/crosire/scripthookvdotnet/blob/dev_v3/source/scripting/World/E
ntities/Vehicles/Vehicle.cs
\end_layout

\end_inset

.
 
\end_layout

\begin_layout Subsubsection
Gathering data from active camera
\end_layout

\begin_layout Standard
In this part, I'll describe how to access data from internal GPU buffers
 and how to persist these data.
 
\end_layout

\begin_layout Standard
Before we gather buffers contents, we want to pause the game.
 Since we don't get the data from buffers perfectly synchronized, we pause
 the game to make sure RGB, depth and stencil buffer contain data coming
 from same frame.
 
\begin_inset Flex Code
status open

\begin_layout Plain Layout
Game.Pause(true)
\end_layout

\end_inset

 pauses the game.
 The whole data retrieval from buffers is done via GTAVisionExport native
 plugin 
\begin_inset CommandInset citation
LatexCommand cite
key "driving-in-matrix"

\end_inset

.
 When the game is stopped, we can obtain RGB, depth and stencil buffer contents
 by 
\end_layout

\begin_layout LyX-Code
var color = VisionNative.GetColorBuffer();
\end_layout

\begin_layout LyX-Code
var depth = VisionNative.GetDepthBuffer();
\end_layout

\begin_layout LyX-Code
var stencil = VisionNative.GetStencilBuffer(); 
\end_layout

\begin_layout Standard
and then, we save it to the filesystem as TIFF image.
 Although TIFF is not the most used format for data, it is able to persist
 image whose pixels contain float values, which is crucial for depth buffer.
\end_layout

\begin_layout Standard
The RGB buffer is usual 8bit unsigned integer RGBA image.
 The depth buffer contains float values, with 32bits precision, in range
 from 0 to 1, representing depth value in NDC space.
 The stencil buffer contains 8bit unsigned integer image.
 
\end_layout

\begin_layout Section
\begin_inset CommandInset label
LatexCommand label
name "sec:Reverse-engineering-the-RAGE"

\end_inset

Reverse-engineering the RAGE rendering pipeline
\end_layout

\begin_layout Standard
As mentioned above 
\begin_inset CommandInset ref
LatexCommand ref
reference "rage"

\end_inset

, GTA V uses proprietary game engine, Rockstar Advanced Game Engine (RAGE).
 The basic premise of rendering pipeline is same as in well known graphics
 engines like OpenGL.
 The pipeline is shown in figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Visualization-pipeline"

\end_inset

.
 Following section will be discussing mostly computer graphics related problems.
 Due to some terminology inconsistency between computer graphics and computer
 vision, all terms used here will be computer graphics related.
 Probably most confusion here could be caused by projection matrix.
 In computer vision, projection matrix is projection from 3D to 2D, the
 matrix reduces dimension.
 In computer graphics, all coordinates are kept in 4D, in homogeneous coordinate
s as long as possible.
 Here the projection matrix represents projection from frustum seen by eye
 into cuboid space of Normalized Device Coordinates.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename obrazky/transformationPipeline.png
	lyxscale 30
	scale 65

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:Visualization-pipeline"

\end_inset

Rendering pipeline
\end_layout

\end_inset


\end_layout

\end_inset

In is part, I will describe some transformations between individual RAGE
 coordinate systems.
 Some points here will have part of name in lower index.
 The name of coordinate system will be denoted in upper index.
 In RAGE there are 6 coordinate systems.
\end_layout

\begin_layout Standard
\align center
\begin_inset Tabular
<lyxtabular version="3" rows="7" columns="3">
<features tabularvalignment="middle">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<row>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Name
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Abbreviation
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Example point 
\begin_inset Formula $x$
\end_inset


\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Object Coordinates
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
O
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $x^{O}$
\end_inset


\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
World Coordinates
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
W
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $x^{W}$
\end_inset


\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Camera Coordinates
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
C
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $x^{C}$
\end_inset


\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Clip Coordinates
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
L
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $x^{L}$
\end_inset


\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Normalized Device Coordinates
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
NDC
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $x^{NDC}$
\end_inset


\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Windows Coordinates
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
P
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $x^{P}$
\end_inset


\end_layout

\end_inset
</cell>
</row>
</lyxtabular>

\end_inset


\end_layout

\begin_layout Standard
Most of points we handle in GTA already are in world coordinates.
 
\end_layout

\begin_layout Standard
But some points, like GAMEPLAY::GET_MODEL_DIMENSIONS
\end_layout

\begin_layout Standard
\begin_inset Formula $=\begin{pmatrix}x_{max}^{O} & y_{max}^{O} & z_{max}^{O}\end{pmatrix}\begin{pmatrix}x_{min}^{O} & y_{min}^{O} & z_{min}^{O}\end{pmatrix}$
\end_inset

 output, are in object coordinates.
 Transitions between adjacent coordinate systems will be demonstrated on
 model dimensions because it is on the few vectors which are obtained in
 Object Coordinates and there is need to project them into Window Coordinates.
\end_layout

\begin_layout Subsection
Object to World Coordinates
\end_layout

\begin_layout Standard
To get world coordinates of model dimensions, we use traditional rigid body
 transformation based on ENTITY::GET_ENTITY_ROTATION
\begin_inset Formula $=\begin{pmatrix}\alpha & \beta & \gamma\end{pmatrix}$
\end_inset

 Euler angles, 
\end_layout

\begin_layout Standard
and ENTITY::GET_ENTITY_COORDS
\begin_inset Formula $=\begin{pmatrix}x^{W} & y^{W} & z^{W}\end{pmatrix}$
\end_inset

.
\end_layout

\begin_layout Standard
Because all coordinates will be homogeneous coordinates, the above-mentioned
 model dimensions vectors will be transformed to following form 
\begin_inset Formula $\begin{pmatrix}x_{max}^{O} & y_{max}^{O} & z_{max}^{O} & 1\end{pmatrix}\begin{pmatrix}x_{min}^{O} & y_{min}^{O} & z_{min}^{O} & 1\end{pmatrix}$
\end_inset

.
 
\end_layout

\begin_layout Standard
The transition is represented by model matrix
\begin_inset Formula 
\begin{align*}
M & =\begin{bmatrix}1 & 0 & 0 & x^{W}\\
0 & 1 & 0 & y^{W}\\
0 & 0 & 1 & z^{W}\\
0 & 0 & 0 & 1
\end{bmatrix}\begin{bmatrix}1 & 0 & 0 & 0\\
0 & \cos\left(\alpha\right) & -\sin\left(\alpha\right) & 0\\
0 & \sin\left(\alpha\right) & \cos\left(\alpha\right) & 0\\
0 & 0 & 0 & 1
\end{bmatrix}\begin{bmatrix}\cos\left(\beta\right) & 0 & \sin\left(\beta\right) & 0\\
0 & 1 & 0 & 0\\
-\sin\left(\beta\right) & 0 & \cos\left(\beta\right) & 0\\
0 & 0 & 0 & 1
\end{bmatrix}\begin{bmatrix}\cos\left(\gamma\right) & -\sin\left(\gamma\right) & 0 & 0\\
\sin\left(\gamma\right) & \cos\left(\gamma\right) & 0 & 0\\
0 & 0 & 1 & 0\\
0 & 0 & 0 & 1
\end{bmatrix}\\
 & =\begin{bmatrix}\cos\left(\beta\right)\cos\left(\gamma\right) & -\cos\left(\beta\right)\sin\left(\gamma\right) & \sin\left(\beta\right) & x^{W}\\
\sin\left(\alpha\right)\sin\left(\beta\right)\cos\left(\gamma\right)+\cos\left(\alpha\right)\sin\left(\gamma\right) & \cos\left(\alpha\right)\cos\left(\gamma\right)-\sin\left(\alpha\right)\sin\left(\beta\right)\sin\left(\gamma\right) & -\sin\left(\alpha\right)\cos\left(\beta\right) & y^{W}\\
\sin\left(\alpha\right)\sin\left(\gamma\right)-\cos\left(\alpha\right)\sin\left(\beta\right)\cos\left(\gamma\right) & \cos\left(\alpha\right)\sin\left(\beta\right)\sin\left(\gamma\right)+\sin\left(\alpha\right)\cos\left(\gamma\right) & \cos\left(\alpha\right)\cos\left(\beta\right) & z^{W}\\
0 & 0 & 0 & 1
\end{bmatrix}
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
and whole transformation is, as expected 
\begin_inset Formula 
\[
M\begin{bmatrix}x_{max}^{O} & x_{min}^{O}\\
y_{max}^{O} & y_{min}^{O}\\
z_{max}^{O} & z_{min}^{O}\\
1 & 1
\end{bmatrix}=\begin{bmatrix}x_{max}^{W} & x_{min}^{W}\\
y_{max}^{W} & y_{min}^{W}\\
z_{max}^{W} & z_{min}^{W}\\
w_{max}^{W} & w_{min}^{W}
\end{bmatrix}
\]

\end_inset


\end_layout

\begin_layout Subsection
\begin_inset CommandInset label
LatexCommand label
name "subsec:World-to-Camera"

\end_inset

World to Camera Coordinates
\end_layout

\begin_layout Standard
The transformation from world coordinates is principally the same, but counter-i
ntuitive in definition of used rotation matrices.
 It also is rigid body transformation, but rotation is defined differently
 than we are usually used to in computer graphics.
 The rotation matrices were reverse engineered as part of this thesis from
 camera position, rotation and resulting view matrix, this coordinate system
 is nowhere else documented.
 The camera position is CAM::GET_CAM_COORD
\begin_inset Formula $=\begin{pmatrix}x^{W} & y^{W} & z^{W}\end{pmatrix}$
\end_inset

 and the camera rotation is CAM::GET_CAM_ROT
\begin_inset Formula $=\begin{pmatrix}\alpha & \beta & \gamma\end{pmatrix}$
\end_inset

.
\end_layout

\begin_layout Standard
The transformation is represented by view matrix
\begin_inset Formula 
\begin{align*}
V & =\begin{bmatrix}1 & 0 & 0 & 0\\
0 & \sin\left(\alpha\right) & \cos\left(\alpha\right) & 0\\
0 & \cos\left(\alpha\right) & -\sin\left(\alpha\right) & 0\\
0 & 0 & 0 & 1
\end{bmatrix}\begin{bmatrix}\cos\left(\beta\right) & 0 & -\sin\left(\beta\right) & 0\\
0 & 1 & 0 & 0\\
\sin\left(\beta\right) & 0 & \cos\left(\beta\right) & 0\\
0 & 0 & 0 & 1
\end{bmatrix}\begin{bmatrix}\cos\left(\gamma\right) & \sin\left(\gamma\right) & 0 & 0\\
\sin\left(\gamma\right) & -\cos\left(\gamma\right) & 0 & 0\\
0 & 0 & 1 & 0\\
0 & 0 & 0 & 1
\end{bmatrix}\begin{bmatrix}1 & 0 & 0 & x^{W}\\
0 & 1 & 0 & y^{W}\\
0 & 0 & 1 & z^{W}\\
0 & 0 & 0 & 1
\end{bmatrix}
\end{align*}

\end_inset

to fit the matrix into page, let us propose following substitutions 
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\cos\left(\alpha\right)=c_{\alpha},sin\left(\alpha\right)=s_{\alpha}
\]

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\cos\left(\beta\right)=c_{\beta},sin\left(\beta\right)=s_{\beta}
\]

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\cos\left(\gamma\right)=c_{\gamma},sin\left(\gamma\right)=s_{\gamma}
\]

\end_inset


\begin_inset Formula 
\begin{align}
V & =\begin{bmatrix}c_{\beta}c_{\gamma} & c_{\beta}s_{\gamma} & -s_{\beta} & 0\\
c_{\alpha}s_{\beta}c_{\gamma}+s_{\alpha}s_{\gamma} & c_{\alpha}s_{\beta}s_{\gamma}-s_{\alpha}c_{\gamma} & c_{\alpha}c_{\beta} & 0\\
c_{\alpha}s_{\gamma}-s_{\alpha}s_{\beta}c_{\gamma} & -s_{\alpha}s_{\beta}s_{\gamma}-c_{\alpha}c_{\gamma} & -s_{\alpha}c_{\beta} & 0\\
0 & 0 & 0 & 1
\end{bmatrix}\begin{bmatrix}1 & 0 & 0 & x^{W}\\
0 & 1 & 0 & y^{W}\\
0 & 0 & 1 & z^{W}\\
0 & 0 & 0 & 1
\end{bmatrix}\nonumber \\
 & =\begin{bmatrix}c_{\beta}c_{\gamma} & c_{\beta}s_{\gamma} & -s_{\beta} & x^{W}c_{\beta}c_{\gamma}+y^{W}c_{\beta}s_{\gamma}-z^{W}s_{\beta}\\
c_{\alpha}s_{\beta}c_{\gamma}+s_{\alpha}s_{\gamma} & c_{\alpha}s_{\beta}s_{\gamma}-s_{\alpha}c_{\gamma} & c_{\alpha}c_{\beta} & x^{W}\left(c_{\alpha}s_{\beta}c_{\gamma}+s_{\alpha}s_{\gamma}\right)+y^{W}\left(c_{\alpha}s_{\beta}s_{\gamma}-s_{\alpha}c_{\gamma}\right)+z^{W}c_{\alpha}c_{\beta}\\
c_{\alpha}s_{\gamma}-s_{\alpha}s_{\beta}c_{\gamma} & -s_{\alpha}s_{\beta}s_{\gamma}-c_{\alpha}c_{\gamma} & -s_{\alpha}c_{\beta} & x^{W}\left(c_{\alpha}s_{\gamma}-s_{\alpha}s_{\beta}c_{\gamma}\right)+y^{W}\left(-s_{\alpha}s_{\beta}s_{\gamma}-c_{\alpha}c_{\gamma}\right)-z^{W}s_{\alpha}c_{\beta}\\
0 & 0 & 0 & 1
\end{bmatrix}\label{eq:camera-m}
\end{align}

\end_inset


\end_layout

\begin_layout Standard
and whole transformation is, as expected 
\begin_inset Formula 
\[
V\begin{bmatrix}x_{max}^{W} & x_{min}^{W}\\
y_{max}^{W} & y_{min}^{W}\\
z_{max}^{W} & z_{min}^{W}\\
w_{max}^{W} & w_{min}^{W}
\end{bmatrix}=\begin{bmatrix}x_{max}^{C} & x_{min}^{C}\\
y_{max}^{C} & y_{min}^{C}\\
z_{max}^{C} & z_{min}^{C}\\
w_{max}^{C} & w_{min}^{C}
\end{bmatrix}
\]

\end_inset


\end_layout

\begin_layout Standard
From definition of rotation axes in the rotation matrices, following observation
 can be made.
 
\begin_inset Formula $z^{C}$
\end_inset

 represents distance from camera in direction of camera heading, and 
\begin_inset Formula $x^{C}$
\end_inset

and 
\begin_inset Formula $y^{C}$
\end_inset

 represent horizontal and vertical position of point relative to camera,
 respectively.
 But the view frustum of camera is in opposite direction than 
\begin_inset Formula $z^{C}$
\end_inset

axis, which means the camera 
\begin_inset Quotes eld
\end_inset

is looking
\begin_inset Quotes erd
\end_inset

 into negative 
\begin_inset Formula $z^{C}$
\end_inset

 coordinates.
\end_layout

\begin_layout Subsection
\begin_inset CommandInset label
LatexCommand label
name "subsec:Camera-to-NDC"

\end_inset

Camera to NDC
\end_layout

\begin_layout Standard
This is the first transformation which is not rigid-body transformation.
 Because camera sees only frustum, this transformation represents transition
 from frustum to cuboid in Normalized Device Coordinates.
 The frustum being projected is specified by near clip, far clip, field
 of view and screen resolution width and height.
 Usually, none of these parameters are changing during the game, so the
 projection matrix is usually the same for multiple scenes during data gathering
 session.
 Although all of these parameters can be changed programmatically if needed.
 
\end_layout

\begin_layout Standard
The near clip and far clip of camera can be obtained by 
\begin_inset CommandInset label
LatexCommand label
name "native-call-near-clip"

\end_inset

 CAM::GET_CAM_NEAR_CLIP
\begin_inset Formula $=n_{c}$
\end_inset

 and CAM::GET_CAM_FAR_CLIP
\begin_inset Formula $=f_{c}$
\end_inset

.
 Width and height of screen resolution are obtained by GRAPHICS::_GET_ACTIVE_SCR
EEN_RESOLUTION
\begin_inset Formula $=\begin{pmatrix}W & H\end{pmatrix}$
\end_inset

 and field of view of camera by CAM::GET_CAM_FOV
\begin_inset Formula $=\varphi_{VD}$
\end_inset

 in degrees.
 
\begin_inset Formula $\varphi_{VD}$
\end_inset

 in radians will be denoted as 
\begin_inset Formula $\varphi_{VR}$
\end_inset

.
\end_layout

\begin_layout Standard
The near clip and far clip define planes between which the content is being
 rendered.
 Nothing before the near clip and behind the far clip is rendered.
\end_layout

\begin_layout Standard
The field of view 
\begin_inset Formula $\varphi_{VD}$
\end_inset

 is only vertical.
 Horizontal field of view can be calculated from 
\begin_inset Formula $W$
\end_inset

 and 
\begin_inset Formula $H$
\end_inset

 ratio, but currently we don't need it.
\end_layout

\begin_layout Standard
There is important observation, the far clip 
\begin_inset Formula $f_{c}$
\end_inset

 does not figure in the projection matrix at all.
 In the projection matrix, only 
\begin_inset Formula $n_{c}$
\end_inset

 is used.
 Far clip used in projection matrix is non-changing value which can not
 be obtained through Camera native function.
 By reverse-engineering I calculated the value of this new far clip to be
 
\begin_inset Formula $10003.815$
\end_inset

, details of this calculation are covered in experiments
\begin_inset CommandInset ref
LatexCommand ref
reference "subsec:Reverse-engineering"

\end_inset

.
 
\end_layout

\begin_layout Standard
The transformation is represented by projection matrix
\begin_inset Formula 
\begin{align}
P & =\begin{bmatrix}\frac{H}{W\cdot\tan\left(\frac{\varphi_{VR}}{2}\right)} & 0 & 0 & 0\\
0 & \frac{1}{\tan\left(\frac{\varphi_{VR}}{2}\right)} & 0 & 0\\
0 & 0 & \frac{-10003.815}{n_{c}-10003.815} & \frac{-10003.815\cdot n_{c}}{n_{c}-10003.815}\\
0 & 0 & -1 & 0
\end{bmatrix}\label{eq:projection-m}
\end{align}

\end_inset


\end_layout

\begin_layout Standard
So the projection to Clip Coordinates is 
\begin_inset Formula 
\[
P\begin{bmatrix}x_{max}^{C} & x_{min}^{C}\\
y_{max}^{C} & y_{min}^{C}\\
z_{max}^{C} & z_{min}^{C}\\
w_{max}^{C} & w_{min}^{C}
\end{bmatrix}=\begin{bmatrix}x_{max}^{L} & x_{min}^{L}\\
y_{max}^{L} & y_{min}^{L}\\
z_{max}^{L} & z_{min}^{L}\\
w_{max}^{L} & w_{min}^{L}
\end{bmatrix}
\]

\end_inset


\end_layout

\begin_layout Standard
The transition between Clip Coordinates and NDC is only division by width,
 so it is
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\begin{bmatrix}x_{max}^{L} & x_{min}^{L}\\
y_{max}^{L} & y_{min}^{L}\\
z_{max}^{L} & z_{min}^{L}\\
w_{max}^{L} & w_{min}^{L}
\end{bmatrix}\circ\begin{bmatrix}\frac{1}{w_{max}^{L}} & \frac{1}{w_{min}^{L}}\\
\frac{1}{w_{max}^{L}} & \frac{1}{w_{min}^{L}}\\
\frac{1}{w_{max}^{L}} & \frac{1}{w_{min}^{L}}\\
\frac{1}{w_{max}^{L}} & \frac{1}{w_{min}^{L}}
\end{bmatrix}=\begin{bmatrix}x_{max}^{NDC} & x_{min}^{NDC}\\
y_{max}^{NDC} & y_{min}^{NDC}\\
z_{max}^{NDC} & z_{min}^{NDC}\\
1 & 1
\end{bmatrix}
\]

\end_inset

where 
\begin_inset Formula $\circ$
\end_inset

 is Hadamard product, also known as entry-wise product or element-wise matrix
 multiplication.
 
\end_layout

\begin_layout Standard
Let us have vector 
\begin_inset Formula $\boldsymbol{x}=\begin{bmatrix}x & y & z & w\end{bmatrix}^{T}$
\end_inset

in both coordinate systems, 
\begin_inset Formula $\boldsymbol{x}^{L}=\begin{bmatrix}x^{L} & y^{L} & z^{L} & w^{L}\end{bmatrix}^{T}$
\end_inset

, 
\begin_inset Formula $\boldsymbol{x}^{NDC}=\begin{bmatrix}x^{NDC} & y^{NDC} & z^{NDC} & w^{NDC}\end{bmatrix}^{T}$
\end_inset

.
 Then, the relation between Clip Coordinates and NDC can also be expressed
 by following relationship 
\begin_inset Formula 
\[
\boldsymbol{x}^{L}=\begin{bmatrix}x^{L}\\
y^{L}\\
z^{L}\\
w^{L}
\end{bmatrix}=\begin{bmatrix}x^{NDC}w^{L}\\
y^{NDC}w^{L}\\
z^{NDC}w^{L}\\
w^{L}
\end{bmatrix}=w^{L}\begin{bmatrix}x^{NDC}\\
y^{NDC}\\
z^{NDC}\\
1
\end{bmatrix}=w^{L}\boldsymbol{x}^{NDC}
\]

\end_inset


\end_layout

\begin_layout Standard
The view frustum was now transformed into NDC cuboid.
 The NDC cuboid has dimensions 
\begin_inset Formula $x\in\left[-1,1\right],y\in\left[-1,1\right],z\in\left[0,1\right]$
\end_inset

.
 The x and y coordinates are intuitive, but the z-axis is reverted, so near
 clip is being mapped to 1 and far clip is being mapped to 0.
 The NDC is important because it is coordinate space in which GPU operates
 and depth is gathered from GPU in NDC.
 The value of 
\begin_inset Formula $z^{NDC}=0$
\end_inset

 usually belongs to sky.
\end_layout

\begin_layout Standard
The camera divides the camera space to two half-spaces, in front of camera
 
\begin_inset Formula $z^{C}<0$
\end_inset

, and behind camera 
\begin_inset Formula $z^{C}\geq0$
\end_inset

 .
 The projection transformation from camera space to NDC space works correctly
 only for points that belong to half-space 
\begin_inset Formula $z^{C}<0$
\end_inset

.
 For every point in camera space, we can easily verify to which half-space
 it belongs and project only points belonging to the 
\begin_inset Formula $z^{C}<0$
\end_inset

 half-space.
 If we project points behind the camera,
\begin_inset Formula $z^{C}\geq0$
\end_inset

 to the NDC space, they will be mapped into the NDC space as if they were
 in front of camera.
\end_layout

\begin_layout Subsection
NDC to Window Coordinates
\end_layout

\begin_layout Standard
This is the last transformation of the rendering pipeline and only in this
 transformation the dimension reduction happen.
 So far points have been kept in homogeneous coordinates, but window coordinates
 are only 2D, expressing 
\begin_inset Formula $x$
\end_inset

 and 
\begin_inset Formula $y$
\end_inset

 coordinates of pixel where point will be rendered.
 Here, we need only GRAPHICS::_GET_ACTIVE_SCREEN_RESOLUTION
\begin_inset Formula $=\begin{pmatrix}W & H\end{pmatrix}$
\end_inset

 because this transformation depends only on screen with and height.
 
\end_layout

\begin_layout Standard
The transformation matrix is 
\begin_inset Formula 
\begin{align*}
T & =\begin{bmatrix}\frac{W}{2} & 0 & 0 & \frac{W}{2}\\
0 & \frac{-H}{2} & 0 & \frac{H}{2}
\end{bmatrix}
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
so the NDC to screen transformation is
\begin_inset Formula 
\[
T\begin{bmatrix}x_{max}^{NDC} & x_{min}^{NDC}\\
y_{max}^{NDC} & y_{min}^{NDC}\\
z_{max}^{NDC} & z_{min}^{NDC}\\
1 & 1
\end{bmatrix}=\begin{bmatrix}x_{max}^{P} & x_{min}^{P}\\
y_{max}^{P} & y_{min}^{P}
\end{bmatrix}
\]

\end_inset


\end_layout

\begin_layout Standard
Due to the division by width, the pipeline unfortunately can not be expressed
 as matrix multiplication by matrix constant for all points in one scene.
 
\end_layout

\begin_layout Section
Datasets proposal
\end_layout

\begin_layout Standard
As part of my thesis, I propose two novel synthetic datasets.
 Both of these datasets are outdoor, taken from virtual car.
 Both of these datasets contain outdoor images in all parts of day, dawn,
 day, evening, and night.
 They contain most of data described above, to provide as much information
 as possible for usability in various tasks.
 Both of these datasets contain full HD RGB-D images, stencil images, position
 and rotation of camera, positions, rotations, identifiers and types of
 cars and pedestrians around the camera, projection and view matrix for
 aligning data between different images.
 
\end_layout

\begin_layout Standard
The first dataset was used for voxel map reconstruction.
 It contains images from 4 virtual cameras attached to driving car, placed
 in circle opposite to each other, mapping space in front of car to create
 its detailed 3D reconstruction.
 There are 8371 scenes, each taken from 4 cameras.
 During this dataset gathering, 
\begin_inset Formula $13059$
\end_inset

 meters were driven in virtual world.
\end_layout

\begin_layout Standard
The second dataset is demonstration of common automotive dataset from driving
 car.
 Compared to real-world datasets, this one has advantage of pixel-wise depth,
 precise on all surfaces and even in high distances, outperforming LIDAR
 technology in accuracy and depth point density.
 It is directly aligned with pixels of RGB images.
 The datasets consists of 22285 scenes, every of them captured by 4 cameras
 attached to car, heading different direction, as seen in figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Camera-positions-for"

\end_inset

 where camera positions are marked by white cubes.
 During this dataset gathering, 
\begin_inset Formula $34340$
\end_inset

 meters were driven in virtual world.
\end_layout

\begin_layout Standard
todo: dodělat datasets proposal
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Graphics
	filename obrazky/dataset-2-cameras.png
	lyxscale 40
	width 12cm

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:Camera-positions-for"

\end_inset

Camera positions for second dataset
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Chapter
3D map estimation
\end_layout

\begin_layout Standard
Restoring 3D map from single image is a complex task.
 We can think of depth estimation as preceding problem to the 3D map estimation,
 because in depth estimation, we estimate distance of nearest object pixel-wise,
 and in 3D map estimation, we have multiple depth levels per pixel and estimate
 occupancy per each depth level.
 The neural network output is cuboid with size of XxYxZ neurons, hence it
 is useful to represent 3D map of a cuboid as a voxel map with X width levels,
 Y height levels and Z depth levels.
 Then we can estimate occupancy per voxel, in other words, per each neuron
 output.
 Thus we can represent both depth and 3D map estimation as similar instances,
 depth estimation being single class occupancy classification per pixel,
 and 3D map estimation being multi-class occupancy classification per pixel.
\end_layout

\begin_layout Section
Residual networks
\end_layout

\begin_layout Standard
In machine learning, deep convolutional neural networks have been used more
 and more mostly due to significant breakthroughs in image classification
 tasks.
 Advantage of deep neural networks is they are capable to naturally utilize
 low-level, mid-level, and high-level features, without need for manual
 feature-engineering and lets us train whole architectures end-to-end.
 Recent results 
\begin_inset CommandInset citation
LatexCommand cite
key "going-deeper-with-convolutions,very-deep-image-recognition"

\end_inset

 in visual recognition tasks, mostly on ImageNet dataset 
\begin_inset CommandInset citation
LatexCommand cite
key "imagenet"

\end_inset

, reveal depth of network plays crucial role in model accuracy.
 When deeper networks were created simply by stacking more and more layers
 in the model, these models become hard to train.
 One of these problems is the notorious vanishing gradient, which is being
 addressed by many approaches, like normalized initialization 
\begin_inset CommandInset citation
LatexCommand cite
key "efficient-backprop,difficulty-deep-forward"

\end_inset

 and normalization layers 
\begin_inset CommandInset citation
LatexCommand cite
key "batch-normalization"

\end_inset

.
 Other problem is the degradation problems, where training accuracy starts
 to worsen, after stacking more layers 
\begin_inset CommandInset citation
LatexCommand cite
key "highway-networks"

\end_inset

.
 Residual networks aim to address this problem of degradation.
\end_layout

\begin_layout Standard
If shallower model is able to learn with higher accuracy than deeper model,
 we want the deeper model to be able to learn at least same as shallower
 one, or better.
 If we stack new layer into model, we want them to be able to learn identity
 mapping.
 Then the model with new layers will behave same as shallower model during
 prediction.
 Experiments show that current solvers using gradient descent are not able
 to find such solution which would let newer layer to learn identity mapping
 in feasible time.
 The residual learning aims to tackle this problem by explicitly learning
 the residual mapping instead of original mapping.
 If we want our newly stacked layers to represent mapping 
\begin_inset Formula $\mathcal{H}\left(\boldsymbol{x}\right)$
\end_inset

, instead we let them learn another mapping 
\begin_inset Formula $\mathcal{F}\left(\boldsymbol{x}\right)=\mathcal{H}\left(\boldsymbol{x}\right)-\boldsymbol{x}$
\end_inset

.
 The original mapping we aim to, 
\begin_inset Formula $\mathcal{H}\left(\boldsymbol{x}\right)$
\end_inset

 is then reconstructed as 
\begin_inset Formula $\mathcal{H}\left(\boldsymbol{x}\right)=\mathcal{F}\left(\boldsymbol{x}\right)+\boldsymbol{x}$
\end_inset

.
 He et al.
 
\begin_inset CommandInset citation
LatexCommand cite
key "resnet"

\end_inset

 shows it is easier to learn the residual mapping than to learn the original
 mapping.
 It also more easily preserves the identity mapping, if 
\begin_inset Formula $\mathcal{F}\left(\boldsymbol{x}\right)=\boldsymbol{0}$
\end_inset

, because it is easier to fit these layers to zero than to explicitly learn
 identity mapping.
 
\end_layout

\begin_layout Standard
The formulation of 
\begin_inset Formula $\mathcal{F}\left(\boldsymbol{x}\right)+\boldsymbol{x}$
\end_inset

 can be realized by feed-forward neural network with shortcut connections
 
\begin_inset CommandInset citation
LatexCommand cite
key "bishop-neural-networks"

\end_inset

, also known as skip connections.
 The advantage of this approach is it can be easily implemented in current
 neural network frameworks out of the box, and it whole network can still
 be learned by SGD end-to-end without any further modifications.
 Intuitively, the identity mapping, shortcut connection can be seen as an
 information highway, where information flows unchanged both forwards and
 backwards without any changes.
 He et al.
 
\begin_inset CommandInset citation
LatexCommand cite
key "resnet"

\end_inset

 show the residual learning lets us train very deep models and present several
 architectures based on residual learning: ResNet-18, ResNet-34, Resnet-50,
 ResNet-101, and ResNet-152.
 basically ResNet architecture consists of 4 residual blocks, repeated many
 times.
 The difference between individual architectures is only in number of repetition
s these residual blocks.
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Graphics
	filename obrazky/resnet-layers.png
	lyxscale 80
	width 5cm

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Residual and shortcut layers
\begin_inset CommandInset citation
LatexCommand cite
key "resnet"

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Section
Depth estimation
\end_layout

\begin_layout Standard
Depth estimation is one of the most fundamental tasks in computer vision.
 Most existing methods 
\begin_inset CommandInset citation
LatexCommand cite
key "depth-and-surface-estimation,predicting-depth"

\end_inset

 formulate depth estimation as a regression task due to the continuous nature
 of depth values.
 Those models for depth estimation are usually trained to minimize L2 norm
 between ground truth and predicted depths.
 However it shows that regressing to exact depth is difficult task.
 In many applications, depth can be known only approximately.
 With aforementioned approach, I approximate the regression formulation
 of depth estimation with classification problem.
 Thus instead of training to predict exact depth, we predict only depth
 range, still small enough to be useful for our application.
 Advantage of classification formulation of the problem is that after applying
 softmax on output of neural network, depths are naturally predicted as
 confidence in form of probability that particular depth level is occupied
 
\begin_inset CommandInset citation
LatexCommand cite
key "depth-estimation-as-classification"

\end_inset

.
 Most notable methods 
\begin_inset CommandInset citation
LatexCommand cite
key "depth-estimation-as-classification,depth-estimation-hierarchical-fusion-soft-weighting"

\end_inset

 use deep convolutional networks based on famous object classification architect
ures.
 
\end_layout

\begin_layout Standard
Li et al.
 
\begin_inset CommandInset citation
LatexCommand cite
key "depth-estimation-hierarchical-fusion-soft-weighting"

\end_inset

 use architecture based on deep residual network 
\begin_inset CommandInset citation
LatexCommand cite
key "resnet"

\end_inset

, specifically Resnet-152.
 The architecture is modified, it does not use the fully connected layer
 in the end of the network, which drastically decreases the number of trainable
 parameters, and instead it appends one convolutional and one deconvolutional
 layer.
 Also, Li et al.
 utilize the hierarchical fusion, where output of each block of ResNet is
 concatenated to other outputs, with dropout layer afterwards, then the
 network utilizes both low-level and high-level features of the input image
 during the final layers of depth estimation.
 The output of final layer is 120x160x200, meaning it outputs 120x160 pixels
 image with 200 depth levels.
 The depth space is equally discretized in log space.
 Specifically, 
\begin_inset Formula 
\[
l=round\left(\frac{\log\left(d\right)-\log\left(d_{min}\right)}{q}\right)
\]

\end_inset

where 
\begin_inset Formula $l$
\end_inset

 is the quantized label, q is the width of quantization bin, 
\begin_inset Formula $d$
\end_inset

 is the continuous depth value and 
\begin_inset Formula $d_{min}$
\end_inset

 is the minimum depth value in the dataset.
 
\end_layout

\begin_layout Standard
Other notable approach of Li et al.
 is soft weighted sum inference.
 Usually, the depth is reconstructed intuitively as a centre of depth bin
 with maximum value pixel-wise 
\begin_inset CommandInset citation
LatexCommand cite
key "depth-estimation-as-classification"

\end_inset

.
 The last layer is softmax, so values can be interpreted as probabilities
 of each depth bin being being correct one.
 Thus the output contains probability distribution of depth pixel-wise.
 Li et al.
 reconstruct depth as a soft weighted sum inference by 
\begin_inset Formula 
\[
\hat{d}=\exp\left\{ \boldsymbol{w}^{T}\boldsymbol{p}\right\} ,w_{i}=\log\left(d_{min}\right)+q\cdot i
\]

\end_inset

where 
\begin_inset Formula $\boldsymbol{w}$
\end_inset

 is the weight vector of depth bins, 
\begin_inset Formula $i$
\end_inset

 is bin index, 
\begin_inset Formula $\boldsymbol{p}$
\end_inset

 is the output score, and 
\begin_inset Formula $\hat{d}$
\end_inset

 is reconstructed depth.
\end_layout

\begin_layout Standard
Then the model is trained end-to-end to minimize the pixel-wise multinomial
 logistic loss 
\begin_inset Formula 
\[
L=-\left[\stackrel[i=1]{N}{\sum}\stackrel[k=1]{K}{\sum}1\left\{ D_{k}=D_{i}^{*}\right\} \log\left(p_{i}^{D_{k}}\right)\right]
\]

\end_inset

where 
\begin_inset Formula $N$
\end_inset

 is the number of pixels, 
\begin_inset Formula $K$
\end_inset

 is the number of depth bins, 
\begin_inset Formula $D_{i}^{*}$
\end_inset

 is ground truth depth on pixel 
\begin_inset Formula $i$
\end_inset

, and 
\begin_inset Formula $p_{i}^{D_{k}}$
\end_inset

 is the probability of pixel 
\begin_inset Formula $i$
\end_inset

 to be labelled with 
\begin_inset Formula $D_{k}$
\end_inset

 depth bin.
\end_layout

\begin_layout Standard
The classical multi-class classification assumes categorical labels without
 any ordering defined.
 However depth bins are ordinal labels, not categorical, because they have
 defined ordering and semantically it makes sense to take into account distance
 between correct and predicted label during training.
 This approach is not usable for categorical data, but for nominal data,
 this knowledge can be used to provide more information for the learning.
 Cao et al.
 
\begin_inset CommandInset citation
LatexCommand cite
key "depth-estimation-as-classification"

\end_inset

 propose modified loss function which takes distance between depth ins into
 account.
 They use similar architecture, with pixel-wise multinomial logistic loss,
 but with slight modification.
 In usual logistic loss, the loss is non-zero only for correct label, due
 to the 
\begin_inset Formula $1\left\{ D_{k}=D_{i}^{*}\right\} $
\end_inset

part of the function.
 Cao et al.
 propose 
\begin_inset Quotes eld
\end_inset

information gain
\begin_inset Quotes erd
\end_inset

 
\begin_inset CommandInset citation
LatexCommand cite
key "depth-estimation-as-classification"

\end_inset

 instead, where 
\begin_inset Formula $1\left\{ D_{k}=D_{i}^{*}\right\} $
\end_inset

 is replaced by 
\begin_inset Formula $H\left(D_{i}^{*},D_{k}\right)$
\end_inset

, where 
\begin_inset Formula $H$
\end_inset

 is 
\begin_inset Formula $B\times B$
\end_inset

 symmetric matrix with elements 
\begin_inset Formula $H\left(p,q\right)=exp\left(-\alpha\left(p-q\right)^{2}\right)$
\end_inset

 where 
\begin_inset Formula $\alpha$
\end_inset

 is a constant.
 This information gain matrix lets the SGD propagate the error not only
 through correct label and preceding softmax layer, but also through its
 neighbouring labels, which helps updating the network parameters.
 So the modified loss is 
\begin_inset Formula 
\[
L=-\left[\stackrel[i=1]{N}{\sum}\stackrel[k=1]{K}{\sum}H\left(D_{i}^{*},D_{k}\right)\log\left(p_{i}^{D_{k}}\right)\right]
\]

\end_inset


\end_layout

\begin_layout Standard
Multiple datasets are being used for depth estimation.
 Most of them are indoor, since indoor we are able to measure depth more
 precisely and we generally deal with smaller distances.
 In my case, I trained for estimating outdoor depth.
 The synthetic dataset retrieved from GTA V has big advantage in dense and
 high precision depth labels seen nowhere in real datasets.
 In the outdoor depth estimation, I don't care about very distant points,
 so the depth estimation is thresholded 
\end_layout

\begin_layout Section
3D map estimation
\end_layout

\begin_layout Standard
The 3D map estimation from single RGB image is an abstract task and has
 many possible representations.
 In many tasks, voxel-map has proven to be good representation of the 3D
 space.
 Voxel (volumetric pixel) is the smallest distinguishable unit in 3D space,
 it is 3D analogy of 2D pixel.
 In this scenario, each voxel has one of three states: free, occupied and
 unknown.
 The size of voxel is not generally set and is being chosen to fit particular
 task.
 Too big voxel size causes high loss of information, and too small voxel
 size leads to high amount of voxels, more complicated manipulation with
 the resulting voxelmap and higher size when persisting the voxelmap.
 
\end_layout

\begin_layout Standard
Image which is used as an input for 3D map estimation is representing only
 a frustum in 3D map and does not contain information about space outside
 of this frustum.
 Because of this fact, setup in this thesis focuses only on reconstruction
 of 3D space inside the frustum viewed by the camera taking the image.
 Output of the neural network is 
\begin_inset Formula $X\times Y\times Z$
\end_inset

 cuboid, which will be mapped to the frustum so output of each neuron represents
 point in the frustum.
 Mapping between the the cuboid and frustum is contained in the projection
 matrix 
\begin_inset CommandInset ref
LatexCommand ref
reference "subsec:Camera-to-NDC"

\end_inset

 which is used for the training dataset creation and for reconstruction
 of pointcloud from neural network output during prediction.
 With this setup, the model is predicting occupancy of points sampled from
 the frustum.
 In my setup, the frustum will be up to 25m from camera.
\end_layout

\begin_layout Subsection
Training dataset construction from depth images
\end_layout

\begin_layout Standard
The synthetic dataset created from GTA V contains depth images and camera
 parameters, so there is need to reconstruct the 3D map and sample it to
 create the training dataset.
 For 3D map reconstruction, I gathered data from four cameras with positions
 relative to the driving car, as seen in figure X where each white cube
 represents camera position 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Camera-positions-for-3d"

\end_inset

.
 Totally there are 4 cameras, equally placed on the circle with 8 meters
 circumference.
 This setup will demonstrate the whole process of building 3D frustum map
 from 4 cameras
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename obrazky/dataset-1-cameras.png
	lyxscale 50
	width 12cm

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:Camera-positions-for-3d"

\end_inset

Camera positions for 3d reconstruction
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Graphics
	filename obrazky/2018-05-08--14-15-35--576.jpg
	lyxscale 40
	width 7cm

\end_inset


\begin_inset Graphics
	filename obrazky/2018-05-08--14-15-35--972.jpg
	lyxscale 40
	width 7cm

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Graphics
	filename obrazky/2018-05-08--14-15-36--349.jpg
	lyxscale 40
	width 7cm

\end_inset


\begin_inset Graphics
	filename obrazky/2018-05-08--14-15-36--728.jpg
	lyxscale 40
	width 7cm

\end_inset


\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:extracted-RGB-images"

\end_inset

extracted RGB images from 4 cameras
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Graphics
	filename obrazky/2018-05-08--14-15-35--576-depth.png
	lyxscale 40
	width 7cm

\end_inset


\begin_inset Graphics
	filename obrazky/2018-05-08--14-15-35--972-depth.png
	lyxscale 40
	width 7cm

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Graphics
	filename obrazky/2018-05-08--14-15-36--349-depth.png
	lyxscale 40
	width 7cm

\end_inset


\begin_inset Graphics
	filename obrazky/2018-05-08--14-15-36--728-depth.png
	lyxscale 40
	width 7cm

\end_inset


\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:extracted-depth-images"

\end_inset

extracted depth images from 4 cameras
\end_layout

\end_inset


\end_layout

\end_inset

From these 4 cameras, I gathered 4 RGB and depth images, shown in figures
 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:extracted-RGB-images"

\end_inset

 and 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:extracted-depth-images"

\end_inset

.
 I also gathered camera parameters, namely position, rotation, near clip,
 and field of view.
 With these parameters, depth images can be easily transformed into the
 pointcloud in world coordinate system.
 Let us have 
\begin_inset Formula $\boldsymbol{I}^{1}$
\end_inset

depth image from the 1st camera with with width 
\begin_inset Formula $W$
\end_inset

 and height 
\begin_inset Formula $H$
\end_inset

 pixels.
 Since depth image contains value in NDC space then, 
\begin_inset Formula $\boldsymbol{I}^{1}=\left(d_{i,j}\right)\in\left[0,1\right]^{W\times H}$
\end_inset

 holds, where 
\begin_inset Formula $d_{i,j}$
\end_inset

 is value of pixel with coordinates 
\begin_inset Formula $\left[i,j\right]$
\end_inset

.
 For every pixel, we know its depth value and coordinates in the pixel space,
 thus we can describe it as a point in NDC space
\begin_inset Formula 
\[
\boldsymbol{x}_{i,j}^{NDC}=\begin{bmatrix}x^{NDC}\\
y^{NDC}\\
z^{NDC}\\
1
\end{bmatrix}=\begin{bmatrix}\frac{2i}{W}-1\\
-\left(\frac{2j}{H}-1\right)\\
d_{i,j}\\
1
\end{bmatrix}
\]

\end_inset

, where the 
\begin_inset Formula $\boldsymbol{x}_{i,j}^{NDC}$
\end_inset

 is pixel
\begin_inset Formula $\left[i,j\right]$
\end_inset

 transformed into NDC space.
 The sign change in 
\begin_inset Formula $y^{NDC}$
\end_inset

 is cause by indexing conventions of images, where lowest pixel height is
 in the upper part of image but in NDC space, lowest 
\begin_inset Formula $y^{NDC}$
\end_inset

 value is in the lower part of image.
 This holds because 
\begin_inset Formula $x^{NDC},x^{NDC}\in\left[-1,1\right]$
\end_inset

.
 With this pixel-wise transformation, we can transform each depth image
 
\begin_inset Formula $\boldsymbol{I}^{k},k\in1..4$
\end_inset

 into pointcloud in NDC 
\begin_inset Formula $\boldsymbol{P}_{k}^{NDC}$
\end_inset

.
 By using transformation matrices described in 
\begin_inset CommandInset ref
LatexCommand ref
reference "subsec:World-to-Camera"

\end_inset

 and 
\begin_inset CommandInset ref
LatexCommand ref
reference "subsec:Camera-to-NDC"

\end_inset

 we can transform these pointclouds from different cameras into the same
 world coordinate space.
 Let us have 
\begin_inset Formula $P$
\end_inset

 matrix from 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:projection-m"

\end_inset

 and 
\begin_inset Formula $V_{k}$
\end_inset

 matrix from 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:camera-m"

\end_inset

 denoting the view matrix of 
\begin_inset Formula $k$
\end_inset

-th camera, since these matrices are regular, we can do the transformation
 
\begin_inset Formula 
\[
\boldsymbol{P}_{k}^{W}=V_{k}^{-1}P^{-1}\boldsymbol{P}_{k}^{NDC}\forall k\in1..4
\]

\end_inset

.
 In this setup, all images are nearly Full HD, specifically 
\begin_inset Formula $1920\times1057$
\end_inset

.
 This leads to pointcloud of size 
\begin_inset Formula $1920\cdot1057=2029440\thickapprox2M$
\end_inset

 points.
 Most of these points are near camera, in unnecessarily high density for
 this application.
 To decrease the size of pointclouds and the time to process, them I clustered
 them into 12cm big voxels and sampled only 1 point per voxel.
 This sub-sampling decreases the pointcloud size from 
\begin_inset Formula $\sim2M$
\end_inset

 points into 
\begin_inset Formula $\sim80k$
\end_inset

 to 
\begin_inset Formula $\sim120k$
\end_inset

 points, which is 4% to 6% of original size.
 Then, all pointclouds are merged into one pointcloud 
\begin_inset Formula $\boldsymbol{P}^{W}$
\end_inset

 of whole scene seen from 4 cameras
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\boldsymbol{P}^{W}=\underset{k\in1..4}{\bigcup}\boldsymbol{P}_{k}^{W}
\]

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
original and sub-sampled pointclouds
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
todo: dodat obrázek s pozicemi kamery, ukázat celé workflow
\end_layout

\begin_layout Standard
todo: popsat můj setup, dodat obrázky a experimenty
\end_layout

\begin_layout Chapter
Experiments
\end_layout

\begin_layout Section
\begin_inset CommandInset label
LatexCommand label
name "subsec:Reverse-engineering"

\end_inset

Reverse engineering the true Far Clip
\end_layout

\begin_layout Standard
For reverse engineering the far clip, I gathered 33293 screenshots with
 parameters for projection matrix reconstruction and projection matrices.
 Because during whole data gathering none of the parameters used to reconstruct
 projection matrix, was changed, the projection matrix should be same for
 all records.
 As mentioned in 
\begin_inset CommandInset ref
LatexCommand ref
reference "subsec:Camera-to-NDC"

\end_inset

 parameters for reconstructing the Projection matrix are near clip, far
 clip, screen width, screen height and field of view.
\end_layout

\begin_layout Standard
The screenshot contain both RGB images and depth buffer from GPU.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename obrazky/2018-03-30--06-00-56--114.jpg
	lyxscale 40
	scale 20

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:Example-of-RGB"

\end_inset

Example of RGB image
\end_layout

\end_inset


\end_layout

\end_inset


\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename obrazky/2018-03-30--06-00-56--114-depth-8bit-rescaled.png
	lyxscale 40
	scale 20

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:Example-of-depth"

\end_inset

Example of depth buffer
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
The projection matrix transforms frustum into cuboid.
 Open frameworks have publicly available projection matrices, but RAGE does
 not have publicly available any information about projection matrix, so
 in order to obtain true far clip, I needed to reverse-engineer the mathematical
 description of the projection matrix.
 For approximate estimation of projection matrix parameters, I used DirectX
 projection matrix
\begin_inset CommandInset citation
LatexCommand cite
key "real-time-rendering"

\end_inset

 as a starting point for analysis, because GTA V requires DirectX, so I
 assumed it is underlying framework of RAGE.
\end_layout

\begin_layout Standard
The DirectX projection matrix is 
\begin_inset Formula 
\[
P^{DirectX}=\begin{bmatrix}\frac{2n}{r-l} & 0 & -\frac{r+l}{r-l} & 0\\
0 & \frac{2n}{t-b} & -\frac{t+b}{t-b} & 0\\
0 & 0 & \frac{f}{f-n} & -\frac{fn}{f-n}\\
0 & 0 & 1 & 0
\end{bmatrix}
\]

\end_inset


\end_layout

\begin_layout Standard
where 
\begin_inset Formula $n$
\end_inset

 is near clip, 
\begin_inset Formula $f$
\end_inset

 is far clip, 
\begin_inset Formula $l$
\end_inset

 and 
\begin_inset Formula $r$
\end_inset

 determine distance between left and right planes of the frustum and 
\begin_inset Formula $t$
\end_inset

 and 
\begin_inset Formula $b$
\end_inset

 determine distance between top and bottom planes.
\end_layout

\begin_layout Standard
The view frustum is symmetric, so 
\begin_inset Formula $r=-l$
\end_inset

 and 
\begin_inset Formula $t=-b$
\end_inset

 
\begin_inset CommandInset citation
LatexCommand cite
key "real-time-rendering"

\end_inset

.
 In that case, the projection matrix is simplified to form
\begin_inset Formula 
\[
P^{formal}=\begin{bmatrix}\frac{2n}{r+r} & 0 & -\frac{r-r}{r+r} & 0\\
0 & \frac{2n}{t+t} & -\frac{t-t}{t+t} & 0\\
0 & 0 & \frac{f}{f-n} & -\frac{fn}{f-n}\\
0 & 0 & 1 & 0
\end{bmatrix}=\begin{bmatrix}\frac{n}{r} & 0 & 0 & 0\\
0 & \frac{n}{t} & 0 & 0\\
0 & 0 & \frac{f}{f-n} & -\frac{fn}{f-n}\\
0 & 0 & 1 & 0
\end{bmatrix}
\]

\end_inset

The DirectX maps near clip to 0 and far clip to 1, but from data, where
 obviously
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Example-of-depth"

\end_inset

 nearer pixels had higher value in depth buffer than pixel more far from
 camera, I concluded that near and far clip are being mapped to 1 and 0,
 respectively.
 The far clip being mapped to 0 can also be deduced by pixels for sky having
 0 value.
\end_layout

\begin_layout Standard
Due to this fact, we switch the near and clip in the matrix formal description
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
P^{formal}=\begin{bmatrix}\frac{f}{r} & 0 & 0 & 0\\
0 & \frac{f}{t} & 0 & 0\\
0 & 0 & \frac{n}{n-f} & -\frac{fn}{n-f}\\
0 & 0 & 1 & 0
\end{bmatrix}
\]

\end_inset


\end_layout

\begin_layout Standard
The example in 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Example-of-depth"

\end_inset

 does not have actual depth buffer values, but instead, it is rescaled visualiza
tion.
 Since the depth buffer pixels are in range 
\begin_inset Formula $\left[0,1\right]$
\end_inset

 and PNG images take unsigned 8bit integer, this image is mapped linearly
 from 
\begin_inset Formula $\left[0,1\right]$
\end_inset

 to 
\begin_inset Formula $\left[0,255\right]$
\end_inset

.
 Since even the nearest pixels were distant from near clip and real range
 of pixels in this image was 
\begin_inset Formula $\left[0,19\right]$
\end_inset

, I rescaled it 10 times to range 
\begin_inset Formula $\left[0,190\right]$
\end_inset

, so the depth is visible.
\end_layout

\begin_layout Standard
At first, I assumed the camera near clip and far clip obtained by native
 calls 
\begin_inset CommandInset ref
LatexCommand ref
reference "native-call-near-clip"

\end_inset

 and the projection matrix is same as in DirectX.
 
\end_layout

\begin_layout Standard
The near clip and far clip calculation can be demonstrated on image 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Example-of-RGB"

\end_inset

.
 
\end_layout

\begin_layout Standard
By calling CAM::GET_CAM_NEAR_CLIP
\begin_inset Formula $=n_{c}$
\end_inset

 and CAM::GET_CAM_FAR_CLIP
\begin_inset Formula $=f_{c}$
\end_inset

 I obtained values 
\begin_inset Formula $n_{c}=1.5$
\end_inset

 and 
\begin_inset Formula $f_{c}=800$
\end_inset

.
 I also obtain projection matrix calculated by method described in 
\begin_inset CommandInset ref
LatexCommand ref
reference "subsec:Rendering-pipeline-data"

\end_inset

, which is 
\begin_inset Formula 
\[
P^{real}=\begin{bmatrix}1.210067 & 0 & 0 & -0.000004\\
0 & 2.144507 & 0 & 0.000002\\
0 & 0 & 0.00015 & 1.500225\\
0 & 0 & -1 & 0
\end{bmatrix}
\]

\end_inset


\end_layout

\begin_layout Standard
In the formalization of the matrix, 
\begin_inset Formula $P_{formal}$
\end_inset

, there are 4 variables.
 
\begin_inset Formula $r$
\end_inset

 and 
\begin_inset Formula $t$
\end_inset

 appear only in one element of matrix, so they can be verified only after
 reverse engineering the far clip.
 From the 
\begin_inset Formula $P_{2,2}^{formal}$
\end_inset

 and 
\begin_inset Formula $P_{2,3}^{formal}$
\end_inset

, I can calculate the near and far clip by 
\begin_inset Formula 
\begin{align*}
P_{2,2}^{formal} & =\frac{n}{n-f}\\
nP_{2,2}^{formal}-fP_{2,2}^{formal} & =n\\
\frac{n\left(P_{2,2}^{formal}-1\right)}{P_{2,2}^{formal}} & =f
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align*}
P_{2,3}^{formal} & =-\frac{fn}{n-f}\\
P_{2,3}^{formal}\left(n-f\right) & =-fn\\
nP_{2,3}^{formal} & =f\left(P_{2,3}^{formal}-n\right)\\
\frac{nP_{2,3}^{formal}}{\left(P_{2,3}^{formal}-n\right)} & =f
\end{align*}

\end_inset


\begin_inset Formula 
\begin{align*}
\frac{nP_{2,3}^{formal}}{\left(P_{2,3}^{formal}-n\right)} & =\frac{n\left(P_{2,2}^{formal}-1\right)}{P_{2,2}^{formal}}\\
P_{2,3}^{formal}P_{2,2}^{formal} & =\left(P_{2,2}^{formal}-1\right)\left(P_{2,3}^{formal}-n\right)\\
\frac{P_{2,3}^{formal}P_{2,2}^{formal}}{P_{2,2}^{formal}-1} & =P_{2,3}^{formal}-n\\
P_{2,3}^{formal}-\frac{P_{2,3}^{formal}P_{2,2}^{formal}}{P_{2,2}^{formal}-1} & =n\\
-\frac{P_{2,3}^{formal}}{P_{2,2}^{formal}-1} & =n
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align*}
\frac{\left(-\frac{P_{2,3}^{formal}}{P_{2,2}^{formal}-1}\right)\left(P_{2,2}^{formal}-1\right)}{P_{2,2}^{formal}} & =f\\
-\frac{P_{2,3}^{formal}}{P_{2,2}^{formal}} & =f
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
From these calculations, we can calculate near and far clip as 
\begin_inset Formula 
\begin{align*}
n & =-\frac{P_{2,3}^{formal}}{P_{2,2}^{formal}-1}=-\frac{1.500225}{0.00015-1}=1.500225\\
f & =-\frac{P_{2,3}^{formal}}{P_{2,2}^{formal}}=-\frac{1.500225}{0.00015}=-10001.5
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
From these calculations we can see the third column of the projection matrix
 has incorrect sign, because the 
\begin_inset Formula $P_{3,2}^{formal}$
\end_inset

 should be 1 and instead it is -1, and the far clip is negative, which should
 not be.
 When changing signs of third column of projection matrix, we obtain following
 formal definition of projection matrix.
 That sign switching means the view frustum is in opposite direction of
 Z axis.
\begin_inset Formula 
\[
P^{formal}=\begin{bmatrix}\frac{f}{r} & 0 & 0 & 0\\
0 & \frac{f}{t} & 0 & 0\\
0 & 0 & -\frac{n}{n-f} & -\frac{fn}{n-f}\\
0 & 0 & -1 & 0
\end{bmatrix}
\]

\end_inset


\end_layout

\begin_layout Standard
After fixing the sign issue, the relationship between 
\begin_inset Formula $P^{formal}$
\end_inset

 and clips is
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align*}
\frac{P_{2,3}^{formal}}{P_{2,2}^{formal}+1} & =n=\frac{1.500225}{0.00015+1}=1.499999\\
\frac{P_{2,3}^{formal}}{P_{2,2}^{formal}} & =f=\frac{1.500225}{0.00015}=10001.5
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
As we can see, the 
\begin_inset Formula $n=1.499999\approx n_{c}=1.5$
\end_inset

 so for near clip, we can say we successfully reverse-engineered the relation
 between the projection matrix and the near clip.
 The far clip, on the other hand, differs 
\begin_inset Formula $f=10001.5\neq f_{c}=800$
\end_inset

.
 The difference is very high, which lead us to assumption that there is
 some new far clip, which is not same as obtained through API, 
\begin_inset Formula $f_{c}$
\end_inset

.
 
\end_layout

\begin_layout Standard
The other check we can perform is projecting points laying on near clip
 and far clip into NDC space.
 
\end_layout

\begin_layout Standard
We prepare two points.
 Because of many zero elements in 
\begin_inset Formula $P^{formal}$
\end_inset

, we can see 
\begin_inset Formula $x$
\end_inset

-axis and 
\begin_inset Formula $y$
\end_inset

-axis don't affect the 
\begin_inset Formula $z$
\end_inset

-axis of projected point.
 Thus I prepared two points:
\begin_inset Formula 
\[
\begin{bmatrix}1 & 1\\
1 & 1\\
-1.5 & -800\\
1 & 1
\end{bmatrix}
\]

\end_inset

, which are laying on the near clip and far clip, respectively.
 We would assume that they would be mapped to 1 and 0, respectively.
 The negative sign is here because in RAGE, the camera view frustum is in
 negative part of Z axis.
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\begin{bmatrix}1.210067 & 0 & 0 & -0.000004\\
0 & 2.144507 & 0 & 0.000002\\
0 & 0 & 0.00015 & 1.500225\\
0 & 0 & -1 & 0
\end{bmatrix}\begin{bmatrix}1 & 1\\
1 & 1\\
-0.15 & -800\\
1 & 1
\end{bmatrix}=\begin{bmatrix}1.210063 & 1.210063\\
2.144509 & 2.144509\\
1.5 & 1.380225\\
1.5 & 800
\end{bmatrix}
\]

\end_inset

by normalization we obtain 
\begin_inset Formula 
\[
\begin{bmatrix}\frac{1.210063}{1.5} & \frac{1.210063}{800}\\
\frac{2.144509}{1.5} & \frac{2.144509}{800}\\
\frac{1.50045}{1.5} & \frac{1.620225}{800}\\
\frac{1.5}{1.5} & \frac{800}{800}
\end{bmatrix}=\begin{bmatrix}0.80670867 & 0.00151258\\
1.42967267 & 0.00268064\\
1 & 0.00172528\\
1 & 1
\end{bmatrix}
\]

\end_inset

from which we can see the near clip 
\begin_inset Formula $n_{c}$
\end_inset

 is being projected correctly, but far clip 
\begin_inset Formula $f_{c}$
\end_inset

 is not being projected into 0 and that true far clip 
\begin_inset Formula $f$
\end_inset

 is behind this far flip 
\begin_inset Formula $f_{c}$
\end_inset

.
\end_layout

\begin_layout Standard
These calculations give us some insight into projection matrix and its role
 in far clip estimation, but for more robust estimate, I analysed all 33293
 matrices.
 
\end_layout

\begin_layout Standard
In GTA, matrices are not gathered correctly every time and in some cases,
 resulting matrices are unusable.
 Because I knew the near clip precisely, I discarded all matrices, with
 calculated near clip 
\begin_inset Formula $\left|n-n_{c}\right|>10^{-4}$
\end_inset

.
\end_layout

\begin_layout Section
Depth estimation
\end_layout

\begin_layout Chapter
Future work
\end_layout

\begin_layout Standard
In GTA V reverse-engineering, many stencil values semantics remain to be
 discovered.
 
\end_layout

\begin_layout Standard
\begin_inset CommandInset bibtex
LatexCommand bibtex
bibfiles "bibliography"
options "bibtotoc,plainnat"

\end_inset


\end_layout

\begin_layout Chapter
\start_of_appendix
Contents of the enclosed CD
\end_layout

\begin_layout Standard
\begin_inset listings
inline false
status open

\begin_layout Plain Layout

appendix content
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Newpage cleardoublepage
\end_inset


\end_layout

\end_body
\end_document
